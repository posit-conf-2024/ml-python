[
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to machine learning in Python with Scikit-learn",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n09:00 - 10:30\nSession 1 - Introduction to workshop and intro to machine learning (via Classification I)\n\n\n10:30 - 11:00\nCoffee break\n\n\n11:00 - 12:30\nSession 2 - Classification II\n\n\n12:30 - 13:30\nLunch break\n\n\n13:30 - 15:00\nSession 3 - Regression\n\n\n15:00 - 15:30\nCoffee break\n\n\n15:30 - 17:00\nSession 4 - Tree-based and ensemble methods"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression2/py_worksheet_regression2.html",
    "href": "materials/worksheets/py_worksheet_regression2/py_worksheet_regression2.html",
    "title": "Worksheet 9 - Regression Continued",
    "section": "",
    "text": "After completing this week’s lecture and assignment work, you will be able to:\n\nRecognize situations where a simple regression analysis would be appropriate for making predictions.\nExplain the \\(k\\)-nearest neighbour regression algorithm and describe how it differs from k-nn classification.\nInterpret the output of a \\(k\\)-nn regression.\nIn a dataset with two variables, perform \\(k\\)-nearest neighbour regression in Python using scikit-learn to predict the values for a test dataset.\nExecute cross-validation in Python to choose the number of neighbours.\nUsing Python, evaluate \\(k\\)-nn regression prediction accuracy using a test data set and an appropriate metric (e.g., root means square prediction error).\nIn a dataset with &gt; 2 variables, perform \\(k\\)-nn regression in Python using scikit-learn to predict the values for a test dataset.\nIn the context of \\(k\\)-nn regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE).\nDescribe advantages and disadvantages of the \\(k\\)-nearest neighbour regression approach.\nPerform ordinary least squares regression in Python using scikit-learn to predict the values for a test dataset.\nCompare and contrast predictions obtained from \\(k\\)-nearest neighbour regression to those obtained using simple ordinary least squares regression from the same dataset.\n\nThis worksheet covers parts of Chapter 8 of the online textbook. You should read this chapter before attempting this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression2/py_worksheet_regression2.html#marathon-training-revisited-with-linear-regression",
    "href": "materials/worksheets/py_worksheet_regression2/py_worksheet_regression2.html#marathon-training-revisited-with-linear-regression",
    "title": "Worksheet 9 - Regression Continued",
    "section": "Marathon Training Revisited with Linear Regression!",
    "text": "Marathon Training Revisited with Linear Regression!\n\nSource: https://media.giphy.com/media/BDagLpxFIm3SM/giphy.gif\nRemember our question from last week: what features predict whether athletes will perform better than others? Specifically, we are interested in marathon runners, and looking at how the maximum distance ran per week during training predicts the time it takes a runner to end the race?\nThis time around, however, we will analyze the data using simple linear regression rather than \\(k\\)-nn regression. In the end, we will compare our results to what we found last week with \\(k\\)-nn regression.\nQuestion 2.0  {points: 1}\nLoad the marathon data from the data/ folder and assign it to an object called marathon.\n\n# your code here\nraise NotImplementedError\nmarathon\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon is None)).encode(\"utf-8\")+b\"739e2\").hexdigest() == \"6e92f9a4e4c3ee38226eb47ccf55024c5f32aa31\", \"type of marathon is None is not bool. marathon is None should be a bool\"\nassert sha1(str(marathon is None).encode(\"utf-8\")+b\"739e2\").hexdigest() == \"59296bffcfb7bcd365f6c79ebfd8e5a40df62b2f\", \"boolean value of marathon is None is not correct\"\n\nassert sha1(str(type(marathon)).encode(\"utf-8\")+b\"739e3\").hexdigest() == \"bff0380c6a360860e46c818fdc334b2bcfeb5adb\", \"type of type(marathon) is not correct\"\n\nassert sha1(str(type(marathon.shape)).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"1eada99c236028fcff78fd6303354632c2cb5e62\", \"type of marathon.shape is not tuple. marathon.shape should be a tuple\"\nassert sha1(str(len(marathon.shape)).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"2e34911a50836d3cf47d13ee702a774b05b66acf\", \"length of marathon.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon.shape))).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"07d7c57045b521db23e59bcd29a30b64f750e408\", \"values of marathon.shape are not correct\"\nassert sha1(str(marathon.shape).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"c1b0dafb0e4d543a159d3cef1509b02336267aef\", \"order of elements of marathon.shape is not correct\"\n\nassert sha1(str(type(\"time_hrs\" in marathon.columns)).encode(\"utf-8\")+b\"739e5\").hexdigest() == \"b804c528bed5bf422960fd5c803d8d8689b2cdd6\", \"type of \\\"time_hrs\\\" in marathon.columns is not bool. \\\"time_hrs\\\" in marathon.columns should be a bool\"\nassert sha1(str(\"time_hrs\" in marathon.columns).encode(\"utf-8\")+b\"739e5\").hexdigest() == \"65ee06ccfc8ecf0d0497cf6b3322c74797040486\", \"boolean value of \\\"time_hrs\\\" in marathon.columns is not correct\"\n\nassert sha1(str(type(\"max\" in marathon.columns)).encode(\"utf-8\")+b\"739e6\").hexdigest() == \"73f7cbaa58ac0ae4fcf13fdb39b6a596d0a9e637\", \"type of \\\"max\\\" in marathon.columns is not bool. \\\"max\\\" in marathon.columns should be a bool\"\nassert sha1(str(\"max\" in marathon.columns).encode(\"utf-8\")+b\"739e6\").hexdigest() == \"11b4588727f7e1cd86d42c8e61e257ddf72cb9cf\", \"boolean value of \\\"max\\\" in marathon.columns is not correct\"\n\nassert sha1(str(type(round(sum(marathon['max']), 0))).encode(\"utf-8\")+b\"739e7\").hexdigest() == \"cb0dcf331ef1975fca37d4d6dc1e453b08bf2a33\", \"type of round(sum(marathon['max']), 0) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(round(sum(marathon['max']), 0), 2)).encode(\"utf-8\")+b\"739e7\").hexdigest() == \"0c743d369241ee5ae9d99436f698cf6105d89029\", \"value of round(sum(marathon['max']), 0) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(round(sum(marathon['time_hrs']), 0))).encode(\"utf-8\")+b\"739e8\").hexdigest() == \"bc74cad374cdd1f333d8f0dbdd3e8f40872b3827\", \"type of round(sum(marathon['time_hrs']), 0) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(round(sum(marathon['time_hrs']), 0), 2)).encode(\"utf-8\")+b\"739e8\").hexdigest() == \"014c3d62ed4ff997c449ec3c8c8a4ee9a2353073\", \"value of round(sum(marathon['time_hrs']), 0) is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 2.1  {points: 1}\nSimilar to what we have done for the last few weeks, we will first split the dataset into the training and testing datasets, using 75% of the original data as the training data. Remember, we will be putting the test dataset away in a ‘lock box’ that we will comeback to later after we choose our final model. Assign your training dataset to an object named marathon_training and your testing dataset to an object named marathon_testing.\nNext, set the time_hrs as the target (y) and max as the feature (X). Store the features as X_train and X_test and targets as y_train and y_test respectively for the marathon_training and marathon_testing.\nAssign the objects to marathon_training, marathon_testing, X_train, y_train, X_test and y_test respectively.\n\n# ___, ___ = train_test_split(\n#     ___,\n#     test_size=___,\n#     random_state=2000,  # Do not change the random_state\n# )\n\n# X_train = ___[___]  # A single column data frame\n# y_train = ___[___]  # A series\n\n# X_test = ___[___]  # A single column data frame\n# y_test = ___[___]  # A series\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_training is None)).encode(\"utf-8\")+b\"4f6ad\").hexdigest() == \"f674e38d6fdcb566897037d6bd60a47404feeb21\", \"type of marathon_training is None is not bool. marathon_training is None should be a bool\"\nassert sha1(str(marathon_training is None).encode(\"utf-8\")+b\"4f6ad\").hexdigest() == \"6fa797429392a5d615e12d87d6c270c224b62e87\", \"boolean value of marathon_training is None is not correct\"\n\nassert sha1(str(type(marathon_training.shape)).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"c65c0f879cf7d4bd9e072e641450bd00c675daa9\", \"type of marathon_training.shape is not tuple. marathon_training.shape should be a tuple\"\nassert sha1(str(len(marathon_training.shape)).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"33b155dbfcff513325406a6efb32adad20db7a02\", \"length of marathon_training.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_training.shape))).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"30d227d9ac1ed57ba85872a2a07b6117d273c251\", \"values of marathon_training.shape are not correct\"\nassert sha1(str(marathon_training.shape).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"bb16852ef07255cdb14cab3907de9fdd370231a4\", \"order of elements of marathon_training.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_training.age))).encode(\"utf-8\")+b\"4f6af\").hexdigest() == \"ba50b4a4b9e30582e162d71f7e94533f0cd2670e\", \"type of sum(marathon_training.age) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_training.age)).encode(\"utf-8\")+b\"4f6af\").hexdigest() == \"97e4849e109b18815b375aabbc4a675bf8b572f8\", \"value of sum(marathon_training.age) is not correct\"\n\nassert sha1(str(type(marathon_testing is None)).encode(\"utf-8\")+b\"4f6b0\").hexdigest() == \"874282999c909b82564d7ce122d4cc20808ae07d\", \"type of marathon_testing is None is not bool. marathon_testing is None should be a bool\"\nassert sha1(str(marathon_testing is None).encode(\"utf-8\")+b\"4f6b0\").hexdigest() == \"ca991e955d46513ca9dc43cb7faad1510405e6e2\", \"boolean value of marathon_testing is None is not correct\"\n\nassert sha1(str(type(marathon_testing.shape)).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"3827f444c705a95782a04332415cd5d97f324216\", \"type of marathon_testing.shape is not tuple. marathon_testing.shape should be a tuple\"\nassert sha1(str(len(marathon_testing.shape)).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"906078a9f2abf4de01fcf0daf6461d5788643f77\", \"length of marathon_testing.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_testing.shape))).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"2cc1bdf059f9d6117e78f7d6f6e5821ec9fc1bdb\", \"values of marathon_testing.shape are not correct\"\nassert sha1(str(marathon_testing.shape).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"5a1c8ea5fc9a728dee1c0b720c690e0a130064e2\", \"order of elements of marathon_testing.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_testing.age))).encode(\"utf-8\")+b\"4f6b2\").hexdigest() == \"e7a8984d43f4d32c814e54d4358442ad0a96003c\", \"type of sum(marathon_testing.age) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_testing.age)).encode(\"utf-8\")+b\"4f6b2\").hexdigest() == \"34fa963b05468045a3dfe6fa84cff7cdcc47d488\", \"value of sum(marathon_testing.age) is not correct\"\n\nassert sha1(str(type(X_train.columns.values)).encode(\"utf-8\")+b\"4f6b3\").hexdigest() == \"58cab9eda29ff2b2d6749caed7393fbdc6843f76\", \"type of X_train.columns.values is not correct\"\nassert sha1(str(X_train.columns.values).encode(\"utf-8\")+b\"4f6b3\").hexdigest() == \"569ca1bdf2aee914b88e0cd7bf39a6fb4701d65a\", \"value of X_train.columns.values is not correct\"\n\nassert sha1(str(type(X_train.shape)).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"c6004691c652e546af1b6b67844b4e846344c1d6\", \"type of X_train.shape is not tuple. X_train.shape should be a tuple\"\nassert sha1(str(len(X_train.shape)).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"b5e0ccffd5b8cbe7ba6731bff5c23dab8c0f4590\", \"length of X_train.shape is not correct\"\nassert sha1(str(sorted(map(str, X_train.shape))).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"1e3e3ec67d9ab5e7ee523bab8f560ee2549bc591\", \"values of X_train.shape are not correct\"\nassert sha1(str(X_train.shape).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"010ca15c9c455f5f34e84a6483db2b38abbf6bc0\", \"order of elements of X_train.shape is not correct\"\n\nassert sha1(str(type(y_train.name)).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"dffb93a65b9c15090a1dac65485392d02dd3b05e\", \"type of y_train.name is not str. y_train.name should be an str\"\nassert sha1(str(len(y_train.name)).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"0def0bf84e8da55beab10cd1fbfd9f7fbd31a1cc\", \"length of y_train.name is not correct\"\nassert sha1(str(y_train.name.lower()).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"9ede03ef4c98e99b336e0465df700ab86f738d01\", \"value of y_train.name is not correct\"\nassert sha1(str(y_train.name).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"9ede03ef4c98e99b336e0465df700ab86f738d01\", \"correct string value of y_train.name but incorrect case of letters\"\n\nassert sha1(str(type(y_train.shape)).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"14ef9f53aed59146615896c647d72fdee84a030d\", \"type of y_train.shape is not tuple. y_train.shape should be a tuple\"\nassert sha1(str(len(y_train.shape)).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"bcfdddcab49f87ca8545d798e2d70e638926a45e\", \"length of y_train.shape is not correct\"\nassert sha1(str(sorted(map(str, y_train.shape))).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"9149d98edddfb638a4de90d61ed99ae9884bb8a7\", \"values of y_train.shape are not correct\"\nassert sha1(str(y_train.shape).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"b7502ed93b7fb6823b51c2982622b915a4fb1041\", \"order of elements of y_train.shape is not correct\"\n\nassert sha1(str(type(X_test.columns.values)).encode(\"utf-8\")+b\"4f6b7\").hexdigest() == \"4474a8a8bfdfc8b012d956db8931980b76744722\", \"type of X_test.columns.values is not correct\"\nassert sha1(str(X_test.columns.values).encode(\"utf-8\")+b\"4f6b7\").hexdigest() == \"b2549900463f19a702b41da70c671104c0e8b682\", \"value of X_test.columns.values is not correct\"\n\nassert sha1(str(type(X_test.shape)).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"4c2da79bd29e66ef3fbb291d5188459fc5c4706f\", \"type of X_test.shape is not tuple. X_test.shape should be a tuple\"\nassert sha1(str(len(X_test.shape)).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"ecd604671069f62e45124763ec88fc80d7b82498\", \"length of X_test.shape is not correct\"\nassert sha1(str(sorted(map(str, X_test.shape))).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"3bf5695fc56458e4405a6f88a4f5506f5d901cb8\", \"values of X_test.shape are not correct\"\nassert sha1(str(X_test.shape).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"352788e35c8d809802def81e35c175258a93bd69\", \"order of elements of X_test.shape is not correct\"\n\nassert sha1(str(type(y_test.name)).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"4b81d94ef69cd62c1396789593bbee6e8a5fa1a5\", \"type of y_test.name is not str. y_test.name should be an str\"\nassert sha1(str(len(y_test.name)).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"f9f25f4c81c55229317166657fe150f016b21d38\", \"length of y_test.name is not correct\"\nassert sha1(str(y_test.name.lower()).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"b708b2f0439fe252df818bf470eee84eb19da428\", \"value of y_test.name is not correct\"\nassert sha1(str(y_test.name).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"b708b2f0439fe252df818bf470eee84eb19da428\", \"correct string value of y_test.name but incorrect case of letters\"\n\nassert sha1(str(type(y_test.shape)).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"f18361d9de35bec90f44b45ee43725ea9777275c\", \"type of y_test.shape is not tuple. y_test.shape should be a tuple\"\nassert sha1(str(len(y_test.shape)).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"e2309e5b908c5857ae791835ba17a8025f1c4312\", \"length of y_test.shape is not correct\"\nassert sha1(str(sorted(map(str, y_test.shape))).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"89c5cd5a88c20980c89973a438767b9f8a88ce3e\", \"values of y_test.shape are not correct\"\nassert sha1(str(y_test.shape).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"78ec5b8e1118ac59272fb5d5fdde68b526f3e166\", \"order of elements of y_test.shape is not correct\"\n\nprint('Success!')\n\nQuestion 2.2  {points: 1}\nUsing only the observations in the training dataset, create a scatterplot to assess the relationship between race time (time_hrs) and maximum distance ran per week during training (max). Put time_hrs on the y-axis and max on the x-axis. Use mark_point and remember to do whatever is necessary to make this an effective visualization, including addressing overplotting in a suitable manner.\nAssign this plot to an object called marathon_scatter.\n\n# your code here\nraise NotImplementedError\nmarathon_scatter\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_scatter is None)).encode(\"utf-8\")+b\"6986b\").hexdigest() == \"be72f6a83cd9482319f43f4b049fdd940e1f4c6d\", \"type of marathon_scatter is None is not bool. marathon_scatter is None should be a bool\"\nassert sha1(str(marathon_scatter is None).encode(\"utf-8\")+b\"6986b\").hexdigest() == \"79682770c2f1c151e70550446c11cc299f292dd9\", \"boolean value of marathon_scatter is None is not correct\"\n\nassert sha1(str(type(marathon_scatter.encoding.x['shorthand'])).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"6d3c30ce678b314024b4d80b82e5ed9697c02ce0\", \"type of marathon_scatter.encoding.x['shorthand'] is not str. marathon_scatter.encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_scatter.encoding.x['shorthand'])).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"0d12f86dda5f48bc1e5d3675345950ca7a48c4e7\", \"length of marathon_scatter.encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"8630293695025e3d22cab897d7c35cb472b47b6a\", \"value of marathon_scatter.encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.x['shorthand']).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"8630293695025e3d22cab897d7c35cb472b47b6a\", \"correct string value of marathon_scatter.encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_scatter.encoding.y['shorthand'])).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"2d41e0469437832c726023d3cf4e14159bd09b4c\", \"type of marathon_scatter.encoding.y['shorthand'] is not str. marathon_scatter.encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_scatter.encoding.y['shorthand'])).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"d931044a5378994c5f2df4ddb4637dbabf5d23f6\", \"length of marathon_scatter.encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"c3e46f0aece8b220b81d7e8ea490b10e5265e671\", \"value of marathon_scatter.encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.y['shorthand']).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"c3e46f0aece8b220b81d7e8ea490b10e5265e671\", \"correct string value of marathon_scatter.encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_scatter.mark.type)).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"25bf70dd77143087753edd66a74bdbdab609b641\", \"type of marathon_scatter.mark.type is not str. marathon_scatter.mark.type should be an str\"\nassert sha1(str(len(marathon_scatter.mark.type)).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"64843ac47bf38de071408c16ebd821f565913dc5\", \"length of marathon_scatter.mark.type is not correct\"\nassert sha1(str(marathon_scatter.mark.type.lower()).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"0c5b2e8e01caf2686729e9e986b2e067d2f5f42c\", \"value of marathon_scatter.mark.type is not correct\"\nassert sha1(str(marathon_scatter.mark.type).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"0c5b2e8e01caf2686729e9e986b2e067d2f5f42c\", \"correct string value of marathon_scatter.mark.type but incorrect case of letters\"\n\nassert sha1(str(type(marathon_scatter.data.shape[0])).encode(\"utf-8\")+b\"6986f\").hexdigest() == \"fb072f8df94e039c392391cbcbafbdcbb5f6b693\", \"type of marathon_scatter.data.shape[0] is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(marathon_scatter.data.shape[0]).encode(\"utf-8\")+b\"6986f\").hexdigest() == \"b7c9c137f9bca9116ce605daa01dcd1c0edf9ec4\", \"value of marathon_scatter.data.shape[0] is not correct\"\n\nassert sha1(str(type('opacity' in marathon_scatter.mark.to_dict())).encode(\"utf-8\")+b\"69870\").hexdigest() == \"dd9e9a03b10d76cac8120e773139b76b99491ff9\", \"type of 'opacity' in marathon_scatter.mark.to_dict() is not bool. 'opacity' in marathon_scatter.mark.to_dict() should be a bool\"\nassert sha1(str('opacity' in marathon_scatter.mark.to_dict()).encode(\"utf-8\")+b\"69870\").hexdigest() == \"3c374d1e3da1cbd8cdef7d942941642f04285685\", \"boolean value of 'opacity' in marathon_scatter.mark.to_dict() is not correct\"\n\nassert sha1(str(type(isinstance(marathon_scatter.encoding.x['title'], str))).encode(\"utf-8\")+b\"69871\").hexdigest() == \"e02db54bd87fbf96616612a56f8acf8cf441d515\", \"type of isinstance(marathon_scatter.encoding.x['title'], str) is not bool. isinstance(marathon_scatter.encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_scatter.encoding.x['title'], str)).encode(\"utf-8\")+b\"69871\").hexdigest() == \"acd2e62da16b691eb93e82f53cbf50df9822ab62\", \"boolean value of isinstance(marathon_scatter.encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_scatter.encoding.y['title'], str))).encode(\"utf-8\")+b\"69872\").hexdigest() == \"8295031153ea95799755569e0f06ae54b3f3b979\", \"type of isinstance(marathon_scatter.encoding.y['title'], str) is not bool. isinstance(marathon_scatter.encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_scatter.encoding.y['title'], str)).encode(\"utf-8\")+b\"69872\").hexdigest() == \"db51f97f3427306d46b9eb73b81474c0e722baa9\", \"boolean value of isinstance(marathon_scatter.encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 2.3  {points: 1}\nNow that we have looked at our training data, the next step is to build a linear regression model.\nInstead of using the KNeighborsRegressor function, we will be using the LinearRegression function to let scikit-learn know we want to perform a linear regression.\nAssign your answer to an object named lm.\n\n# lm = _____()\n\n# your code here\nraise NotImplementedError\nlm\n\n\nfrom hashlib import sha1\nassert sha1(str(type(lm is None)).encode(\"utf-8\")+b\"72c66\").hexdigest() == \"e97b0813818ca802348d6ece9741f60284f4097b\", \"type of lm is None is not bool. lm is None should be a bool\"\nassert sha1(str(lm is None).encode(\"utf-8\")+b\"72c66\").hexdigest() == \"3ce1f42f1c88457c878430a27cbeb6f909b0785c\", \"boolean value of lm is None is not correct\"\n\nassert sha1(str(type(type(lm))).encode(\"utf-8\")+b\"72c67\").hexdigest() == \"1d32ce0e00e2e0a8dd67995b0fad71c00083aa05\", \"type of type(lm) is not correct\"\nassert sha1(str(type(lm)).encode(\"utf-8\")+b\"72c67\").hexdigest() == \"c91e4bde80d303487c49aae5c34b1f381a58f9c1\", \"value of type(lm) is not correct\"\n\nprint('Success!')\n\nQuestion 2.3.1 {points: 1}\nAfter we have created our linear regression model, the next step is to fit the training dataset.\nAssign your answer to an object named lm_fit.\n\n# ___ = ___.fit(___, ___)\n\n# your code here\nraise NotImplementedError\nlm_fit\n\n\nfrom hashlib import sha1\nassert sha1(str(type(lm_fit is None)).encode(\"utf-8\")+b\"ad3bc\").hexdigest() == \"1def08aec5fd67069ff460655ea29adfcd0cb743\", \"type of lm_fit is None is not bool. lm_fit is None should be a bool\"\nassert sha1(str(lm_fit is None).encode(\"utf-8\")+b\"ad3bc\").hexdigest() == \"fa8f0226d403f32ba484b227d8cc4455061bc019\", \"boolean value of lm_fit is None is not correct\"\n\nassert sha1(str(type(type(lm_fit))).encode(\"utf-8\")+b\"ad3bd\").hexdigest() == \"4366e1fb6059f520d285ba77171d1b1744923f34\", \"type of type(lm_fit) is not correct\"\nassert sha1(str(type(lm_fit)).encode(\"utf-8\")+b\"ad3bd\").hexdigest() == \"86f605051db98874d8cf0eb9c0db6fd94496091f\", \"value of type(lm_fit) is not correct\"\n\nassert sha1(str(type(lm_fit.coef_)).encode(\"utf-8\")+b\"ad3be\").hexdigest() == \"228ee7178527d0f2e5cff2853156700111ed8d69\", \"type of lm_fit.coef_ is not correct\"\nassert sha1(str(lm_fit.coef_).encode(\"utf-8\")+b\"ad3be\").hexdigest() == \"dfde7db1fb226af0937f0a18a269f587a6be23e6\", \"value of lm_fit.coef_ is not correct\"\n\nassert sha1(str(type(lm_fit.intercept_)).encode(\"utf-8\")+b\"ad3bf\").hexdigest() == \"6823fdcd989c2ea0c1c36707732f9cba4a6b0553\", \"type of lm_fit.intercept_ is not correct\"\nassert sha1(str(lm_fit.intercept_).encode(\"utf-8\")+b\"ad3bf\").hexdigest() == \"f6636d47b2984c4b1e045b86c33e93392da3738e\", \"value of lm_fit.intercept_ is not correct\"\n\nprint('Success!')\n\nQuestion 2.4  {points: 1}\nNow, let’s visualize the model predictions as a straight line overlaid on the training data. Use the predict function of lm to create predictions for the marathon_training data. Then, add the column of predictions to the marathon_training data frame using the assign function. Name the resulting data frame marathon_preds and the new column predictions.\nNext, create a scatterplot with the marathon time (y-axis) against the maximum distance run per week (x-axis) from marathon_preds. Use mark_circle with an opacity of 0.4 to avoid overplotting. Assign your plot to a variable called marathon_plot. Plot the predictions as a black line over the data points. Remember the fundamentals of effective visualizations such as having a human-readable axes titles.\nName your plot marathon_plot.\n\n# marathon_preds = ____.assign(\n#     predictions= _____.predict(____)\n# )\n# scatterplot = ___\n#\n# marathon_plot = scatterplot + ___.mark_line(___).encode(___)\n\n# your code here\nraise NotImplementedError\nmarathon_plot\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_preds is None)).encode(\"utf-8\")+b\"e4358\").hexdigest() == \"bc2dd580b40ad4a5e8f6594b34d9ed807fc56664\", \"type of marathon_preds is None is not bool. marathon_preds is None should be a bool\"\nassert sha1(str(marathon_preds is None).encode(\"utf-8\")+b\"e4358\").hexdigest() == \"02df6a309e4c0c99b29a5ebbbf4dc40df215e117\", \"boolean value of marathon_preds is None is not correct\"\n\nassert sha1(str(type(marathon_preds)).encode(\"utf-8\")+b\"e4359\").hexdigest() == \"65a340844bf360cea4cd1679e420c55d09e7829e\", \"type of type(marathon_preds) is not correct\"\n\nassert sha1(str(type(marathon_preds.shape)).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"94b8396ef806348aa6e6d9b48d968db839e0de43\", \"type of marathon_preds.shape is not tuple. marathon_preds.shape should be a tuple\"\nassert sha1(str(len(marathon_preds.shape)).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"80b67edab105b410e9f659d46edd8aafd252ea78\", \"length of marathon_preds.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_preds.shape))).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"c199087afcc62232805f8ca370d7d5302e4062f1\", \"values of marathon_preds.shape are not correct\"\nassert sha1(str(marathon_preds.shape).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"75a932b02b8b81bcd148774ce58e3466adaa9909\", \"order of elements of marathon_preds.shape is not correct\"\n\nassert sha1(str(type(\"predictions\" in marathon_preds.columns)).encode(\"utf-8\")+b\"e435b\").hexdigest() == \"7b0e6ae601f0ebf430c4e607354a84912f34ebc0\", \"type of \\\"predictions\\\" in marathon_preds.columns is not bool. \\\"predictions\\\" in marathon_preds.columns should be a bool\"\nassert sha1(str(\"predictions\" in marathon_preds.columns).encode(\"utf-8\")+b\"e435b\").hexdigest() == \"d146815eae7ededa977915471ab4ffbe2dd98791\", \"boolean value of \\\"predictions\\\" in marathon_preds.columns is not correct\"\n\nassert sha1(str(type(sum(marathon_preds.predictions))).encode(\"utf-8\")+b\"e435c\").hexdigest() == \"3531d0a530b71a74d474021791451f589648ab04\", \"type of sum(marathon_preds.predictions) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_preds.predictions), 2)).encode(\"utf-8\")+b\"e435c\").hexdigest() == \"6c69e3cb1469494214b9c3c30df43792dcd4283e\", \"value of sum(marathon_preds.predictions) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(marathon_preds.time_hrs))).encode(\"utf-8\")+b\"e435d\").hexdigest() == \"f6d32fcdfb7d338ca64c10d130c219848d79c187\", \"type of sum(marathon_preds.time_hrs) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_preds.time_hrs), 2)).encode(\"utf-8\")+b\"e435d\").hexdigest() == \"2f02cb1ab8ae3a31440a49ce11b661ff8fe9147e\", \"value of sum(marathon_preds.time_hrs) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(marathon_plot is None)).encode(\"utf-8\")+b\"e435e\").hexdigest() == \"6895d3cddb99e4147ba45a6c81bc4ddc364e23c2\", \"type of marathon_plot is None is not bool. marathon_plot is None should be a bool\"\nassert sha1(str(marathon_plot is None).encode(\"utf-8\")+b\"e435e\").hexdigest() == \"5390df15c8c3f55e9daedb39794d57638e99efee\", \"boolean value of marathon_plot is None is not correct\"\n\nassert sha1(str(type(len(marathon_plot.layer))).encode(\"utf-8\")+b\"e435f\").hexdigest() == \"e97a9a1e7ad3c233095b4ba0008121bc6a74261d\", \"type of len(marathon_plot.layer) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(marathon_plot.layer)).encode(\"utf-8\")+b\"e435f\").hexdigest() == \"2b20945255b0f2a7695411e709eafe5b6b888d2f\", \"value of len(marathon_plot.layer) is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[0].mark)).encode(\"utf-8\")+b\"e4360\").hexdigest() == \"94d874e6d384f6f20dee36b57d02ee0e9d835e2d\", \"type of marathon_plot.layer[0].mark is not correct\"\nassert sha1(str(marathon_plot.layer[0].mark).encode(\"utf-8\")+b\"e4360\").hexdigest() == \"a944ff98b1e615df7fa19be2866a054377367251\", \"value of marathon_plot.layer[0].mark is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[1].mark)).encode(\"utf-8\")+b\"e4361\").hexdigest() == \"71a33d1d974bc8658efed76d2ca77308b690d2fd\", \"type of marathon_plot.layer[1].mark is not correct\"\nassert sha1(str(marathon_plot.layer[1].mark).encode(\"utf-8\")+b\"e4361\").hexdigest() == \"a277cc73edf5fe1ef0dbef797094351e1114f778\", \"value of marathon_plot.layer[1].mark is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"d8ab0ac01648e3309ae4d80e94515a5364bb5499\", \"type of marathon_plot.layer[0].encoding.x['shorthand'] is not str. marathon_plot.layer[0].encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"839d138b1eeed4328365ef5af63e42c0910bb888\", \"length of marathon_plot.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"c6293c84c9e09e66386aeb65a91fcd8df6601e09\", \"value of marathon_plot.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.x['shorthand']).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"c6293c84c9e09e66386aeb65a91fcd8df6601e09\", \"correct string value of marathon_plot.layer[0].encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"3fd3774342c69f8858f9acea94a03c9890a18a93\", \"type of marathon_plot.layer[0].encoding.y['shorthand'] is not str. marathon_plot.layer[0].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"c5b64aeb2742f503eb3854087c4f836b06acede1\", \"length of marathon_plot.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"06f9ff5b96ee6e75cd8cf966d5552fe7f717f260\", \"value of marathon_plot.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.y['shorthand']).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"06f9ff5b96ee6e75cd8cf966d5552fe7f717f260\", \"correct string value of marathon_plot.layer[0].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"14b5ec8e3ca9e9418bdbbaa77a9b3aa179999fe9\", \"type of marathon_plot.layer[1].encoding.y['shorthand'] is not str. marathon_plot.layer[1].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"6f6fc84ebe98dc81fb989889721610b969b2e700\", \"length of marathon_plot.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[1].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"6ae5bc9df9fd334f0eac6345e13a01b9dd019844\", \"value of marathon_plot.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[1].encoding.y['shorthand']).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"6ae5bc9df9fd334f0eac6345e13a01b9dd019844\", \"correct string value of marathon_plot.layer[1].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(isinstance(marathon_plot.layer[0].encoding.x['title'], str))).encode(\"utf-8\")+b\"e4365\").hexdigest() == \"6d1dfcd98beb4541c755ed9ee5b65dbd667070f0\", \"type of isinstance(marathon_plot.layer[0].encoding.x['title'], str) is not bool. isinstance(marathon_plot.layer[0].encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot.layer[0].encoding.x['title'], str)).encode(\"utf-8\")+b\"e4365\").hexdigest() == \"7b15eabe489e4ffa5a44043c97d017234a7e0a0a\", \"boolean value of isinstance(marathon_plot.layer[0].encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_plot.layer[0].encoding.y['title'], str))).encode(\"utf-8\")+b\"e4366\").hexdigest() == \"f0651e9d8742d2b0750aea688fcf11ea2e3383f8\", \"type of isinstance(marathon_plot.layer[0].encoding.y['title'], str) is not bool. isinstance(marathon_plot.layer[0].encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot.layer[0].encoding.y['title'], str)).encode(\"utf-8\")+b\"e4366\").hexdigest() == \"0e91055a5f12d74e8593f359e143862b6a8fc193\", \"boolean value of isinstance(marathon_plot.layer[0].encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 2.5  {points: 1}\nGreat! We can now see the line of best fit on the graph. Now let’s calculate the RMSPE using the test data. To get to this point, first, use the lm object to make predictions on the test data. Then, add the column of predictions to the marathon_testing data frame using the assign function. Name the resulting data frame test_preds and the new column predictions.\nAfterwards, calculate the RMSPE using the mean_squared_error function.\nAssign the RMSPE score to an object called lm_rmspe.\n\n# ___ = ___.assign(\n#     predictions=___.predict(___)\n# )\n\n# ___ = ___(___, ___)**(1/2)\n\n# your code here\nraise NotImplementedError\nlm_rmspe\n\n\nfrom hashlib import sha1\nassert sha1(str(type(test_preds is None)).encode(\"utf-8\")+b\"3ebdc\").hexdigest() == \"29ebee7ab2061555cdd2621507b3792dbb886297\", \"type of test_preds is None is not bool. test_preds is None should be a bool\"\nassert sha1(str(test_preds is None).encode(\"utf-8\")+b\"3ebdc\").hexdigest() == \"6a7c108f74c9bf5ddfe1618a204e71dd5069e8c5\", \"boolean value of test_preds is None is not correct\"\n\nassert sha1(str(type(test_preds)).encode(\"utf-8\")+b\"3ebdd\").hexdigest() == \"763191835b51c258b842454c271700f9cdfea3bf\", \"type of type(test_preds) is not correct\"\n\nassert sha1(str(type(test_preds.shape)).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"89d3f1293bc4be750513c8cdb8c60056333d8f96\", \"type of test_preds.shape is not tuple. test_preds.shape should be a tuple\"\nassert sha1(str(len(test_preds.shape)).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"bc721a860c2268ce910d4702796d17031d8ea8a8\", \"length of test_preds.shape is not correct\"\nassert sha1(str(sorted(map(str, test_preds.shape))).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"1a83b659bfa801824e9221c8426149a4b5fa861b\", \"values of test_preds.shape are not correct\"\nassert sha1(str(test_preds.shape).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"bd984a5c94d60104a3e575235235426ccfbc98c7\", \"order of elements of test_preds.shape is not correct\"\n\nassert sha1(str(type(sum(test_preds.predictions))).encode(\"utf-8\")+b\"3ebdf\").hexdigest() == \"a072e028d9a3b80e8f68ddb270a40eb499596870\", \"type of sum(test_preds.predictions) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(test_preds.predictions), 2)).encode(\"utf-8\")+b\"3ebdf\").hexdigest() == \"71c296aa4d1218d1231b125f36f9a6c915f67cf1\", \"value of sum(test_preds.predictions) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(lm_rmspe is None)).encode(\"utf-8\")+b\"3ebe0\").hexdigest() == \"dd5ba914f2ed74bf83a35c83506ab2ef26e21ed8\", \"type of lm_rmspe is None is not bool. lm_rmspe is None should be a bool\"\nassert sha1(str(lm_rmspe is None).encode(\"utf-8\")+b\"3ebe0\").hexdigest() == \"750b26eac59f5780869eb7f85afe415614161310\", \"boolean value of lm_rmspe is None is not correct\"\n\nassert sha1(str(type(lm_rmspe)).encode(\"utf-8\")+b\"3ebe1\").hexdigest() == \"21a6df5760ffaceab2771ac0da61c372b6ed6d25\", \"type of type(lm_rmspe) is not correct\"\n\nassert sha1(str(type(round(lm_rmspe, 1))).encode(\"utf-8\")+b\"3ebe2\").hexdigest() == \"9a9ddc39cbb7a45670e4d1dd2b4ad91714ad8852\", \"type of round(lm_rmspe, 1) is not correct\"\nassert sha1(str(round(lm_rmspe, 1)).encode(\"utf-8\")+b\"3ebe2\").hexdigest() == \"a6d4449e0d5e3251aa0d7523dcbda18946bf141d\", \"value of round(lm_rmspe, 1) is not correct\"\n\nprint('Success!')\n\nQuestion 2.5.1  {points: 1}\nNow, let’s visualize the model predictions as a straight line overlaid on the test data. First, create a scatterplot to assess the relationship between race time (time_hrs) and maximum distance ran per week during training (max) on the testing data. Use mark_circle with an opacity of 0.4 to avoid overplotting. Then add a line to the plot corresponding to the predictions (predictions) from the fit linear regression model. Remember to do whatever is necessary to make this an effective visualization.\nAssign the plot to an object called marathon_plot_test.\n\n# marathon_plot = ___\n\n# your code here\nraise NotImplementedError\nmarathon_plot_test\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_plot_test is None)).encode(\"utf-8\")+b\"61e8f\").hexdigest() == \"ee748f5ffeeccfa151a95c1c8e40fb2e51b1ce9f\", \"type of marathon_plot_test is None is not bool. marathon_plot_test is None should be a bool\"\nassert sha1(str(marathon_plot_test is None).encode(\"utf-8\")+b\"61e8f\").hexdigest() == \"f71b2edba5243e8d1a90ce0d7e7b37557a0f0052\", \"boolean value of marathon_plot_test is None is not correct\"\n\nassert sha1(str(type(len(marathon_plot_test.layer))).encode(\"utf-8\")+b\"61e90\").hexdigest() == \"6d39d35690606462cf06c5a978d8b727d3090082\", \"type of len(marathon_plot_test.layer) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(marathon_plot_test.layer)).encode(\"utf-8\")+b\"61e90\").hexdigest() == \"421355b186989ec09a7f6e161cda4600cd1021bb\", \"value of len(marathon_plot_test.layer) is not correct\"\n\nassert sha1(str(type(marathon_plot_test.layer[0].mark)).encode(\"utf-8\")+b\"61e91\").hexdigest() == \"3246755d74b7833a9ea578910cdf8696d3fce524\", \"type of marathon_plot_test.layer[0].mark is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].mark).encode(\"utf-8\")+b\"61e91\").hexdigest() == \"bf51585cca690c54f425c1a947e05ea8faa39710\", \"value of marathon_plot_test.layer[0].mark is not correct\"\n\nassert sha1(str(type(marathon_plot_test.layer[1].mark)).encode(\"utf-8\")+b\"61e92\").hexdigest() == \"fed14b1460cbd668b78fe2a24b76c721a7b6ef35\", \"type of marathon_plot_test.layer[1].mark is not correct\"\nassert sha1(str(marathon_plot_test.layer[1].mark).encode(\"utf-8\")+b\"61e92\").hexdigest() == \"7c3f41c9edb9129aef9b6afbb5afbd53b5a21bc3\", \"value of marathon_plot_test.layer[1].mark is not correct\"\n\nassert sha1(str(type(marathon_plot_test.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"b36d5e994a7b12c3f08e6d34610ef7745b04a68b\", \"type of marathon_plot_test.layer[0].encoding.x['shorthand'] is not str. marathon_plot_test.layer[0].encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot_test.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"6ab52032148d76cb4c992137ba98a5642ccd1c07\", \"length of marathon_plot_test.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"f5d25df889e84991486b73bd90133cc33805a2c7\", \"value of marathon_plot_test.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.x['shorthand']).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"f5d25df889e84991486b73bd90133cc33805a2c7\", \"correct string value of marathon_plot_test.layer[0].encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot_test.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"ddef04a4b677f36a2461ae87bd64c145cbe414e7\", \"type of marathon_plot_test.layer[0].encoding.y['shorthand'] is not str. marathon_plot_test.layer[0].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot_test.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"d12e59210898a0cbdd59886ed5b66fe72ef419a5\", \"length of marathon_plot_test.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"0528e9c8beb5841dd4f6ea3e120aa2ea6851f6d3\", \"value of marathon_plot_test.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.y['shorthand']).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"0528e9c8beb5841dd4f6ea3e120aa2ea6851f6d3\", \"correct string value of marathon_plot_test.layer[0].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot_test.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"393f83ca4126abac28e3721499e74cbc8eec49f5\", \"type of marathon_plot_test.layer[1].encoding.y['shorthand'] is not str. marathon_plot_test.layer[1].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot_test.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"cf6648e5023cea85b35e732f848817cbef50a2ea\", \"length of marathon_plot_test.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[1].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"482407e92cdb21525edc25dc9759ecd8f28ac4b0\", \"value of marathon_plot_test.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[1].encoding.y['shorthand']).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"482407e92cdb21525edc25dc9759ecd8f28ac4b0\", \"correct string value of marathon_plot_test.layer[1].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(isinstance(marathon_plot_test.layer[0].encoding.x['title'], str))).encode(\"utf-8\")+b\"61e96\").hexdigest() == \"4e9a552197c0d9f091c287c51186481faa373b84\", \"type of isinstance(marathon_plot_test.layer[0].encoding.x['title'], str) is not bool. isinstance(marathon_plot_test.layer[0].encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot_test.layer[0].encoding.x['title'], str)).encode(\"utf-8\")+b\"61e96\").hexdigest() == \"b5a959a717c4d36b796c0351445d67ceba59555b\", \"boolean value of isinstance(marathon_plot_test.layer[0].encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_plot_test.layer[0].encoding.y['title'], str))).encode(\"utf-8\")+b\"61e97\").hexdigest() == \"69d55630d9b08b127c64a06a01079ec5fd719e2c\", \"type of isinstance(marathon_plot_test.layer[0].encoding.y['title'], str) is not bool. isinstance(marathon_plot_test.layer[0].encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot_test.layer[0].encoding.y['title'], str)).encode(\"utf-8\")+b\"61e97\").hexdigest() == \"9669df5114cc0166cef2d5b5f35d9333a937d24f\", \"boolean value of isinstance(marathon_plot_test.layer[0].encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 2.6  {points: 1}\nCompare the RMSPE of k-nn regression (0.616 from last worksheet) to that of simple linear regression. Which is greater?\nA. Simple linear regression has a greater RMSPE\nB. \\(k\\)-nn regression has a greater RMSPE\nC. Neither, they are identical\nSave the letter of your answer to a variable named answer3_6. Make sure you put quotations around the letter and pay attention to case.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer3_6)).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"3cebb18cc78a764b9b957667aa3b539e088a53b6\", \"type of answer3_6 is not str. answer3_6 should be an str\"\nassert sha1(str(len(answer3_6)).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"e2ea38993eafe6e4258dd8f2732bc8f10bf05850\", \"length of answer3_6 is not correct\"\nassert sha1(str(answer3_6.lower()).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"d8fc4778f9ae37aeb0d7eebc142c30f17a58afcf\", \"value of answer3_6 is not correct\"\nassert sha1(str(answer3_6).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"3469bf05b66975e09572d8f6cebca168c20a4543\", \"correct string value of answer3_6 but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 2.7  {points: 1}\nWhich model does a better job of predicting on the test dataset?\nA. Simple linear regression\nB. \\(k\\)-nn regression\nC. Neither, they are identical\nSave the letter of your answer to a variable named answer3_7. Make sure you put quotations around the letter and pay attention to case.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer3_7)).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"cbde8d789af13720b9730aa64643b32c579e43c8\", \"type of answer3_7 is not str. answer3_7 should be an str\"\nassert sha1(str(len(answer3_7)).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"1f1de87eb3b5d616cdc5148f63387a030af2dcdd\", \"length of answer3_7 is not correct\"\nassert sha1(str(answer3_7.lower()).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"9500e6ffe1848679b170175d4752de72899e0f9e\", \"value of answer3_7 is not correct\"\nassert sha1(str(answer3_7).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"e7041b0f0cc00781dddf503ef1693bd99029eedf\", \"correct string value of answer3_7 but incorrect case of letters\"\n\nprint('Success!')\n\nGiven that the linear regression model is a straight line, we can write our model as a mathematical equation. We can get the two numbers we need for this from the coef_ and intercept_ attributes from lm_fit.\n\n# run this cell\nprint(f\"The coefficient for the linear regression is {lm_fit.coef_[0]:0.3f}.\")\nprint(f\"The intercept for the linear regression is {lm_fit.intercept_:0.3f}.\")\n\nQuestion 2.8.1  {points: 1}\nWhich of the following mathematical equations represents the model based on the numbers output in the cell above?\nA. \\(Predicted \\ race \\ time \\ (in \\ hours) = 4.851 - 0.022  * max \\ (in \\ miles)\\)\nB. \\(Predicted \\ race \\ time \\ (in \\ hours) = -0.022 + 4.851 * max \\ (in \\ miles)\\)\nC. \\(Predicted \\ max \\ (in \\ miles) = 4.851 - 0.022 *  \\ race \\ time \\ (in \\ hours)\\)\nD. \\(Predicted \\ max \\ (in \\ miles) = -0.022 + 4.851 *  \\ race \\ time \\ (in \\ hours)\\)\nSave the letter of your answer to a variable named answer3_8_1. Make sure you put quotations around the letter and pay attention to case.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer3_8_1)).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"c1e898538331c6feb698ad544c8e708eb101f335\", \"type of answer3_8_1 is not str. answer3_8_1 should be an str\"\nassert sha1(str(len(answer3_8_1)).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"4f4756a152ba2c274ee3b194dbe295ac57e849ac\", \"length of answer3_8_1 is not correct\"\nassert sha1(str(answer3_8_1.lower()).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"11099d2bab2f6a29a949f290697d5e5906cece6b\", \"value of answer3_8_1 is not correct\"\nassert sha1(str(answer3_8_1).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"8a5fd0b1b4b71ea33d160ad2d610f6af5c67f963\", \"correct string value of answer3_8_1 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html",
    "title": "Worksheet - Classification (Part II)",
    "section": "",
    "text": "After completing this workshop session, you will be able to:\n\nDescribe what a test data set is and how it is used in classification.\nUnderstand several ways of representing classifier performance: accuracy, precision, and recall, and the confusion matrix.\nUsing Python, evaluate classifier performance using a test data set and appropriate metrics.\nUsing Python, execute cross-validation in Python to choose the number of neighbours.\nIdentify when it is necessary to scale variables before classification and do this using Python\nIn a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in Python using the scikit-learn package to predict the class of a test dataset.\nDescribe advantages and disadvantages of the k-nearest neighbour classification algorithm.\n\nThis worksheet covers parts of Chapter 6 of the online textbook. You should read this chapter before attempting this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell!\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")\n\nQuestion 0.1 Multiple Choice:\nThe confusion matrix is:\nA. A way to confuse you.\nB. A table where rows correspond to predicted class and columns correspond to true class.\nC. Each cell in the confusion matrix displays the number of observations with a particular predicted/true class as given by the row and column labels.\nD. Is an important tool for understanding what type of mistakes a classifier makes and how often these mistakes happen.\nE. All of the above except A.\nAssign your answer to an object called answer0_1. Make sure your answer is an uppercase letter and is surrounded by quotation marks (e.g. \"F\").\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_1)).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"351b6021c8f87571484eee4e4c7bf0a0b7ed21fc\", \"type of answer0_1 is not str. answer0_1 should be an str\"\nassert sha1(str(len(answer0_1)).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"439859842b2ff5343f142aebc166bbd2f5c1ba63\", \"length of answer0_1 is not correct\"\nassert sha1(str(answer0_1.lower()).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"738521fce4199fe135093d56bdcd11e5af3694cb\", \"value of answer0_1 is not correct\"\nassert sha1(str(answer0_1).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"f04c8abf26aed0bfc1e3cac0304301b24f47827f\", \"correct string value of answer0_1 but incorrect case of letters\"\n\nprint('Success!')\n\nSuccess!\n\n\nQuestion 0.2 Multiple Choice:\nPrecision and recall are ways to summarize the confusion matrix. What is something we must do before calculating precision and recall?\nA. Turn the values (counts of observations) appearing in each cell of the table into a proportion.\nB. Choose one of the class label as being more interesting and equate that with the “positive” label.\nC. Flip the column and rows of the matrix.\nD. None of the above.\nAssign your answer to an object called answer0_2. Make sure your answer is an uppercase letter and is surrounded by quotation marks (e.g. \"F\").\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_2)).encode(\"utf-8\")+b\"40f78\").hexdigest() == \"88fbc36d74dbc11e3dc49cd606c9d12aab8dadde\", \"type of answer0_2 is not str. answer0_2 should be an str\"\nassert sha1(str(len(answer0_2)).encode(\"utf-8\")+b\"40f78\").hexdigest() == \"8f13c1689337a5e773751fc0a20a001db5754a5a\", \"length of answer0_2 is not correct\"\nassert sha1(str(answer0_2.lower()).encode(\"utf-8\")+b\"40f78\").hexdigest() == \"904faf5c0cc90042d3f8b8e6064bae76b809e406\", \"value of answer0_2 is not correct\"\nassert sha1(str(answer0_2).encode(\"utf-8\")+b\"40f78\").hexdigest() == \"a586864d6d403b52cf8043aaeda3ab9125c88230\", \"correct string value of answer0_2 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#fruit-data-example---part-ii",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#fruit-data-example---part-ii",
    "title": "Worksheet - Classification (Part II)",
    "section": "1. Fruit Data Example - (Part II)",
    "text": "1. Fruit Data Example - (Part II)\nQuestion 1.0\nIn the agricultural industry, cleaning, sorting, grading, and packaging food products are all necessary tasks in the post-harvest process. Products are classified based on appearance, size and shape, attributes which helps determine the quality of the food. Sorting can be done by humans, but it is tedious and time consuming. Automatic sorting could help save time and money. Images of the food products are captured and analysed to determine visual characteristics.\nThe dataset contains observations of fruit described with four features: (1) mass (in g), (2) width (in cm), (3) height (in cm), and (4) color score (on a scale from 0 - 1).\nTo get started building a classifier that can classfiy a fruit based on its appearance, use pd.read_csv to load the file fruit_data.csv (found in the data folder) from the previous tutorial into your notebook.\nAssign your data to an object called fruit_data.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_data is None)).encode(\"utf-8\")+b\"8fbd4\").hexdigest() == \"94dbd13369ff838be1cc37ddd2571e56a2adbad6\", \"type of fruit_data is None is not bool. fruit_data is None should be a bool\"\nassert sha1(str(fruit_data is None).encode(\"utf-8\")+b\"8fbd4\").hexdigest() == \"32b4f132c7e4d7fd9a4568ae950c0ade0f91c972\", \"boolean value of fruit_data is None is not correct\"\n\nassert sha1(str(type(fruit_data.shape)).encode(\"utf-8\")+b\"8fbd5\").hexdigest() == \"8bc91c167da94224c2c03ff132c43919b3ec3977\", \"type of fruit_data.shape is not tuple. fruit_data.shape should be a tuple\"\nassert sha1(str(len(fruit_data.shape)).encode(\"utf-8\")+b\"8fbd5\").hexdigest() == \"e95430ca7bc9a404e2b167dfe8a30b4055fc9bcd\", \"length of fruit_data.shape is not correct\"\nassert sha1(str(sorted(map(str, fruit_data.shape))).encode(\"utf-8\")+b\"8fbd5\").hexdigest() == \"89046d3b818261b39a3c972f538928b8817bb9ba\", \"values of fruit_data.shape are not correct\"\nassert sha1(str(fruit_data.shape).encode(\"utf-8\")+b\"8fbd5\").hexdigest() == \"9a07d2fbcd6742a34bdf9d9a0f7b4d90d70fe81b\", \"order of elements of fruit_data.shape is not correct\"\n\nassert sha1(str(type(fruit_data.fruit_name.dtype)).encode(\"utf-8\")+b\"8fbd6\").hexdigest() == \"77c18e2fcbfeb18070e7059fdc3a6049fd40025b\", \"type of fruit_data.fruit_name.dtype is not correct\"\nassert sha1(str(fruit_data.fruit_name.dtype).encode(\"utf-8\")+b\"8fbd6\").hexdigest() == \"226ebd072ebdd8910b1c851f48d365fc91a37987\", \"value of fruit_data.fruit_name.dtype is not correct\"\n\nprint('Success!')\n\nLet’s take a look at the first few observations in the fruit dataset. Run the cell below.\n\n# Run this cell.\nfruit_data.head()\n\nNow let’s investigate the class proportions for each kind of fruit:\n\nfruit_data['fruit_name'].value_counts(normalize=True)"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#randomness-and-setting-seeds",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#randomness-and-setting-seeds",
    "title": "Worksheet - Classification (Part II)",
    "section": "Randomness and Setting Seeds",
    "text": "Randomness and Setting Seeds\nThis worksheet uses functions from the scikit-learn library, which not only allows us to perform K-nearest neighbour classification, but also allows us to evaluate how well our classification worked. In order to ensure that the steps in the worksheet are reproducible, we need to set a random_state or random seed, i.e., a numerical “starting value,” which determines the sequence of random numbers Python will generate.\nBelow in many cells we have included an argument to set the random_state or np.random.seed. They are necessary to make sure the autotesting code functions properly."
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#splitting-the-data-into-a-training-and-test-set",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#splitting-the-data-into-a-training-and-test-set",
    "title": "Worksheet - Classification (Part II)",
    "section": "2. Splitting the data into a training and test set",
    "text": "2. Splitting the data into a training and test set\nIn this exercise, we will be partitioning fruit_data into a training (75%) and testing (25%) set using the scikit-learn package. After creating the test set, we will put the test set away in a lock box and not touch it again until we have found the best k-nn classifier we can make using the training set. We will use the variable fruit_name as our class label.\nQuestion 2.0\nTo create the training and test set, we would use the train_test_split function from scikit-learn package. Save the trained dataset and test dataset as fruit_train and fruit_test, respectively.\n\n# Randomly take 75% of the data in the training set.\n# This will be proportional to the different number of fruit names in the dataset.\n\n# ___, ___ = train_test_split(___, test_size=___, random_state=123) # set the random state to be 123\n\n# your code here\nraise NotImplementedError\nfruit_train\n\n\nfruit_test\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_train is None)).encode(\"utf-8\")+b\"ef8b1\").hexdigest() == \"9f77123ff7006f15c739b7b78f5f2d4de80459f5\", \"type of fruit_train is None is not bool. fruit_train is None should be a bool\"\nassert sha1(str(fruit_train is None).encode(\"utf-8\")+b\"ef8b1\").hexdigest() == \"7ed9c577a01bef83e68375a03929f6c1cba7755b\", \"boolean value of fruit_train is None is not correct\"\n\nassert sha1(str(type(fruit_test is None)).encode(\"utf-8\")+b\"ef8b2\").hexdigest() == \"b0edefb6186b34edf34d83f7bcf5cbb59e46ea56\", \"type of fruit_test is None is not bool. fruit_test is None should be a bool\"\nassert sha1(str(fruit_test is None).encode(\"utf-8\")+b\"ef8b2\").hexdigest() == \"96404ff84888a103c5f891759de786517fe317e2\", \"boolean value of fruit_test is None is not correct\"\n\nassert sha1(str(type(fruit_train.shape)).encode(\"utf-8\")+b\"ef8b3\").hexdigest() == \"ad58db68d7f455473cab66cff03bdba319b16f11\", \"type of fruit_train.shape is not tuple. fruit_train.shape should be a tuple\"\nassert sha1(str(len(fruit_train.shape)).encode(\"utf-8\")+b\"ef8b3\").hexdigest() == \"1d7f9a2b05a377135a71d8c34ecb0a5926601bb4\", \"length of fruit_train.shape is not correct\"\nassert sha1(str(sorted(map(str, fruit_train.shape))).encode(\"utf-8\")+b\"ef8b3\").hexdigest() == \"9e1e7bdacecb3a487b64e0f64d1f61366a6ea4fe\", \"values of fruit_train.shape are not correct\"\nassert sha1(str(fruit_train.shape).encode(\"utf-8\")+b\"ef8b3\").hexdigest() == \"342376346f8cf46351b13bed9b0160a140dd9fbb\", \"order of elements of fruit_train.shape is not correct\"\n\nassert sha1(str(type(fruit_test.shape)).encode(\"utf-8\")+b\"ef8b4\").hexdigest() == \"c00a1e9b6f64d1629e4f464c1935eda7b0f96279\", \"type of fruit_test.shape is not tuple. fruit_test.shape should be a tuple\"\nassert sha1(str(len(fruit_test.shape)).encode(\"utf-8\")+b\"ef8b4\").hexdigest() == \"6c6e5d05f1db9c0aef7b920ba2cfd609e1a2d2d1\", \"length of fruit_test.shape is not correct\"\nassert sha1(str(sorted(map(str, fruit_test.shape))).encode(\"utf-8\")+b\"ef8b4\").hexdigest() == \"ade72264f11f0c795f1ca06d6df199248d2fda40\", \"values of fruit_test.shape are not correct\"\nassert sha1(str(fruit_test.shape).encode(\"utf-8\")+b\"ef8b4\").hexdigest() == \"191ee842577393f5745b2c256304dcdd2895b4a6\", \"order of elements of fruit_test.shape is not correct\"\n\nassert sha1(str(type(sum(fruit_train.mass))).encode(\"utf-8\")+b\"ef8b5\").hexdigest() == \"d43052ac9a4c7109e3fd472091a2a67541dc4590\", \"type of sum(fruit_train.mass) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(fruit_train.mass)).encode(\"utf-8\")+b\"ef8b5\").hexdigest() == \"03b793f8288b890907d114552b291e4cb42a95f0\", \"value of sum(fruit_train.mass) is not correct\"\n\nassert sha1(str(type(sum(fruit_test.mass))).encode(\"utf-8\")+b\"ef8b6\").hexdigest() == \"21310037fbeb70908577535bbc83ff4f6b2a975a\", \"type of sum(fruit_test.mass) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(fruit_test.mass)).encode(\"utf-8\")+b\"ef8b6\").hexdigest() == \"baabdedf68419227ffd00e67f81a343e9c77d3e5\", \"value of sum(fruit_test.mass) is not correct\"\n\nprint('Success!')\n\nQuestion 2.1\nK-nearest neighbors is sensitive to the scale of the predictors so we should do some preprocessing to standardize them. Remember that standardizing involves centering/shifting (subtracting the mean of each variable) and scaling (dividing by its standard deviation). Also remember that standardization is part of your training procedure, so you can’t use your test data to compute the centered / scaled values for each variable. Therefore, you must pass only the training data to your preprocessor to compute the preprocessing steps. This ensures that our test data does not influence any aspect of our model training. Once we have created the standardization preprocessor, we can then later on apply it separately to both the training and test data sets.\nFor this exercise, let’s see if mass and color_score can predict fruit_name.\nTo scale and center the data, first, pass the predictors to the make_column_transformer function to make the preprocessor.\nAssign your answer to an object called fruit_preprocessor.\n\n# ___ = make_column_transformer(\n#     (___, [___, ___]),\n#     verbose_feature_names_out=False\n# )\n\n# your code here\nraise NotImplementedError\nfruit_preprocessor\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_preprocessor is None)).encode(\"utf-8\")+b\"36d09\").hexdigest() == \"c16043f1e011ae5e24d73f767d244e153de052d1\", \"type of fruit_preprocessor is None is not bool. fruit_preprocessor is None should be a bool\"\nassert sha1(str(fruit_preprocessor is None).encode(\"utf-8\")+b\"36d09\").hexdigest() == \"a36c675b8869eb47d14d616bcfc17bf0679c1480\", \"boolean value of fruit_preprocessor is None is not correct\"\n\nassert sha1(str(type(type(fruit_preprocessor))).encode(\"utf-8\")+b\"36d0a\").hexdigest() == \"5b4a6271e3d19f04ad8e3a02389bdde94d645e8f\", \"type of type(fruit_preprocessor) is not correct\"\nassert sha1(str(type(fruit_preprocessor)).encode(\"utf-8\")+b\"36d0a\").hexdigest() == \"33fc23392075e2159cb275f5b072c2bd3eef06f2\", \"value of type(fruit_preprocessor) is not correct\"\n\nassert sha1(str(type(fruit_preprocessor.transformers[0][0])).encode(\"utf-8\")+b\"36d0b\").hexdigest() == \"c2ee407b203b6e1c1faf4f5bbefb51099c64d2fa\", \"type of fruit_preprocessor.transformers[0][0] is not str. fruit_preprocessor.transformers[0][0] should be an str\"\nassert sha1(str(len(fruit_preprocessor.transformers[0][0])).encode(\"utf-8\")+b\"36d0b\").hexdigest() == \"b66e860a0a8f53af64caf60afdbf123a56eddbfa\", \"length of fruit_preprocessor.transformers[0][0] is not correct\"\nassert sha1(str(fruit_preprocessor.transformers[0][0].lower()).encode(\"utf-8\")+b\"36d0b\").hexdigest() == \"b0dd09d0d913f060dfae366a65bdc73cc4c6dbe9\", \"value of fruit_preprocessor.transformers[0][0] is not correct\"\nassert sha1(str(fruit_preprocessor.transformers[0][0]).encode(\"utf-8\")+b\"36d0b\").hexdigest() == \"b0dd09d0d913f060dfae366a65bdc73cc4c6dbe9\", \"correct string value of fruit_preprocessor.transformers[0][0] but incorrect case of letters\"\n\nassert sha1(str(type(fruit_preprocessor.transformers[0][2])).encode(\"utf-8\")+b\"36d0c\").hexdigest() == \"462915beb3245eb3f238c0e6100eba8c60d5eeeb\", \"type of fruit_preprocessor.transformers[0][2] is not list. fruit_preprocessor.transformers[0][2] should be a list\"\nassert sha1(str(len(fruit_preprocessor.transformers[0][2])).encode(\"utf-8\")+b\"36d0c\").hexdigest() == \"dbcd1005f772edc467986417157dca8731db4c76\", \"length of fruit_preprocessor.transformers[0][2] is not correct\"\nassert sha1(str(sorted(map(str, fruit_preprocessor.transformers[0][2]))).encode(\"utf-8\")+b\"36d0c\").hexdigest() == \"280af5a8b2775780fabf4d84c5f5db2cfdbdbe56\", \"values of fruit_preprocessor.transformers[0][2] are not correct\"\nassert sha1(str(fruit_preprocessor.transformers[0][2]).encode(\"utf-8\")+b\"36d0c\").hexdigest() == \"2b1472a2c7fea5769561b8e3eef648c0052e5e8a\", \"order of elements of fruit_preprocessor.transformers[0][2] is not correct\"\n\nprint('Success!')\n\nNow that we have split the data, we can do things like exploratory data analysis and model fitting. Before we move onto the latter, run the cell below to visualize the two of the predictors (mass in grams, and width in cm) as a scatter plot, colouring the observations by their class labels.\n\n# Create the scatterplot\nfruit_chart = alt.Chart(fruit_data).mark_point(size=15).encode(\n    x=alt.X(\"mass\").title(\"Mass (grams)\"),\n    y=alt.Y(\"width\")\n        .title(\"Width (cm)\")\n        .scale(zero=False),\n    color=alt.Color(\"fruit_name\").title(\"Fruit\")\n)\n\nfruit_chart\n\nQuestion 2.2\nSo far, we have split the training and testing datasets as well as preprocessed the data. Now, let’s create our K-nearest neighbour classifier with only the training set using the scikit-learn package. First, create the classifier by specifying that we want \\(K = 3\\) neighbors. Assign your answer to an object called knn_spec.\nNext, separate the predictor columns from the target column. Name the predictor variable X and the target y.\nTrain the classifier with the training data set using the make_pipeline and fit function. The make_pipeline function allows you to bundle together your pre-processing, modeling, and post-processing requests. Scaffolding is provided below for you.\nAssign your answer to an object called fruit_fit.\n\n# ___ = KNeighborsClassifier(n_neighbors=___)\n\n# ___ = ___[[\"mass\", \"color_score\"]]\n# ___ = fruit_train[___]\n\n# ___ = make_pipeline(___, ___).fit(___, ___)\n\n# your code here\nraise NotImplementedError\nfruit_fit\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_spec is None)).encode(\"utf-8\")+b\"27e07\").hexdigest() == \"e2d16a9547e72a3ae1e7aef7c84c6c0c587bad37\", \"type of knn_spec is None is not bool. knn_spec is None should be a bool\"\nassert sha1(str(knn_spec is None).encode(\"utf-8\")+b\"27e07\").hexdigest() == \"a15dcfacc29248935e7777a673f2a5eb8c9afe27\", \"boolean value of knn_spec is None is not correct\"\n\nassert sha1(str(type(type(knn_spec))).encode(\"utf-8\")+b\"27e08\").hexdigest() == \"8192fb1b7fe569d5b3c2f3eab3fb65464e75bfb1\", \"type of type(knn_spec) is not correct\"\nassert sha1(str(type(knn_spec)).encode(\"utf-8\")+b\"27e08\").hexdigest() == \"69a8d47f5a15872b58b9d89610eb519425f3fde2\", \"value of type(knn_spec) is not correct\"\n\nassert sha1(str(type(knn_spec.effective_metric_)).encode(\"utf-8\")+b\"27e09\").hexdigest() == \"8a3cc99092881053a4372e18346c2e25f3f9c98e\", \"type of knn_spec.effective_metric_ is not str. knn_spec.effective_metric_ should be an str\"\nassert sha1(str(len(knn_spec.effective_metric_)).encode(\"utf-8\")+b\"27e09\").hexdigest() == \"70f6652e73aa1506c93346ed63d995b98fab1b15\", \"length of knn_spec.effective_metric_ is not correct\"\nassert sha1(str(knn_spec.effective_metric_.lower()).encode(\"utf-8\")+b\"27e09\").hexdigest() == \"ca0f4ad54d67bcd18ff83d85539d213a6136c949\", \"value of knn_spec.effective_metric_ is not correct\"\nassert sha1(str(knn_spec.effective_metric_).encode(\"utf-8\")+b\"27e09\").hexdigest() == \"ca0f4ad54d67bcd18ff83d85539d213a6136c949\", \"correct string value of knn_spec.effective_metric_ but incorrect case of letters\"\n\nassert sha1(str(type(knn_spec.n_neighbors)).encode(\"utf-8\")+b\"27e0a\").hexdigest() == \"b053dd855a7ee23c1a8f2db8de53400908f659ab\", \"type of knn_spec.n_neighbors is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_spec.n_neighbors).encode(\"utf-8\")+b\"27e0a\").hexdigest() == \"b726c76a82a1e689bd8bf1a87e14148b1e1ffa28\", \"value of knn_spec.n_neighbors is not correct\"\n\nassert sha1(str(type(sum(X.mass))).encode(\"utf-8\")+b\"27e0b\").hexdigest() == \"1d6efa7539e497d2785a27892fb55925101adc69\", \"type of sum(X.mass) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(X.mass)).encode(\"utf-8\")+b\"27e0b\").hexdigest() == \"acf33445c79fe8216094e045df98b2d792ffd530\", \"value of sum(X.mass) is not correct\"\n\nassert sha1(str(type(sum(X.color_score))).encode(\"utf-8\")+b\"27e0c\").hexdigest() == \"9d9694d1c2272d3f4d59434862483d10ab72a496\", \"type of sum(X.color_score) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X.color_score), 2)).encode(\"utf-8\")+b\"27e0c\").hexdigest() == \"ae307b97500019064af59ef4e7b7ed62c89d5454\", \"value of sum(X.color_score) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(y.name)).encode(\"utf-8\")+b\"27e0d\").hexdigest() == \"6abe773b05068bd331b9458fe572e23efd0ba854\", \"type of y.name is not str. y.name should be an str\"\nassert sha1(str(len(y.name)).encode(\"utf-8\")+b\"27e0d\").hexdigest() == \"90f4248559ecfae4d207bd46db84457909a6af5f\", \"length of y.name is not correct\"\nassert sha1(str(y.name.lower()).encode(\"utf-8\")+b\"27e0d\").hexdigest() == \"92c4a59d1166ec03a74d8d9e0e7550629d7e35d5\", \"value of y.name is not correct\"\nassert sha1(str(y.name).encode(\"utf-8\")+b\"27e0d\").hexdigest() == \"92c4a59d1166ec03a74d8d9e0e7550629d7e35d5\", \"correct string value of y.name but incorrect case of letters\"\n\nassert sha1(str(type(fruit_fit is None)).encode(\"utf-8\")+b\"27e0e\").hexdigest() == \"15099bad8aa8fd88e3ed8b0336fcd45b8f39af37\", \"type of fruit_fit is None is not bool. fruit_fit is None should be a bool\"\nassert sha1(str(fruit_fit is None).encode(\"utf-8\")+b\"27e0e\").hexdigest() == \"cc6d59b8d76d220ac7ceeac94e7f984f2f6aacdb\", \"boolean value of fruit_fit is None is not correct\"\n\nassert sha1(str(type(type(fruit_fit))).encode(\"utf-8\")+b\"27e0f\").hexdigest() == \"7b4a4056a45db874cc809a3221d59cca0c09005c\", \"type of type(fruit_fit) is not correct\"\nassert sha1(str(type(fruit_fit)).encode(\"utf-8\")+b\"27e0f\").hexdigest() == \"dd6490f704dd393427bfc2cc32a050c152dd221d\", \"value of type(fruit_fit) is not correct\"\n\nassert sha1(str(type(len(fruit_fit.named_steps))).encode(\"utf-8\")+b\"27e10\").hexdigest() == \"2f62b5548ecff5633a52f4591fa7a95747b18374\", \"type of len(fruit_fit.named_steps) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(fruit_fit.named_steps)).encode(\"utf-8\")+b\"27e10\").hexdigest() == \"da25296125b548d8848520ab24f87345dd290640\", \"value of len(fruit_fit.named_steps) is not correct\"\n\nassert sha1(str(type(fruit_fit.named_steps.keys())).encode(\"utf-8\")+b\"27e11\").hexdigest() == \"281cfae204ddee5837c5ebe161f9bcdcaa605d2e\", \"type of fruit_fit.named_steps.keys() is not correct\"\nassert sha1(str(fruit_fit.named_steps.keys()).encode(\"utf-8\")+b\"27e11\").hexdigest() == \"6287d8aeb5a0ee86fcdc595d4a94d5ed18c48017\", \"value of fruit_fit.named_steps.keys() is not correct\"\n\nprint('Success!')\n\nQuestion 2.3\nNow that we have created our K-nearest neighbor classifier object, let’s predict the class labels for our test set.\nWe want to make sure to assign the predicted class labels to a new column in the dataframe, called predicted. To create the predicted class labels pass your fitted model pipeline and the test dataset to the predict function.\nAssign your answer to an object called fruit_test_predictions.\n\n# ___ = fruit_test.___(\n#     predicted=___.predict(___[[___, ___]])\n# )\n\n# your code here\nraise NotImplementedError\nfruit_test_predictions\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_test_predictions is None)).encode(\"utf-8\")+b\"2f2bd\").hexdigest() == \"2b270d171753a33179223403dd1ff41c35a42b87\", \"type of fruit_test_predictions is None is not bool. fruit_test_predictions is None should be a bool\"\nassert sha1(str(fruit_test_predictions is None).encode(\"utf-8\")+b\"2f2bd\").hexdigest() == \"a4451e8e45f322126a6fbc7a90c20ae3a5bee464\", \"boolean value of fruit_test_predictions is None is not correct\"\n\nassert sha1(str(type(fruit_test_predictions)).encode(\"utf-8\")+b\"2f2be\").hexdigest() == \"11449a9d181d481214112ee5255a27c33852aaac\", \"type of type(fruit_test_predictions) is not correct\"\n\nassert sha1(str(type(fruit_test_predictions.shape)).encode(\"utf-8\")+b\"2f2bf\").hexdigest() == \"2d059656a62390dea1fb448bbdc215c77a9817f5\", \"type of fruit_test_predictions.shape is not tuple. fruit_test_predictions.shape should be a tuple\"\nassert sha1(str(len(fruit_test_predictions.shape)).encode(\"utf-8\")+b\"2f2bf\").hexdigest() == \"c98f20a3aacd58713ce44462685524951a0d6beb\", \"length of fruit_test_predictions.shape is not correct\"\nassert sha1(str(sorted(map(str, fruit_test_predictions.shape))).encode(\"utf-8\")+b\"2f2bf\").hexdigest() == \"269abef7b85b70a76ac2028e7198760edbf0102b\", \"values of fruit_test_predictions.shape are not correct\"\nassert sha1(str(fruit_test_predictions.shape).encode(\"utf-8\")+b\"2f2bf\").hexdigest() == \"ccdebed5e7deb5b68c4991d2b4130e87ba2f5bb8\", \"order of elements of fruit_test_predictions.shape is not correct\"\n\nassert sha1(str(type(\"predicted\" in fruit_test_predictions.columns)).encode(\"utf-8\")+b\"2f2c0\").hexdigest() == \"95b9c71897a9496eda9fa4e37c40745a4b6017c6\", \"type of \\\"predicted\\\" in fruit_test_predictions.columns is not bool. \\\"predicted\\\" in fruit_test_predictions.columns should be a bool\"\nassert sha1(str(\"predicted\" in fruit_test_predictions.columns).encode(\"utf-8\")+b\"2f2c0\").hexdigest() == \"c3cace2eeb4178326381514950fac3f2606c7de7\", \"boolean value of \\\"predicted\\\" in fruit_test_predictions.columns is not correct\"\n\nprint('Success!')\n\nQuestion 2.4\nGreat! We have now computed some predictions for our test datasets! From glancing at the dataframe above, it looks like most of them are correct, but wouldn’t it be interesting if we could find out the exact accuracy of our classifier?\nThankfully, the score function from the scikit-learn package can help us. To get the statistics about the quality of our model, you need to call the score function on the fruit_fit model. Name the predictors X_test and the target y_test. We should pass the X_test and y_test into the score function.\nAssign your answer to an object called fruit_prediction_accuracy.\n\n# ___ = ___[[___, ___]]\n# ___ = ___[\"fruit_name\"]\n\n# ___ = fruit_fit.score(___, ___)\n\n# your code here\nraise NotImplementedError\nfruit_prediction_accuracy\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_prediction_accuracy is None)).encode(\"utf-8\")+b\"58ea0\").hexdigest() == \"7488706d3b954877e08217e6c10b6e56e6a7b906\", \"type of fruit_prediction_accuracy is None is not bool. fruit_prediction_accuracy is None should be a bool\"\nassert sha1(str(fruit_prediction_accuracy is None).encode(\"utf-8\")+b\"58ea0\").hexdigest() == \"2ede9eb19115d81336d63c4f466910ed740a56cc\", \"boolean value of fruit_prediction_accuracy is None is not correct\"\n\nassert sha1(str(type(fruit_prediction_accuracy)).encode(\"utf-8\")+b\"58ea1\").hexdigest() == \"d1b9e2d27e4f648284fe90f1e995ff37b15b3b52\", \"type of fruit_prediction_accuracy is not correct\"\nassert sha1(str(fruit_prediction_accuracy).encode(\"utf-8\")+b\"58ea1\").hexdigest() == \"cba4f4b8508049ee804d50c214f02a7d03ec456b\", \"value of fruit_prediction_accuracy is not correct\"\n\nprint('Success!')\n\nQuestion 2.5\nNow, let’s look at the confusion matrix for the classifier. This will show us a table comparing the predicted labels with the true labels.\nA confusion matrix is essentially a classification matrix. The columns of the confusion matrix represent the actual class and the rows represent the predicted class (or vice versa). Shown below is an example of a confusion matrix.\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nTruly Positive\nTrue Positive\nFalse Negative\n\n\nTruly Negative\nFalse Positive\nTrue Negative\n\n\n\n\nA true positive is an outcome where the model correctly predicts the positive class.\nA true negative is an outcome where the model correctly predicts the negative class.\nA false positive is an outcome where the model incorrectly predicts the positive class.\nA false negative is an outcome where the model incorrectly predicts the negative class.\n\n\nWe can create a confusion matrix by using the crosstab function from pandas. In the dataframe created by crosstab, the true labels will be to the left, and the predicted labels will be on top (as in the matrix above). In contrast to the confusion matrix above where there are only two possible outcomes (positive/negative), we have four possible outcomes (the four fruit names). Therefore, our dataframe will be bigger than the matrix above and contain 16 possible outcomes instead of 4.\nAssign your answer to an object called fruit_mat.\n\n# ___ = pd.___(\n#     fruit_test_predictions[___],  # True labels\n#     fruit_test_predictions[___],  # Predicted labels\n# )\n\n# your code here\nraise NotImplementedError\nfruit_mat\n\nWith many observations, it can be difficult to interpret the confusion matrix when it is presented as a table like above. In these cases, we could instead use the ConfusionMatrixDisplay function of the scikit-learn package to visualize the confusion matrix as a heatmap. Please run the cell below to see the fruit confusion matrix as a heatmap.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(\n    fruit_fit,  # We are directly passing the pipeline and let sklearn do the predictions for us\n    X_test,\n    y_test\n)\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_mat is None)).encode(\"utf-8\")+b\"6d49c\").hexdigest() == \"a56e492407ffda77dae01cf693d7cec3160d24a5\", \"type of fruit_mat is None is not bool. fruit_mat is None should be a bool\"\nassert sha1(str(fruit_mat is None).encode(\"utf-8\")+b\"6d49c\").hexdigest() == \"c7f4085c7df1435511dabd1fe07a5e3f0bf6a04a\", \"boolean value of fruit_mat is None is not correct\"\n\nassert sha1(str(type(fruit_mat)).encode(\"utf-8\")+b\"6d49d\").hexdigest() == \"28c9794a963e59bc5a7cf363c4c0e2789544fc33\", \"type of type(fruit_mat) is not correct\"\n\nassert sha1(str(type(fruit_mat.to_numpy().sum())).encode(\"utf-8\")+b\"6d49e\").hexdigest() == \"01e9653b50d767f4025c03eca241b30e06583876\", \"type of fruit_mat.to_numpy().sum() is not correct\"\nassert sha1(str(fruit_mat.to_numpy().sum()).encode(\"utf-8\")+b\"6d49e\").hexdigest() == \"bf73171c60376a7a7ab9275366e3d8a810042a38\", \"value of fruit_mat.to_numpy().sum() is not correct\"\n\nprint('Success!')\n\nQuestion 2.6 Multiple Choice:\nReading fruit_mat, how many observations were labelled correctly?\nA. 7\nB. 8\nC. 9\nD. 14\nAssign your answer to an object called answer2_6. Make sure your answer is an uppercase letter and is surrounded by quotation marks (e.g. \"F\").\n\n# your code here\nraise NotImplementedError\nanswer2_6\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer2_6)).encode(\"utf-8\")+b\"63bd7\").hexdigest() == \"74364102b77b3fd097894f4b2ed5ffe1a3bedb6e\", \"type of answer2_6 is not str. answer2_6 should be an str\"\nassert sha1(str(len(answer2_6)).encode(\"utf-8\")+b\"63bd7\").hexdigest() == \"aa9b3cc37bc9e2a6492756385bfc9db9b3b3ffad\", \"length of answer2_6 is not correct\"\nassert sha1(str(answer2_6.lower()).encode(\"utf-8\")+b\"63bd7\").hexdigest() == \"ee28d9dab7da140c2703a3142b1a76eca2f29704\", \"value of answer2_6 is not correct\"\nassert sha1(str(answer2_6).encode(\"utf-8\")+b\"63bd7\").hexdigest() == \"f37d9ce6ca5d36961515b5c2a132fd356075f5f3\", \"correct string value of answer2_6 but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 2.7 Multiple Choice:\nReading fruit_mat, let’s suppse that we are really interested in the lemons, and treat “lemon” as being the “positive” class. What is the precision of our classifier?\nAssign your answer to an object called answer2_7.\n\n# your code here\nraise NotImplementedError\nanswer2_7\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer2_7)).encode(\"utf-8\")+b\"cf09a\").hexdigest() == \"a2acc89ec6bf6fbfcaf70c4f3fd8e257714b28f5\", \"type of answer2_7 is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(answer2_7, 2)).encode(\"utf-8\")+b\"cf09a\").hexdigest() == \"6e0748f97ae0f02ee4986a895c656d7199300d4c\", \"value of answer2_7 is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 2.8 Multiple Choice:\nAgain, let us treat “lemon” as being the “positive” class. What is the recall of our classifier?\nAssign your answer to an object called answer2_8.\n\n# your code here\nraise NotImplementedError\nanswer2_8\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer2_8)).encode(\"utf-8\")+b\"51283\").hexdigest() == \"458dc6e2f39b0dc333b972f861a30a8b8840373e\", \"type of answer2_8 is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(answer2_8, 2)).encode(\"utf-8\")+b\"51283\").hexdigest() == \"2769552e2d149c93b991ad429cb50a9b3b1877a6\", \"value of answer2_8 is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\n\n3. Cross-validation\nQuestion 3.1\nThe vast majority of predictive models in statistics and machine learning have parameters that you have to pick. For the past few exercises, we have had to pick the number of neighbours for the class vote, which we have done arbitraily. But, is it possible to make this selection, i.e., tune the model, in a principled way? Ideally, we want to pick the number of neighborurs to maximize the performance of our classifier on data it hasn’t seen yet.\nAn important aspect of the tuning process is that we can, if we want to, split our training data again, train and evaluate a classifier for each split, and then choose the parameter based on all of the different results. If we just split our training data once, our best parameter choice will depend strongly on the randomness from how this single split was made. Using multiple different splits, we’ll get a more robust estimate of accuracy, which will lead to a more suitable choice of the number of neighbours \\(K\\) to perform well on unseen data.\nThe idea of training and evaluating models on multiple training data splits times is called “cross-validation”. In cross-validation, we split our overall training data into \\(C\\) evenly-sized chunks, and then iteratively use 1 chunk as the validation set and combine the remaining \\(C−1\\) chunks as the training set. The validation set is used in a similar was as the test set, except that the test set is only used once at the end to report model performance whether we use model performance on the validation set to select the model during cross-validation.\n\nWe can perform a cross-validation in Python using the cross_validate function from the scikit-learn package. To use this function, you have to identify the model, the training set, and specify the cv parameter (the number of folds \\(C\\), defaults to 5). We should set return_train_score to be True to return the training score as well.\nBefore we use the cross_validate function, we need to perform the pipeline analysis again. You can reuse the X and y variables you constructed from the training data earlier, as well as the fruit_preprocessorand knn_spec variables. However, you will need to create a new pipeline since the one we made earlier is already fitted on all the data and here we want to fit it on different splits of the data during cross-validation. Since the cross_validate function outputs a dictionary, we use pd.DataFrame to convert it to a dataframe for convenience, as in the textbook.\nAssign your answer to an object called fruit_vfold_score.\n\nnp.random.seed(2020)  # DO NOT REMOVE\n\n# ___ = ___(fruit_preprocessor, knn_spec)\n# ___ = pd.___(\n#     cross_validate(\n#         estimator=___,\n#         cv=5,\n#         X=___,\n#         y=___,\n#         return_train_score=True,\n#     )\n# )\n\n# your code here\nraise NotImplementedError\nfruit_vfold_score\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_vfold_score is None)).encode(\"utf-8\")+b\"36a22\").hexdigest() == \"b1fc58d5e23c9423bbf8765714e5e58804fce66a\", \"type of fruit_vfold_score is None is not bool. fruit_vfold_score is None should be a bool\"\nassert sha1(str(fruit_vfold_score is None).encode(\"utf-8\")+b\"36a22\").hexdigest() == \"d2cc03f8ff44ef13491fa232070b6de92b811475\", \"boolean value of fruit_vfold_score is None is not correct\"\n\nassert sha1(str(type(fruit_vfold_score)).encode(\"utf-8\")+b\"36a23\").hexdigest() == \"d2dcb89307585b23fa4ca6f8a76f8c2c0f23346f\", \"type of type(fruit_vfold_score) is not correct\"\n\nassert sha1(str(type(fruit_vfold_score.shape)).encode(\"utf-8\")+b\"36a24\").hexdigest() == \"546d04c6fccac7326ab7400bb48a82c394c90130\", \"type of fruit_vfold_score.shape is not tuple. fruit_vfold_score.shape should be a tuple\"\nassert sha1(str(len(fruit_vfold_score.shape)).encode(\"utf-8\")+b\"36a24\").hexdigest() == \"d8cd8be65e2d915f492f7185bcd2736d2f68a197\", \"length of fruit_vfold_score.shape is not correct\"\nassert sha1(str(sorted(map(str, fruit_vfold_score.shape))).encode(\"utf-8\")+b\"36a24\").hexdigest() == \"653cb8e4264891bd349087f0a6ae4d134fd597c4\", \"values of fruit_vfold_score.shape are not correct\"\nassert sha1(str(fruit_vfold_score.shape).encode(\"utf-8\")+b\"36a24\").hexdigest() == \"89f531606aaf9ca14b74ef68296d4ab26838067f\", \"order of elements of fruit_vfold_score.shape is not correct\"\n\nassert sha1(str(type(fruit_pipe is None)).encode(\"utf-8\")+b\"36a25\").hexdigest() == \"9541e0d3962315e7cfa81d63c9ff1179e06dbe63\", \"type of fruit_pipe is None is not bool. fruit_pipe is None should be a bool\"\nassert sha1(str(fruit_pipe is None).encode(\"utf-8\")+b\"36a25\").hexdigest() == \"020dfe9202b64ad0e6861ad87373f929d736eefc\", \"boolean value of fruit_pipe is None is not correct\"\n\nassert sha1(str(type(type(fruit_pipe))).encode(\"utf-8\")+b\"36a26\").hexdigest() == \"fd97847bfa2e68ff8df95e6074f5a234c145fd58\", \"type of type(fruit_pipe) is not correct\"\nassert sha1(str(type(fruit_pipe)).encode(\"utf-8\")+b\"36a26\").hexdigest() == \"656864c949d89ae0be260be526c38fa4039065bd\", \"value of type(fruit_pipe) is not correct\"\n\nassert sha1(str(type(len(fruit_pipe.named_steps))).encode(\"utf-8\")+b\"36a27\").hexdigest() == \"bbcdb9c08b17d3d8a66fe4a53686f010ae0bccbb\", \"type of len(fruit_pipe.named_steps) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(fruit_pipe.named_steps)).encode(\"utf-8\")+b\"36a27\").hexdigest() == \"33ba12d9299d1f48e8d73ede50813d18a77ff874\", \"value of len(fruit_pipe.named_steps) is not correct\"\n\nassert sha1(str(type(fruit_pipe.named_steps.keys())).encode(\"utf-8\")+b\"36a28\").hexdigest() == \"ea5bb9eb37991a1ed3ecb7735822ec397000ce42\", \"type of fruit_pipe.named_steps.keys() is not correct\"\nassert sha1(str(fruit_pipe.named_steps.keys()).encode(\"utf-8\")+b\"36a28\").hexdigest() == \"577fdf89e2c84a15d65fa7cec8269b13b6314a4e\", \"value of fruit_pipe.named_steps.keys() is not correct\"\n\nprint('Success!')\n\nQuestion 3.2\nNow that we have ran a cross-validation on each train/validation split, one has to ask, how accurate was the classifier’s validation across the folds? We can aggregate the mean and standard error of these scores from each folds. The standard error is essentially a measure of how uncertain we are in the mean value. Use the agg dataframe method to compute both the mean and the standard error; make sure the first row of the dataframe contains the mean values and the second contains the standard error values.\nAssign your answer to an object called fruit_metrics.\n\n# ___ = fruit_vfold_score.___([___, ___])\n\n\n# your code here\nraise NotImplementedError\nfruit_metrics\n\n\nfrom hashlib import sha1\nassert sha1(str(type(fruit_metrics.shape)).encode(\"utf-8\")+b\"7608a\").hexdigest() == \"8e5f57d0f7ed0aa98c690da117537d080d1bff1a\", \"type of fruit_metrics.shape is not tuple. fruit_metrics.shape should be a tuple\"\nassert sha1(str(len(fruit_metrics.shape)).encode(\"utf-8\")+b\"7608a\").hexdigest() == \"f68c65a025bd3924f48305894fadea89e0c5b46e\", \"length of fruit_metrics.shape is not correct\"\nassert sha1(str(sorted(map(str, fruit_metrics.shape))).encode(\"utf-8\")+b\"7608a\").hexdigest() == \"bbca15d44552fa5e34cd23e46733915b6043b927\", \"values of fruit_metrics.shape are not correct\"\nassert sha1(str(fruit_metrics.shape).encode(\"utf-8\")+b\"7608a\").hexdigest() == \"f8ddb51af4648fc9c62d3c1d3a78fc8589ac74ca\", \"order of elements of fruit_metrics.shape is not correct\"\n\nassert sha1(str(type(fruit_metrics.test_score)).encode(\"utf-8\")+b\"7608b\").hexdigest() == \"4fae85143b5b445ce9b9807dfe9a1a379dc90033\", \"type of fruit_metrics.test_score is not correct\"\nassert sha1(str(fruit_metrics.test_score).encode(\"utf-8\")+b\"7608b\").hexdigest() == \"6924635698218860ac3c086773cb0acfa29dace4\", \"value of fruit_metrics.test_score is not correct\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#parameter-value-selection",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#parameter-value-selection",
    "title": "Worksheet - Classification (Part II)",
    "section": "4. Parameter value selection",
    "text": "4. Parameter value selection\nUsing a 5-fold cross-validation, we have established a prediction accuracy for our classifier. If we were to improve our classifier, we would like to try different number of neighbours, \\(K\\). Then we could use cross-validation to calculate an accuracy for each value of \\(K\\) in a reasonable range, and pick the value of \\(K\\) that gives us the best accuracy on the validation data.\nThe great thing about the scikit-learn package is that it provides functions to conveniently tune parameters such as \\(K\\) by training and evaluating models (via crossvalidation) for a range of specified values of \\(K\\). The function we will use here is called “exhaustive grid search” (sklearn.model_selection.GridSearchCV).\nQuestion 4.0\nCreate a new K-nearest neighbor model specification but instead of specifying a particular value for the n_neighbors argument, try exploring a range of values with GridSearchCV. Before we use GridSearchCV, we should define the grid of values that we want to explore, and redefine the pipeline without specifying a particular value of \\(K\\). To save us some time, instruct the grid search to use 4-fold cross-validation, rather than the default 5-fold.\nAssign your answer to an object called knn_tune_grid.\n\n### Run this cell\nparam_grid = {\n    \"kneighborsclassifier__n_neighbors\": range(2, 15, 1),\n}\nfruit_tune_pipe = make_pipeline(fruit_preprocessor, KNeighborsClassifier())\n\n\n# ___ = GridSearchCV(\n#     ___, ___, ___=__,\n# )\n\n\n# your code here\nraise NotImplementedError\nknn_tune_grid\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_tune_grid is None)).encode(\"utf-8\")+b\"de7e3\").hexdigest() == \"b8f0b73e45acd1ecde4311b54cb8ca5c4fd48116\", \"type of knn_tune_grid is None is not bool. knn_tune_grid is None should be a bool\"\nassert sha1(str(knn_tune_grid is None).encode(\"utf-8\")+b\"de7e3\").hexdigest() == \"22ff6a02d8a0748a0942a49f18a56f89c44071a4\", \"boolean value of knn_tune_grid is None is not correct\"\n\nassert sha1(str(type(type(knn_tune_grid))).encode(\"utf-8\")+b\"de7e4\").hexdigest() == \"bba351be309dda4ac68b4ebd1aafb92b79462f94\", \"type of type(knn_tune_grid) is not correct\"\nassert sha1(str(type(knn_tune_grid)).encode(\"utf-8\")+b\"de7e4\").hexdigest() == \"5daba4ef43b4a410d3c641b8d3b2e56daed064e6\", \"value of type(knn_tune_grid) is not correct\"\n\nassert sha1(str(type(knn_tune_grid.param_grid.keys())).encode(\"utf-8\")+b\"de7e5\").hexdigest() == \"ff297bec527b81362f07bc67cd01011f1add1a08\", \"type of knn_tune_grid.param_grid.keys() is not correct\"\nassert sha1(str(knn_tune_grid.param_grid.keys()).encode(\"utf-8\")+b\"de7e5\").hexdigest() == \"1c7370822141bce722ca65597af73993dba4a560\", \"value of knn_tune_grid.param_grid.keys() is not correct\"\n\nassert sha1(str(type(knn_tune_grid.estimator.named_steps.keys())).encode(\"utf-8\")+b\"de7e6\").hexdigest() == \"dd0e8c205b661479a0048e81ead835f12b2b5ddf\", \"type of knn_tune_grid.estimator.named_steps.keys() is not correct\"\nassert sha1(str(knn_tune_grid.estimator.named_steps.keys()).encode(\"utf-8\")+b\"de7e6\").hexdigest() == \"9fe3afc262d1fef9058a7a3342aa8dbc9a76685c\", \"value of knn_tune_grid.estimator.named_steps.keys() is not correct\"\n\nassert sha1(str(type(knn_tune_grid.cv)).encode(\"utf-8\")+b\"de7e7\").hexdigest() == \"947f9f92f52034f3258a2e920f111db2ce0fc28b\", \"type of knn_tune_grid.cv is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_tune_grid.cv).encode(\"utf-8\")+b\"de7e7\").hexdigest() == \"048c41e7d49846e06e4228f89a17df11f2732e82\", \"value of knn_tune_grid.cv is not correct\"\n\nprint('Success!')\n\nQuestion 4.1\nNow, let’s fit the grid search object to the data, using the X and y variables we created earlier.\nAssign your tuned model to a variable called knn_model_grid.\nNext, from knn_model_grid, find out the cv_results_ and save it in a dataframe.\nAssign your answer to a variable called accuracies_grid.\n\n# ___ = ___.fit(___, ___)\n\n# ___ = pd.DataFrame(___.cv_results_)\n\n# your code here\nraise NotImplementedError\naccuracies_grid\n\n\nfrom hashlib import sha1\nassert sha1(str(type(type(knn_model_grid))).encode(\"utf-8\")+b\"5aaa1\").hexdigest() == \"7800ffeeffbe3f109596728a651492f72ed697b9\", \"type of type(knn_model_grid) is not correct\"\nassert sha1(str(type(knn_model_grid)).encode(\"utf-8\")+b\"5aaa1\").hexdigest() == \"a777b6bc7a96e92dc37d7e48e532c4403b013440\", \"value of type(knn_model_grid) is not correct\"\n\nassert sha1(str(type(accuracies_grid is None)).encode(\"utf-8\")+b\"5aaa2\").hexdigest() == \"2bbc8cb94c17fac722104bbe0ae8178d6283ea7d\", \"type of accuracies_grid is None is not bool. accuracies_grid is None should be a bool\"\nassert sha1(str(accuracies_grid is None).encode(\"utf-8\")+b\"5aaa2\").hexdigest() == \"b31adcd31362204c2f193a8d747bac72afd4a47a\", \"boolean value of accuracies_grid is None is not correct\"\n\nassert sha1(str(type(accuracies_grid)).encode(\"utf-8\")+b\"5aaa3\").hexdigest() == \"a67afa6224ce1811c7d07895e39f81d73e309929\", \"type of type(accuracies_grid) is not correct\"\n\nassert sha1(str(type(accuracies_grid.shape)).encode(\"utf-8\")+b\"5aaa4\").hexdigest() == \"07ba20ae9b6524ef5c009590c807287bad2b6ca8\", \"type of accuracies_grid.shape is not tuple. accuracies_grid.shape should be a tuple\"\nassert sha1(str(len(accuracies_grid.shape)).encode(\"utf-8\")+b\"5aaa4\").hexdigest() == \"e362cae886bd94094b80f36e4d1c28b67e1ca7d7\", \"length of accuracies_grid.shape is not correct\"\nassert sha1(str(sorted(map(str, accuracies_grid.shape))).encode(\"utf-8\")+b\"5aaa4\").hexdigest() == \"2a86bd742543c1a3f2d3750c8ae31e564709e00e\", \"values of accuracies_grid.shape are not correct\"\nassert sha1(str(accuracies_grid.shape).encode(\"utf-8\")+b\"5aaa4\").hexdigest() == \"bcd625ee3af5d1cd105b2b2335288631494670e5\", \"order of elements of accuracies_grid.shape is not correct\"\n\nassert sha1(str(type(sum(accuracies_grid.mean_test_score))).encode(\"utf-8\")+b\"5aaa5\").hexdigest() == \"f04499ac18fececdea66994f437f6fe195a9a404\", \"type of sum(accuracies_grid.mean_test_score) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(accuracies_grid.mean_test_score), 2)).encode(\"utf-8\")+b\"5aaa5\").hexdigest() == \"fef13114978598bb2dd1245e857f4ad0cd359ce9\", \"value of sum(accuracies_grid.mean_test_score) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(accuracies_grid.std_test_score))).encode(\"utf-8\")+b\"5aaa6\").hexdigest() == \"90280827318185a637233f593624365d40176a3a\", \"type of sum(accuracies_grid.std_test_score) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(accuracies_grid.std_test_score), 2)).encode(\"utf-8\")+b\"5aaa6\").hexdigest() == \"7ea1bce420fa1d5037e56e6b0e1b99bf030cc962\", \"value of sum(accuracies_grid.std_test_score) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(accuracies_grid.param_kneighborsclassifier__n_neighbors))).encode(\"utf-8\")+b\"5aaa7\").hexdigest() == \"bcd0d8dac389990c3750e6af054592990b173ab3\", \"type of sum(accuracies_grid.param_kneighborsclassifier__n_neighbors) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(accuracies_grid.param_kneighborsclassifier__n_neighbors)).encode(\"utf-8\")+b\"5aaa7\").hexdigest() == \"8805dfb33692c14619c01c7dd91f5135cbda75dd\", \"value of sum(accuracies_grid.param_kneighborsclassifier__n_neighbors) is not correct\"\n\nprint('Success!')\n\nQuestion 4.2\nVisually inspecting the grid search results can help us find the best value for the number of neighbors parameter.\nCreate a line plot using the accuracies_grid dataframe with param_kneighborsclassifier__n_neighbors on the x-axis and the mean_test_score on the y-axis. Use point=True to include a point for each value of \\(K\\). Make it an effective visualization.\nAssign your answer to a variable called accuracy_versus_k_grid.\n\n# ___ = alt.Chart(___).mark_line(___).encode(\n#     x=alt.X(___)\n#         .title(___)\n#         .scale(zero=False),\n#     y=alt.Y(___)\n#         .title(___)\n#         .scale(zero=False)\n# )\n\n\n# your code here\nraise NotImplementedError\naccuracy_versus_k_grid\n\n\nfrom hashlib import sha1\nassert sha1(str(type(accuracy_versus_k_grid is None)).encode(\"utf-8\")+b\"6363\").hexdigest() == \"87aea9e6a7c50496055760ac5f8da1558cd36bbd\", \"type of accuracy_versus_k_grid is None is not bool. accuracy_versus_k_grid is None should be a bool\"\nassert sha1(str(accuracy_versus_k_grid is None).encode(\"utf-8\")+b\"6363\").hexdigest() == \"97d9d9c5a176cbd7e29b0dacb28ca2875d1fc330\", \"boolean value of accuracy_versus_k_grid is None is not correct\"\n\nassert sha1(str(type(accuracy_versus_k_grid.encoding.x['shorthand'])).encode(\"utf-8\")+b\"6364\").hexdigest() == \"e3175586c7aca8d91960689a5362be05a9843531\", \"type of accuracy_versus_k_grid.encoding.x['shorthand'] is not str. accuracy_versus_k_grid.encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(accuracy_versus_k_grid.encoding.x['shorthand'])).encode(\"utf-8\")+b\"6364\").hexdigest() == \"bc650671ac20b6c78d46c4d81c4da5fb776adf7a\", \"length of accuracy_versus_k_grid.encoding.x['shorthand'] is not correct\"\nassert sha1(str(accuracy_versus_k_grid.encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"6364\").hexdigest() == \"b40361992fe114502030798e70d94b33e1ebff13\", \"value of accuracy_versus_k_grid.encoding.x['shorthand'] is not correct\"\nassert sha1(str(accuracy_versus_k_grid.encoding.x['shorthand']).encode(\"utf-8\")+b\"6364\").hexdigest() == \"b40361992fe114502030798e70d94b33e1ebff13\", \"correct string value of accuracy_versus_k_grid.encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(accuracy_versus_k_grid.encoding.y['shorthand'])).encode(\"utf-8\")+b\"6365\").hexdigest() == \"359150eb18472065771be3fabdfe0bc17f14c652\", \"type of accuracy_versus_k_grid.encoding.y['shorthand'] is not str. accuracy_versus_k_grid.encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(accuracy_versus_k_grid.encoding.y['shorthand'])).encode(\"utf-8\")+b\"6365\").hexdigest() == \"a77f4185f8ba84217e96c473d78afe7defecd6d9\", \"length of accuracy_versus_k_grid.encoding.y['shorthand'] is not correct\"\nassert sha1(str(accuracy_versus_k_grid.encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"6365\").hexdigest() == \"888d0123d2b4b33cf32214184508a0981b6737c9\", \"value of accuracy_versus_k_grid.encoding.y['shorthand'] is not correct\"\nassert sha1(str(accuracy_versus_k_grid.encoding.y['shorthand']).encode(\"utf-8\")+b\"6365\").hexdigest() == \"888d0123d2b4b33cf32214184508a0981b6737c9\", \"correct string value of accuracy_versus_k_grid.encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(accuracy_versus_k_grid.mark)).encode(\"utf-8\")+b\"6366\").hexdigest() == \"4f70c15a9e4661590d768563d18d2bf38949d4ca\", \"type of accuracy_versus_k_grid.mark is not correct\"\nassert sha1(str(accuracy_versus_k_grid.mark).encode(\"utf-8\")+b\"6366\").hexdigest() == \"792bb3fbbb9b6f8d1cf136e30874108af9aac99e\", \"value of accuracy_versus_k_grid.mark is not correct\"\n\nassert sha1(str(type(accuracy_versus_k_grid.mark['point'])).encode(\"utf-8\")+b\"6367\").hexdigest() == \"734ecab84aea2bf10ce9cfa3ca4ebe42f82594bc\", \"type of accuracy_versus_k_grid.mark['point'] is not bool. accuracy_versus_k_grid.mark['point'] should be a bool\"\nassert sha1(str(accuracy_versus_k_grid.mark['point']).encode(\"utf-8\")+b\"6367\").hexdigest() == \"a0f7053f039367549a809452ba2ef7ef8eec5bec\", \"boolean value of accuracy_versus_k_grid.mark['point'] is not correct\"\n\nprint('Success!')\n\nFrom the plots above, we can see that \\(K = 2\\) or \\(3\\) provides the highest accuracy. Larger \\(K\\) values result in a reduced accuracy estimate. Remember: the values you see on this plot are estimates of the true accuracy of our classifier. Although this is the best information we have access to for what the ideal value of \\(K\\) would be, it is not a gurantee that the classifier will always be more accurate with this parameter value when it is used in practice!\nGreat, now you have completed a full analysis with cross-validation using the scikit-learn package! For your information, we can choose any number of folds and typically, the more we use the better our accuracy estimate will be (lower standard error). However, more folds would mean a greater computation time. In practice, \\(cv\\) is chosen to be either 5 or 10.\nDiscussion question: because we are learning, we did something in this worksheet we were not supposed to do, what was it?\n\nYour answer here"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression1/py_worksheet_regression1.html",
    "href": "materials/worksheets/py_worksheet_regression1/py_worksheet_regression1.html",
    "title": "Worksheet 8 - Regression",
    "section": "",
    "text": "After completing this week’s lecture and tutorial work, you will be able to: - Recognize situations where a simple regression analysis would be appropriate for making predictions. - Explain the k-nearest neighbour (\\(k\\)-nn) regression algorithm and describe how it differs from \\(k\\)-nn classification. - Interpret the output of a \\(k\\)-nn regression. - In a dataset with two variables, perform k-nearest neighbour regression in Python using scikit-learn to predict the values for a test dataset. - Using Python, execute hyperparameter tuning in Python to choose the number of neighbours. - Using Python, evaluate \\(k\\)-nn regression prediction accuracy using a test data set and an appropriate metric (root mean squared error). - In the context of \\(k\\)-nn regression, compare and contrast goodness of fit and prediction properties (RMSE versus RMSPE). - Describe advantages and disadvantages of the \\(k\\)-nearest neighbour regression approach.\nThis tutorial covers parts of Chapter 7 of the online textbook. You should read this chapter before attempting this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")\n\nQuestion 0.0  {points: 1}\nTo predict a value of \\(Y\\) for a new observation using \\(k\\)-nn regression, we identify the \\(k\\)-nearest neighbours and then:\nA. Assign it the median of the \\(k\\)-nearest neighbours as the predicted value\nB. Assign it the mean of the \\(k\\)-nearest neighbours as the predicted value\nC. Assign it the mode of the \\(k\\)-nearest neighbours as the predicted value\nD. Assign it the majority vote of the \\(k\\)-nearest neighbours as the predicted value\nSave the letter of the answer you think is correct to a variable named answer0_0. Make sure your answer is an uppercase letter and is surrounded by quotation marks (e.g. \"F\").\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_0)).encode(\"utf-8\")+b\"dcefd\").hexdigest() == \"3e545b3f6befc9fb348e77db7f2040e82e9bcbb7\", \"type of answer0_0 is not str. answer0_0 should be an str\"\nassert sha1(str(len(answer0_0)).encode(\"utf-8\")+b\"dcefd\").hexdigest() == \"107f1edd22c2860f5c219827900a4ddae78b3686\", \"length of answer0_0 is not correct\"\nassert sha1(str(answer0_0.lower()).encode(\"utf-8\")+b\"dcefd\").hexdigest() == \"d18bedfe24fae0dd4438932704ba3bd5d6bd4f7d\", \"value of answer0_0 is not correct\"\nassert sha1(str(answer0_0).encode(\"utf-8\")+b\"dcefd\").hexdigest() == \"0216e5aab479cd7c4e52506d6e7a964513305bd0\", \"correct string value of answer0_0 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression1/py_worksheet_regression1.html#marathon-training",
    "href": "materials/worksheets/py_worksheet_regression1/py_worksheet_regression1.html#marathon-training",
    "title": "Worksheet 8 - Regression",
    "section": "Marathon Training",
    "text": "Marathon Training\n\nSource: https://media.giphy.com/media/nUN6InE2CodRm/giphy.gif\nWhat predicts which athletes will perform better than others? Specifically, we are interested in marathon runners, and looking at how the maximum distance ran per week (in miles) during race training predicts the time it takes a runner to finish the race. For this, we will be looking at the marathon.csv file in the data/ folder.\nQuestion 1.0  {points: 1}\nLoad the data and assign it to an object called marathon.\n\n# your code here\nraise NotImplementedError\nmarathon\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon is None)).encode(\"utf-8\")+b\"30457\").hexdigest() == \"26d5aef6e71dd1f2188f7132ba3c7c26c732d4db\", \"type of marathon is None is not bool. marathon is None should be a bool\"\nassert sha1(str(marathon is None).encode(\"utf-8\")+b\"30457\").hexdigest() == \"598dc6a0d372aa6b4fab47451f4061e3c8d8d3ee\", \"boolean value of marathon is None is not correct\"\n\nassert sha1(str(type(marathon)).encode(\"utf-8\")+b\"30458\").hexdigest() == \"81b9d38b9594935ea28437a78a119c664b8e271a\", \"type of type(marathon) is not correct\"\n\nassert sha1(str(type(marathon.shape)).encode(\"utf-8\")+b\"30459\").hexdigest() == \"fc9069c39d96d09c086a2b82cb34971a576bcec5\", \"type of marathon.shape is not tuple. marathon.shape should be a tuple\"\nassert sha1(str(len(marathon.shape)).encode(\"utf-8\")+b\"30459\").hexdigest() == \"619cbada2e7f2f961d90ed765b5d118334e79c30\", \"length of marathon.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon.shape))).encode(\"utf-8\")+b\"30459\").hexdigest() == \"efe8921e7ce06759f11980cddc7ae55db277e49c\", \"values of marathon.shape are not correct\"\nassert sha1(str(marathon.shape).encode(\"utf-8\")+b\"30459\").hexdigest() == \"e3f7896715d8c8a8f7ada1fd2c5213c678367660\", \"order of elements of marathon.shape is not correct\"\n\nassert sha1(str(type(\"time_hrs\" in marathon.columns)).encode(\"utf-8\")+b\"3045a\").hexdigest() == \"3a3de902ed3e89879d57fc4eff706e656659352b\", \"type of \\\"time_hrs\\\" in marathon.columns is not bool. \\\"time_hrs\\\" in marathon.columns should be a bool\"\nassert sha1(str(\"time_hrs\" in marathon.columns).encode(\"utf-8\")+b\"3045a\").hexdigest() == \"f7e1f0c320e623dd359e1bd60d7a2017c4aef608\", \"boolean value of \\\"time_hrs\\\" in marathon.columns is not correct\"\n\nassert sha1(str(type(\"max\" in marathon.columns)).encode(\"utf-8\")+b\"3045b\").hexdigest() == \"dcbf21f70e6bbc9a4f84d3f0c06f12d13aafa0ac\", \"type of \\\"max\\\" in marathon.columns is not bool. \\\"max\\\" in marathon.columns should be a bool\"\nassert sha1(str(\"max\" in marathon.columns).encode(\"utf-8\")+b\"3045b\").hexdigest() == \"24b9f6b32bda0c76fd2aab9ff718c8c441f6a57b\", \"boolean value of \\\"max\\\" in marathon.columns is not correct\"\n\nprint('Success!')\n\nQuestion 2.0  {points: 1}\nWe want to predict race time (in hours) (time_hrs) given a particular value of maximum distance ran per week (in miles) during race training (max). Let’s take a subset of size 50 individuals of our marathon data and assign it to an object called marathon_50. With this subset, plot a scatterplot (using mark_circle) to assess the relationship between these two variables. Put time_hrs on the y-axis and max on the x-axis. Discuss, with a classmate, the relationship between race time and maximum distance ran per week during training based on the scatterplot you create below.\nHint: To take a subset of your data you can use the sample function\nAssign your plot to an object called answer2.\n\n# ___ = ___.sample(___, random_state=300) # Do not change the random_state\n\n\nanswer2 = alt.Chart(marathon_50).mark_circle().encode(\n    x=alt.X(\"max\").title(\"Max Distance Ran per Week During Training (miles)\"),\n    y=alt.Y(\"time_hrs\")\n        .title(\"Race Time (hours)\")\n        .scale(zero=False)\n)\n\nanswer2\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer2 is None)).encode(\"utf-8\")+b\"9767\").hexdigest() == \"c9f213a0f76bf1899a7c7ab41bd64020e00a5d8a\", \"type of answer2 is None is not bool. answer2 is None should be a bool\"\nassert sha1(str(answer2 is None).encode(\"utf-8\")+b\"9767\").hexdigest() == \"07c6d302a12aea10f94ce4ac438ed8846da04ee4\", \"boolean value of answer2 is None is not correct\"\n\nassert sha1(str(type(marathon_50.shape)).encode(\"utf-8\")+b\"9768\").hexdigest() == \"1dc6a7e0dc1faf456fc0a56183f1a383ab13ea95\", \"type of marathon_50.shape is not tuple. marathon_50.shape should be a tuple\"\nassert sha1(str(len(marathon_50.shape)).encode(\"utf-8\")+b\"9768\").hexdigest() == \"2211b0a1f1461259f232290064bd4f32d781712d\", \"length of marathon_50.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_50.shape))).encode(\"utf-8\")+b\"9768\").hexdigest() == \"2102806f043d7c36dcd6b39270f2bc3bf06645c2\", \"values of marathon_50.shape are not correct\"\nassert sha1(str(marathon_50.shape).encode(\"utf-8\")+b\"9768\").hexdigest() == \"38fe5c44247a3db3a62933578c3250c807ac8a1c\", \"order of elements of marathon_50.shape is not correct\"\n\nassert sha1(str(type(answer2.data.equals(marathon_50))).encode(\"utf-8\")+b\"9769\").hexdigest() == \"4cf4607212e9a4c28c631904ff116572eab419e5\", \"type of answer2.data.equals(marathon_50) is not bool. answer2.data.equals(marathon_50) should be a bool\"\nassert sha1(str(answer2.data.equals(marathon_50)).encode(\"utf-8\")+b\"9769\").hexdigest() == \"bbab45c34d4ff267cf8e57a05d36d09aa94cc953\", \"boolean value of answer2.data.equals(marathon_50) is not correct\"\n\nassert sha1(str(type(answer2.encoding.x['shorthand'])).encode(\"utf-8\")+b\"976a\").hexdigest() == \"839b1f6e497ebbed31fb8df0322b128a7b581e7b\", \"type of answer2.encoding.x['shorthand'] is not str. answer2.encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(answer2.encoding.x['shorthand'])).encode(\"utf-8\")+b\"976a\").hexdigest() == \"bbd3ee6c2cefcc8f64a4b15489f070e4e7360127\", \"length of answer2.encoding.x['shorthand'] is not correct\"\nassert sha1(str(answer2.encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"976a\").hexdigest() == \"5488fa8c72460ec31da324fd60582d6a75e894ac\", \"value of answer2.encoding.x['shorthand'] is not correct\"\nassert sha1(str(answer2.encoding.x['shorthand']).encode(\"utf-8\")+b\"976a\").hexdigest() == \"5488fa8c72460ec31da324fd60582d6a75e894ac\", \"correct string value of answer2.encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(answer2.encoding.y['shorthand'])).encode(\"utf-8\")+b\"976b\").hexdigest() == \"ad11bd6cff76d9856350485bf290e058bf89f9e0\", \"type of answer2.encoding.y['shorthand'] is not str. answer2.encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(answer2.encoding.y['shorthand'])).encode(\"utf-8\")+b\"976b\").hexdigest() == \"7e31068cc71027470db684645a245618ecc12f0b\", \"length of answer2.encoding.y['shorthand'] is not correct\"\nassert sha1(str(answer2.encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"976b\").hexdigest() == \"afd8e5f4040c0e6a927a56dd3222c5cb14c29b23\", \"value of answer2.encoding.y['shorthand'] is not correct\"\nassert sha1(str(answer2.encoding.y['shorthand']).encode(\"utf-8\")+b\"976b\").hexdigest() == \"afd8e5f4040c0e6a927a56dd3222c5cb14c29b23\", \"correct string value of answer2.encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(answer2.mark)).encode(\"utf-8\")+b\"976c\").hexdigest() == \"3623b4c339e9c6ee9fdc6fad96976a7d1a2d9557\", \"type of answer2.mark is not str. answer2.mark should be an str\"\nassert sha1(str(len(answer2.mark)).encode(\"utf-8\")+b\"976c\").hexdigest() == \"77446053854ce07f9a468e3c4253ee227343be8c\", \"length of answer2.mark is not correct\"\nassert sha1(str(answer2.mark.lower()).encode(\"utf-8\")+b\"976c\").hexdigest() == \"5b87400edb7ee707ab95967718e9b221d24e92e2\", \"value of answer2.mark is not correct\"\nassert sha1(str(answer2.mark).encode(\"utf-8\")+b\"976c\").hexdigest() == \"5b87400edb7ee707ab95967718e9b221d24e92e2\", \"correct string value of answer2.mark but incorrect case of letters\"\n\nassert sha1(str(type(isinstance(answer2.encoding.x['title'], str))).encode(\"utf-8\")+b\"976d\").hexdigest() == \"851319552403a87bee34ca83d924c7a895eb0db0\", \"type of isinstance(answer2.encoding.x['title'], str) is not bool. isinstance(answer2.encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(answer2.encoding.x['title'], str)).encode(\"utf-8\")+b\"976d\").hexdigest() == \"e82182bc48c146515fd8a30abc08ff860fa54ee5\", \"boolean value of isinstance(answer2.encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(answer2.encoding.y['title'], str))).encode(\"utf-8\")+b\"976e\").hexdigest() == \"265762b7f33189b160c25ee74148700738439490\", \"type of isinstance(answer2.encoding.y['title'], str) is not bool. isinstance(answer2.encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(answer2.encoding.y['title'], str)).encode(\"utf-8\")+b\"976e\").hexdigest() == \"8bb5960c415644b45508b0818b1eda7297426e4f\", \"boolean value of isinstance(answer2.encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 3.0  {points: 1}\nSuppose we want to predict the race time for someone who ran a maximum distance of 100 miles per week during training. In the chart we created in the previous question, we can see that no one has run a maximum distance of exactly 100 miles per week. How can we predict with this data? We can use \\(k\\)-nn regression! To do this we get the \\(Y\\) values (target/response variable) of the nearest \\(k\\) values and then take their average and use that as the prediction.\nFor this question perform \\(k\\)-nn regression manually to predict the race time based on the average value of the 4 runners (“neighbors”) closest to running 100 miles per week during training.\nFill in the scaffolding below and assign your answer to an object named answer3.\n\n# Run this cell to see a visualization of the 4 nearest neighbours to 100 miles / week\n\nrule = alt.Chart().mark_rule().encode(x=alt.datum(100))\n\nlines = alt.Chart(\n    pd.DataFrame({\n        \"x\": [110, 104, 90, 86],\n        \"y\": [2.63, 2.8, 3.27, 2.44]\n    })\n).mark_line(color=\"orange\", size=2).encode(\n    x=\"x\",\n    x2=alt.datum(100),  # we use `x2` to set a constant second x-coordinate at 100 for all the lines\n    y=\"y\",\n)\n\nlines + rule + answer2\n\n\n# ___ = (\n#     marathon_50\n#     .___(diff=(100 - ___).abs())  # Compute the absolute distance to 100 miles for each runner\n#     .___(4, ___)\n#     [___]\n#     .mean()\n# )\n\n# your code here\nraise NotImplementedError\nanswer3\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer3)).encode(\"utf-8\")+b\"7bdb3\").hexdigest() == \"3a71bdfd3d857a0c621cb9c0eca46cd2d17f7a25\", \"type of answer3 is not correct\"\nassert sha1(str(answer3).encode(\"utf-8\")+b\"7bdb3\").hexdigest() == \"37f5342ce74ffaa4f9a707df207bb80d2cfefc4f\", \"value of answer3 is not correct\"\n\nprint('Success!')\n\nQuestion 4.0  {points: 1}\nFor this question, let’s instead predict the race time based on the 2 closest neighbors to the 100 miles per week during training.\nAssign your answer to an object named answer4.\n\n# your code here\nraise NotImplementedError\nanswer4\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer4)).encode(\"utf-8\")+b\"91a96\").hexdigest() == \"56be2360e19074080ea1ee864973930e01b1feb8\", \"type of answer4 is not correct\"\nassert sha1(str(answer4).encode(\"utf-8\")+b\"91a96\").hexdigest() == \"d87f739886a2763fa87919ecfd5616a4cc832881\", \"value of answer4 is not correct\"\n\nprint('Success!')\n\nQuestion 5.0  {points: 1}\nSo far you have calculated the \\(k\\) nearest neighbors predictions manually based on values of \\(k\\) we have told you to use. However, last week we learned how to use a better method to choose the best \\(k\\) for classification.\nBased on what you learned last week and what you have learned about \\(k\\)-nn regression so far this week, which method would you use to choose the \\(k\\) (in the situation where we don’t tell you which \\(k\\) to use)?\n\n\nChoose the \\(k\\) that excludes most outliers\n\n\nChoose the \\(k\\) with the lowest training error\n\n\nChoose the \\(k\\) with the lowest cross-validation error\n\n\nChoose the \\(k\\) that includes the most data points\n\n\nChoose the \\(k\\) with the lowest testing error\n\n\nAssign your answer to an object called answer5. Make sure your answer is an uppercase letter and is surrounded by quotation marks (e.g. \"F\").\n\n# your code here\nraise NotImplementedError\nanswer5\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer5)).encode(\"utf-8\")+b\"37693\").hexdigest() == \"90111e24734de91c15169da5902a3788de0a8444\", \"type of answer5 is not str. answer5 should be an str\"\nassert sha1(str(len(answer5)).encode(\"utf-8\")+b\"37693\").hexdigest() == \"fd218df3c1aeccee91b16c5edb3164742b012b62\", \"length of answer5 is not correct\"\nassert sha1(str(answer5.lower()).encode(\"utf-8\")+b\"37693\").hexdigest() == \"4f99d9a8de2f5a0e124277c1be1e248dce2816c2\", \"value of answer5 is not correct\"\nassert sha1(str(answer5).encode(\"utf-8\")+b\"37693\").hexdigest() == \"57eb3673a1f7ddb554643fa32e78e43ee5424edb\", \"correct string value of answer5 but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 6.0  {points: 1}\nWe have just seen how to perform k-nn regression manually, now we will apply it to the whole dataset using the scikit-learn package. To do so, we will first need to create the training and test datasets. Split the data to use 75% as your training set. Store the training data as marathon_training and the test set as marathon_testing. Remember we won’t touch the test dataset until the end.\nNext, set the time_hrs column as the target (y) and max column as the input feature (X). Store the features as X_train and X_test and targets as y_train and y_test respectively for the marathon_training and marathon_testing. Remember that it is easier to work with input features as a data frame rather than a series, so make sure to extract the single input feature as a data frame by passing the column name inside a list.\nAssign your answers to objects named marathon_training, marathon_testing, X_train, y_train, X_test, and y_test.\n\n# ___, ___ = train_test_split(\n#     ___,\n#     test_size=___,\n#     random_state=2000,  # Do not change the random_state\n# )\n# X_train = ___[___]  # A single column data frame\n# y_train = ___[___]  # A series\n\n# X_test = ___[___]  # A single column data frame\n# y_test = ___[___]  # A series\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_training is None)).encode(\"utf-8\")+b\"e7ecd\").hexdigest() == \"23389e9a6137f9af868ed3d5dcb5563fa02cf9ee\", \"type of marathon_training is None is not bool. marathon_training is None should be a bool\"\nassert sha1(str(marathon_training is None).encode(\"utf-8\")+b\"e7ecd\").hexdigest() == \"a85ea51e13d13be27411312c98c5d8d08427748f\", \"boolean value of marathon_training is None is not correct\"\n\nassert sha1(str(type(marathon_training.shape)).encode(\"utf-8\")+b\"e7ece\").hexdigest() == \"5c9a6007e66cd4aeb4e09f7e1fbad9430720c874\", \"type of marathon_training.shape is not tuple. marathon_training.shape should be a tuple\"\nassert sha1(str(len(marathon_training.shape)).encode(\"utf-8\")+b\"e7ece\").hexdigest() == \"4371251b32814c5ec901659b3f7243b8f06201eb\", \"length of marathon_training.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_training.shape))).encode(\"utf-8\")+b\"e7ece\").hexdigest() == \"8c5dccfad110d40e6e2dd0939e314507b72c82ba\", \"values of marathon_training.shape are not correct\"\nassert sha1(str(marathon_training.shape).encode(\"utf-8\")+b\"e7ece\").hexdigest() == \"79b9d73983831ec1ad4b9a030e13f40e5e631303\", \"order of elements of marathon_training.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_training.age))).encode(\"utf-8\")+b\"e7ecf\").hexdigest() == \"6dd9106e51faadb152c13f46748de9b7631041f4\", \"type of sum(marathon_training.age) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_training.age)).encode(\"utf-8\")+b\"e7ecf\").hexdigest() == \"2e875c992a00e64a049679c247546552107e4806\", \"value of sum(marathon_training.age) is not correct\"\n\nassert sha1(str(type(marathon_testing is None)).encode(\"utf-8\")+b\"e7ed0\").hexdigest() == \"eeab11eee8b5ac43daead049c08370a1d6bd29e1\", \"type of marathon_testing is None is not bool. marathon_testing is None should be a bool\"\nassert sha1(str(marathon_testing is None).encode(\"utf-8\")+b\"e7ed0\").hexdigest() == \"45fc996440f56862fa729a598efb3e94ad2cbcba\", \"boolean value of marathon_testing is None is not correct\"\n\nassert sha1(str(type(marathon_testing.shape)).encode(\"utf-8\")+b\"e7ed1\").hexdigest() == \"580e794a2ad10c9c374bc994cb416f4825c1abc2\", \"type of marathon_testing.shape is not tuple. marathon_testing.shape should be a tuple\"\nassert sha1(str(len(marathon_testing.shape)).encode(\"utf-8\")+b\"e7ed1\").hexdigest() == \"0cfe48ec1593a68181d07cf5f9f1868014b33c5f\", \"length of marathon_testing.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_testing.shape))).encode(\"utf-8\")+b\"e7ed1\").hexdigest() == \"cce555187808a48e1408af0bb536e79e8a2225a5\", \"values of marathon_testing.shape are not correct\"\nassert sha1(str(marathon_testing.shape).encode(\"utf-8\")+b\"e7ed1\").hexdigest() == \"fbf8329576ea0be968fbabcd4c5dd25a6d17c93a\", \"order of elements of marathon_testing.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_testing.age))).encode(\"utf-8\")+b\"e7ed2\").hexdigest() == \"3d8e616c9608ebb5fa199e74614b680d59572017\", \"type of sum(marathon_testing.age) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_testing.age)).encode(\"utf-8\")+b\"e7ed2\").hexdigest() == \"a666611338ec9b1d7c1df5146282bf710f1d54f6\", \"value of sum(marathon_testing.age) is not correct\"\n\nassert sha1(str(type(X_train.columns.values)).encode(\"utf-8\")+b\"e7ed3\").hexdigest() == \"d2af8b1f3ac36ff6af2a760a8b5cfa0388a5a272\", \"type of X_train.columns.values is not correct\"\nassert sha1(str(X_train.columns.values).encode(\"utf-8\")+b\"e7ed3\").hexdigest() == \"17be56c6e47e3d10712619e99b61a6d767fd5666\", \"value of X_train.columns.values is not correct\"\n\nassert sha1(str(type(X_train.shape)).encode(\"utf-8\")+b\"e7ed4\").hexdigest() == \"414b7eb138475c852d35f49fc6d280cb0f9a2a9f\", \"type of X_train.shape is not tuple. X_train.shape should be a tuple\"\nassert sha1(str(len(X_train.shape)).encode(\"utf-8\")+b\"e7ed4\").hexdigest() == \"73a0b26b8110b3197945c70b0650286285cec8d3\", \"length of X_train.shape is not correct\"\nassert sha1(str(sorted(map(str, X_train.shape))).encode(\"utf-8\")+b\"e7ed4\").hexdigest() == \"d74001495311d9550e27e96f7c560f5ed899dc2a\", \"values of X_train.shape are not correct\"\nassert sha1(str(X_train.shape).encode(\"utf-8\")+b\"e7ed4\").hexdigest() == \"f434b7539cfe664a9765b7bbb07e4709fc2491da\", \"order of elements of X_train.shape is not correct\"\n\nassert sha1(str(type(y_train.name)).encode(\"utf-8\")+b\"e7ed5\").hexdigest() == \"07fb7ee407cccab8c19809282e20aecb7bb62185\", \"type of y_train.name is not str. y_train.name should be an str\"\nassert sha1(str(len(y_train.name)).encode(\"utf-8\")+b\"e7ed5\").hexdigest() == \"c159bd913e413006f4be6da2dfb303dd974e4bbb\", \"length of y_train.name is not correct\"\nassert sha1(str(y_train.name.lower()).encode(\"utf-8\")+b\"e7ed5\").hexdigest() == \"a622136689ba01beab477abec3b4b37aa5e2f515\", \"value of y_train.name is not correct\"\nassert sha1(str(y_train.name).encode(\"utf-8\")+b\"e7ed5\").hexdigest() == \"a622136689ba01beab477abec3b4b37aa5e2f515\", \"correct string value of y_train.name but incorrect case of letters\"\n\nassert sha1(str(type(y_train.shape)).encode(\"utf-8\")+b\"e7ed6\").hexdigest() == \"22627a8f279315114f3a5f91de95da0d83389d79\", \"type of y_train.shape is not tuple. y_train.shape should be a tuple\"\nassert sha1(str(len(y_train.shape)).encode(\"utf-8\")+b\"e7ed6\").hexdigest() == \"c61d6dd0518678d74f5f69117e206324490517c6\", \"length of y_train.shape is not correct\"\nassert sha1(str(sorted(map(str, y_train.shape))).encode(\"utf-8\")+b\"e7ed6\").hexdigest() == \"775174c861c6ab54c4c08abf24b2bfebdb30af3f\", \"values of y_train.shape are not correct\"\nassert sha1(str(y_train.shape).encode(\"utf-8\")+b\"e7ed6\").hexdigest() == \"cead78c49de35c73a190c737d2bb46ff4e2fec23\", \"order of elements of y_train.shape is not correct\"\n\nassert sha1(str(type(X_test.columns.values)).encode(\"utf-8\")+b\"e7ed7\").hexdigest() == \"22fe9ec90a423d6060f49aa6d7a76ce3d3974080\", \"type of X_test.columns.values is not correct\"\nassert sha1(str(X_test.columns.values).encode(\"utf-8\")+b\"e7ed7\").hexdigest() == \"7754708b959b4274773a259f79fd376b28310eac\", \"value of X_test.columns.values is not correct\"\n\nassert sha1(str(type(X_test.shape)).encode(\"utf-8\")+b\"e7ed8\").hexdigest() == \"f0858d2513b6eee63f326fcebe57b013cd31df36\", \"type of X_test.shape is not tuple. X_test.shape should be a tuple\"\nassert sha1(str(len(X_test.shape)).encode(\"utf-8\")+b\"e7ed8\").hexdigest() == \"bd7fc6f3096017ac86fb06841d97ddd854492ed0\", \"length of X_test.shape is not correct\"\nassert sha1(str(sorted(map(str, X_test.shape))).encode(\"utf-8\")+b\"e7ed8\").hexdigest() == \"ef154c5dedbd48b6b6a3232c68a2fea5c9f7be4b\", \"values of X_test.shape are not correct\"\nassert sha1(str(X_test.shape).encode(\"utf-8\")+b\"e7ed8\").hexdigest() == \"3a6cfb0fce7e27384ffd615b58fb5cbd20a3d82b\", \"order of elements of X_test.shape is not correct\"\n\nassert sha1(str(type(y_test.name)).encode(\"utf-8\")+b\"e7ed9\").hexdigest() == \"72df4c655d030df71555b431f22397a7e42c0e38\", \"type of y_test.name is not str. y_test.name should be an str\"\nassert sha1(str(len(y_test.name)).encode(\"utf-8\")+b\"e7ed9\").hexdigest() == \"f164546779f465e2b62365d3ab3737c47eebf4d6\", \"length of y_test.name is not correct\"\nassert sha1(str(y_test.name.lower()).encode(\"utf-8\")+b\"e7ed9\").hexdigest() == \"3327e8ccb2ac8ed8cf1dad70018d6f95c820d712\", \"value of y_test.name is not correct\"\nassert sha1(str(y_test.name).encode(\"utf-8\")+b\"e7ed9\").hexdigest() == \"3327e8ccb2ac8ed8cf1dad70018d6f95c820d712\", \"correct string value of y_test.name but incorrect case of letters\"\n\nassert sha1(str(type(y_test.shape)).encode(\"utf-8\")+b\"e7eda\").hexdigest() == \"3caf58a1ec02a722090ad103cb12c17948c05ea8\", \"type of y_test.shape is not tuple. y_test.shape should be a tuple\"\nassert sha1(str(len(y_test.shape)).encode(\"utf-8\")+b\"e7eda\").hexdigest() == \"6bee5d9b7be7422e0080aec785993d4b1645979b\", \"length of y_test.shape is not correct\"\nassert sha1(str(sorted(map(str, y_test.shape))).encode(\"utf-8\")+b\"e7eda\").hexdigest() == \"b4518ece20f13e8647767347afffe9dbd285baf1\", \"values of y_test.shape are not correct\"\nassert sha1(str(y_test.shape).encode(\"utf-8\")+b\"e7eda\").hexdigest() == \"5ff9b39d3ce50b07b91aa77be609c2ec942f2dc3\", \"order of elements of y_test.shape is not correct\"\n\nprint('Success!')\n\nQuestion 7.0  {points: 1}\nNext, we’ll use cross-validation on our training data to choose \\(k\\). In \\(k\\)-nn classification, we used accuracy to see how well our predictions matched the true labels. In the context of \\(k\\)-nn regression, we will use RMSPE as the scoring instead. Interpreting the RMSPE value can be tricky but generally speaking, if the prediction values are very close to the true values, the RMSPE will be small. Conversely, if the prediction values are not very close to the true values, the RMSPE will be quite large.\nLet’s perform a cross-validation and choose the optimal \\(k\\)!\nFirst, create a pipeline for \\(k\\)-nn. We are still using the \\(k\\)-nearest neighbours algorithm, and we will also use the StandardScaler to standardize the numerical values. Store your pipeline in an object called marathon_pipe. Finally, perform a cross-validation with 5 folds using the cross_validate function. Remember that since the cross_validate function always maximizes its “score”, and here we’re using RMSPE (lower is better!), we need to specify that we’re using the negative RMSPE (\"neg_root_mean_squared_error\").\nStore the output of the cross validation as a data frame in an object called marathon_cv.\n\n# ___ = make_pipeline(\n#     ___,\n#     ___,\n# )\n#\n# marathon_cv = pd.___(\n#     cross_validate(\n#         ___,\n#         ___,\n#         ___,\n#         scoring=___,\n#         return_train_score=True,\n#     )\n# )\n\n# your code here\nraise NotImplementedError\nmarathon_cv\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_pipe is None)).encode(\"utf-8\")+b\"8864c\").hexdigest() == \"25299b052f7ab2931046737631887a8cc568dbdc\", \"type of marathon_pipe is None is not bool. marathon_pipe is None should be a bool\"\nassert sha1(str(marathon_pipe is None).encode(\"utf-8\")+b\"8864c\").hexdigest() == \"e30f3e578a40c11694672cc29b9e8ed48e9b5b0a\", \"boolean value of marathon_pipe is None is not correct\"\n\nassert sha1(str(type(marathon_pipe.steps[1][1].n_neighbors)).encode(\"utf-8\")+b\"8864d\").hexdigest() == \"d547f76ed04d14a405bffd75b858fcadc01626b2\", \"type of marathon_pipe.steps[1][1].n_neighbors is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(marathon_pipe.steps[1][1].n_neighbors).encode(\"utf-8\")+b\"8864d\").hexdigest() == \"06ffe53eaaff553f1311c56f2367fab086ff823e\", \"value of marathon_pipe.steps[1][1].n_neighbors is not correct\"\n\nassert sha1(str(type(marathon_pipe.steps[1][1].weights)).encode(\"utf-8\")+b\"8864e\").hexdigest() == \"58751697270ec924b2f324e1b5544996472be14f\", \"type of marathon_pipe.steps[1][1].weights is not str. marathon_pipe.steps[1][1].weights should be an str\"\nassert sha1(str(len(marathon_pipe.steps[1][1].weights)).encode(\"utf-8\")+b\"8864e\").hexdigest() == \"6cd5d1937d5b4571f3f6a7a4880af49e992de8a5\", \"length of marathon_pipe.steps[1][1].weights is not correct\"\nassert sha1(str(marathon_pipe.steps[1][1].weights.lower()).encode(\"utf-8\")+b\"8864e\").hexdigest() == \"ab385b8efd1b981d6c0200273e19c124f3e24308\", \"value of marathon_pipe.steps[1][1].weights is not correct\"\nassert sha1(str(marathon_pipe.steps[1][1].weights).encode(\"utf-8\")+b\"8864e\").hexdigest() == \"ab385b8efd1b981d6c0200273e19c124f3e24308\", \"correct string value of marathon_pipe.steps[1][1].weights but incorrect case of letters\"\n\nassert sha1(str(type(marathon_pipe.steps[0][1])).encode(\"utf-8\")+b\"8864f\").hexdigest() == \"974eac74a0ad1ee3ad51d33c86e4f23b9caac296\", \"type of marathon_pipe.steps[0][1] is not correct\"\nassert sha1(str(marathon_pipe.steps[0][1]).encode(\"utf-8\")+b\"8864f\").hexdigest() == \"88499ff4e2ed2a47e400d341106fb5b49551ed08\", \"value of marathon_pipe.steps[0][1] is not correct\"\n\nassert sha1(str(type(marathon_cv is None)).encode(\"utf-8\")+b\"88650\").hexdigest() == \"82c0b965cf5966540442e3581313923f152295a1\", \"type of marathon_cv is None is not bool. marathon_cv is None should be a bool\"\nassert sha1(str(marathon_cv is None).encode(\"utf-8\")+b\"88650\").hexdigest() == \"e0832f10224a7c3b8fc8349d63cbcc366639d036\", \"boolean value of marathon_cv is None is not correct\"\n\nassert sha1(str(type(len(marathon_cv['train_score']))).encode(\"utf-8\")+b\"88651\").hexdigest() == \"2da35ca4faddbf2b4b47152b40d1fbb51614153a\", \"type of len(marathon_cv['train_score']) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(marathon_cv['train_score'])).encode(\"utf-8\")+b\"88651\").hexdigest() == \"c03676f0aa4939eab9beb8cdd1e1ad156ceff2c0\", \"value of len(marathon_cv['train_score']) is not correct\"\n\nassert sha1(str(type(sum(marathon_cv['train_score']))).encode(\"utf-8\")+b\"88652\").hexdigest() == \"1f1dd6525f2a541022058c1c5c49ef08440be730\", \"type of sum(marathon_cv['train_score']) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_cv['train_score']), 2)).encode(\"utf-8\")+b\"88652\").hexdigest() == \"5839a4484a5996becf1142a0c824e072abf9ae74\", \"value of sum(marathon_cv['train_score']) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(marathon_cv['test_score']))).encode(\"utf-8\")+b\"88653\").hexdigest() == \"b2d3d500fa919b6f72c5a9bfbac6ddf39984c89e\", \"type of sum(marathon_cv['test_score']) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_cv['test_score']), 2)).encode(\"utf-8\")+b\"88653\").hexdigest() == \"ceafaaed8afb31ab587e60437edb7aee28aed3e5\", \"value of sum(marathon_cv['test_score']) is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 8.0  {points: 1}\nThe major difference compared to other models from Chapters 6 and 7 is that we are running a regression rather than a classification. Using KNeighborsRegressor essentially tells scikit-learn that we need to use different metrics (neg_root_mean_squared_error rather than accuracy) for tuning and evaluation.\nNow, let’s use the neg_root_mean_squared_error to find the best setting for \\(k\\) from our model. Let’s test 200 values of \\(k\\).\nFirst, create a parameter grid called param_grid that contains values of range 1 to 200.\nNext, tune your model such that it tests all the values in range(1, 201, 1) using GridSearchCV function with cv=5 and n_jobs=-1 and save the tuned model as marathon_tuned. Finally, fit the tuned model to the training dataset and save the cv_results_ in a dataframe.\nAssign your answer to an object called marathon_results.\n\nnp.random.seed(2019) # DO NOT CHANGE\n\n# param_grid = _____\n# marathon_tuned = GridSearchCV(___, ___, ___, ___, ___)\n# marathon_results = pd.DataFrame(____.fit(____, ____).____)\n\n# your code here\nraise NotImplementedError\nmarathon_results\n\n\nfrom hashlib import sha1\nassert sha1(str(type(param_grid is None)).encode(\"utf-8\")+b\"d182e\").hexdigest() == \"ec7bcbea68f702395cdd5e67180936ae258a3f3c\", \"type of param_grid is None is not bool. param_grid is None should be a bool\"\nassert sha1(str(param_grid is None).encode(\"utf-8\")+b\"d182e\").hexdigest() == \"3cf3edd9eddd1b5c4500aa5b52869832b40083d9\", \"boolean value of param_grid is None is not correct\"\n\nassert sha1(str(type(param_grid)).encode(\"utf-8\")+b\"d182f\").hexdigest() == \"ace9aac86250e076a5706a6c94d179a856ffdd1e\", \"type of type(param_grid) is not correct\"\n\nassert sha1(str(type(\"kneighborsregressor__n_neighbors\" in param_grid)).encode(\"utf-8\")+b\"d1830\").hexdigest() == \"ff29e4faf0e45be91ef5484ace002565aebc17a8\", \"type of \\\"kneighborsregressor__n_neighbors\\\" in param_grid is not bool. \\\"kneighborsregressor__n_neighbors\\\" in param_grid should be a bool\"\nassert sha1(str(\"kneighborsregressor__n_neighbors\" in param_grid).encode(\"utf-8\")+b\"d1830\").hexdigest() == \"423cc17212d6b16b3a09484a00636ed40da12d0c\", \"boolean value of \\\"kneighborsregressor__n_neighbors\\\" in param_grid is not correct\"\n\nassert sha1(str(type(sum(i for i in param_grid['kneighborsregressor__n_neighbors']))).encode(\"utf-8\")+b\"d1831\").hexdigest() == \"e79769dca25d75ac1efdfd501a1a3e50a1248982\", \"type of sum(i for i in param_grid['kneighborsregressor__n_neighbors']) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(i for i in param_grid['kneighborsregressor__n_neighbors'])).encode(\"utf-8\")+b\"d1831\").hexdigest() == \"3062f138a1709066561977581ee37b9da7187b8b\", \"value of sum(i for i in param_grid['kneighborsregressor__n_neighbors']) is not correct\"\n\nassert sha1(str(type(marathon_tuned is None)).encode(\"utf-8\")+b\"d1832\").hexdigest() == \"9f763278e96c032f901645614bcc433e19b6da21\", \"type of marathon_tuned is None is not bool. marathon_tuned is None should be a bool\"\nassert sha1(str(marathon_tuned is None).encode(\"utf-8\")+b\"d1832\").hexdigest() == \"802f7ddda999c610b46d5eefeaad353b41db1f11\", \"boolean value of marathon_tuned is None is not correct\"\n\nassert sha1(str(type(marathon_tuned.n_splits_)).encode(\"utf-8\")+b\"d1833\").hexdigest() == \"4ae49a96c27611edbdc55e4b6a0acdb6a0f4e361\", \"type of marathon_tuned.n_splits_ is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(marathon_tuned.n_splits_).encode(\"utf-8\")+b\"d1833\").hexdigest() == \"03197f439ff046e0849de2e1ea68e5abf3a3227a\", \"value of marathon_tuned.n_splits_ is not correct\"\n\nassert sha1(str(type(marathon_tuned.estimator[0])).encode(\"utf-8\")+b\"d1834\").hexdigest() == \"d6e10402eebb0cf168b578048a88cb545603e32a\", \"type of marathon_tuned.estimator[0] is not correct\"\nassert sha1(str(marathon_tuned.estimator[0]).encode(\"utf-8\")+b\"d1834\").hexdigest() == \"f5465c5fbaa6f3239fc28a1ff25004895b6c5876\", \"value of marathon_tuned.estimator[0] is not correct\"\n\nassert sha1(str(type(marathon_tuned.estimator[1])).encode(\"utf-8\")+b\"d1835\").hexdigest() == \"16467dbeabfbbbb3e3f104b63615a61965d962b2\", \"type of marathon_tuned.estimator[1] is not correct\"\nassert sha1(str(marathon_tuned.estimator[1]).encode(\"utf-8\")+b\"d1835\").hexdigest() == \"bf022c71f2b7bd9757a0f4c333efd82fa9d5a235\", \"value of marathon_tuned.estimator[1] is not correct\"\n\nassert sha1(str(type(marathon_tuned.param_grid == param_grid)).encode(\"utf-8\")+b\"d1836\").hexdigest() == \"576f7dc516eea23c2e820127a7892ceea3d0dab5\", \"type of marathon_tuned.param_grid == param_grid is not bool. marathon_tuned.param_grid == param_grid should be a bool\"\nassert sha1(str(marathon_tuned.param_grid == param_grid).encode(\"utf-8\")+b\"d1836\").hexdigest() == \"681a7a81f3b2eb025a9461956a79a884370c9eed\", \"boolean value of marathon_tuned.param_grid == param_grid is not correct\"\n\nassert sha1(str(type(marathon_results is None)).encode(\"utf-8\")+b\"d1837\").hexdigest() == \"8689f96913286ecd30d823e848b7eb97d738c848\", \"type of marathon_results is None is not bool. marathon_results is None should be a bool\"\nassert sha1(str(marathon_results is None).encode(\"utf-8\")+b\"d1837\").hexdigest() == \"0d219c756c386807c0464fa0cee1b865a1200bac\", \"boolean value of marathon_results is None is not correct\"\n\nassert sha1(str(type(marathon_results)).encode(\"utf-8\")+b\"d1838\").hexdigest() == \"18ba9e44f8dde45008fd1011b189d83f547d79f3\", \"type of type(marathon_results) is not correct\"\n\nassert sha1(str(type(marathon_results.shape)).encode(\"utf-8\")+b\"d1839\").hexdigest() == \"21ab468813730121894ce75777b2e1fd48fdb5da\", \"type of marathon_results.shape is not tuple. marathon_results.shape should be a tuple\"\nassert sha1(str(len(marathon_results.shape)).encode(\"utf-8\")+b\"d1839\").hexdigest() == \"ad46a3a70676d9eb487222097e1246978dd107a0\", \"length of marathon_results.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_results.shape))).encode(\"utf-8\")+b\"d1839\").hexdigest() == \"702ab7880f169feb4a954839cfefe6ea051b42d1\", \"values of marathon_results.shape are not correct\"\nassert sha1(str(marathon_results.shape).encode(\"utf-8\")+b\"d1839\").hexdigest() == \"ac943869f441904a6bb3e649e3086382f6496051\", \"order of elements of marathon_results.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_results.param_kneighborsregressor__n_neighbors))).encode(\"utf-8\")+b\"d183a\").hexdigest() == \"46aa9236ef14c7d42d2b7d92052c5c94af6cab15\", \"type of sum(marathon_results.param_kneighborsregressor__n_neighbors) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_results.param_kneighborsregressor__n_neighbors)).encode(\"utf-8\")+b\"d183a\").hexdigest() == \"61a6b58502c1d2d00fb7a6a65c20cae775b189ff\", \"value of sum(marathon_results.param_kneighborsregressor__n_neighbors) is not correct\"\n\nassert sha1(str(type(sum(marathon_results.mean_test_score))).encode(\"utf-8\")+b\"d183b\").hexdigest() == \"eb6784ad9e0df0d3b20b0857d501db1797afe75c\", \"type of sum(marathon_results.mean_test_score) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_results.mean_test_score), 2)).encode(\"utf-8\")+b\"d183b\").hexdigest() == \"5c2e32ca9aacc4f5726277288252ac363fcd924d\", \"value of sum(marathon_results.mean_test_score) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(marathon_results.std_test_score))).encode(\"utf-8\")+b\"d183c\").hexdigest() == \"a4c7c7e034abaadcaf56057d19893a773dbf8fb1\", \"type of sum(marathon_results.std_test_score) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_results.std_test_score), 2)).encode(\"utf-8\")+b\"d183c\").hexdigest() == \"3000c4e4327bea85d84632c37db03bc1d008b2c0\", \"value of sum(marathon_results.std_test_score) is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 8.1  {points: 1}\nGreat! Now find the number of neighbors that will serve as our best \\(k\\) value by calling the best_params_ attribute of the model marathon_tuned. Your answer should simply be a dictionary with one key-value pair.\nAlso, find the score for the best model by calling the best_score_ attribute of the model marathon_tuned. Make sure to convert the negative RMSPE score we used for cross-validation into a positive RMSPE score for reporting by using a - sign.\nAssign your best parameters to an object called marathon_min, and assign your best RMSPE to an object called marathon_best_RMSPE.\n\n# ___ = ___.best_params_\n# ___ = -___.best_score_\n\n# your code here\nraise NotImplementedError\nmarathon_min\n\n\nmarathon_best_RMSPE\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_min is None)).encode(\"utf-8\")+b\"9ae42\").hexdigest() == \"2feff5d755380f5a1f50804a0c4948d723261eda\", \"type of marathon_min is None is not bool. marathon_min is None should be a bool\"\nassert sha1(str(marathon_min is None).encode(\"utf-8\")+b\"9ae42\").hexdigest() == \"5015b9976a1f2e2f2fa109305796f1e1702ba164\", \"boolean value of marathon_min is None is not correct\"\n\nassert sha1(str(type(marathon_min)).encode(\"utf-8\")+b\"9ae43\").hexdigest() == \"b4658974ee290076b30642296bffc395bf445d21\", \"type of type(marathon_min) is not correct\"\n\nassert sha1(str(type(marathon_min)).encode(\"utf-8\")+b\"9ae44\").hexdigest() == \"3040f94c545ff413539332503528deab08084a62\", \"type of marathon_min is not dict. marathon_min should be a dict\"\nassert sha1(str(len(list(marathon_min.keys()))).encode(\"utf-8\")+b\"9ae44\").hexdigest() == \"84224507b5177db8e9c88f5abd8824ece22645b8\", \"number of keys of marathon_min is not correct\"\nassert sha1(str(sorted(map(str, marathon_min.keys()))).encode(\"utf-8\")+b\"9ae44\").hexdigest() == \"edf882d0536918e0a683172498fcad8c63f6af3c\", \"keys of marathon_min are not correct\"\nassert sha1(str(sorted(map(str, marathon_min.values()))).encode(\"utf-8\")+b\"9ae44\").hexdigest() == \"c3bca7dfbca0e41a87d4ff3e43c335504e37caeb\", \"correct keys, but values of marathon_min are not correct\"\nassert sha1(str(marathon_min).encode(\"utf-8\")+b\"9ae44\").hexdigest() == \"80961d32128e4ca9746e99318dd97c9ed04c317f\", \"correct keys and values, but incorrect correspondence in keys and values of marathon_min\"\n\nassert sha1(str(type(marathon_best_RMSPE is None)).encode(\"utf-8\")+b\"9ae45\").hexdigest() == \"487be9299591e548ea7dcedef4c4e650814148d2\", \"type of marathon_best_RMSPE is None is not bool. marathon_best_RMSPE is None should be a bool\"\nassert sha1(str(marathon_best_RMSPE is None).encode(\"utf-8\")+b\"9ae45\").hexdigest() == \"84ef8bfc80a388930431187a4d0640f62e727b78\", \"boolean value of marathon_best_RMSPE is None is not correct\"\n\nassert sha1(str(type(marathon_best_RMSPE)).encode(\"utf-8\")+b\"9ae46\").hexdigest() == \"78a416ab0e296d01fb1307a654bf9b70b1337868\", \"type of marathon_best_RMSPE is not correct\"\nassert sha1(str(marathon_best_RMSPE).encode(\"utf-8\")+b\"9ae46\").hexdigest() == \"8785f764d752095decf87a8dd24909d0016992f7\", \"value of marathon_best_RMSPE is not correct\"\n\nprint('Success!')\n\nQuestion 8.2  {points: 1}\nTo assess how well our model might do at predicting on unseen data, we will assess its RMSPE on the test data.\nWe will use the predict function to make predictions on the test data and store the predictions marathon_prediction. Remember that GridSearchCV automatically refits the model with the best found parameters, so you can use the predict method of the marathon_tuned variable for this step.\nFinally, we will compute the RMSPE on the test data using the mean_squared_error function. Don’t forget to take the square root to obtain the RMSPE!\nNote: scikit-learn also has a score function for the KNeighborsRegressor. The score function returns the coefficient of determination (often called \\(R^2\\)) of the fit, not the RMSPE.\nAssign your answer in an object called marathon_summary.\n\nnp.random.seed(1234) # DO NOT CHANGE\n\n# ___ = ___.___(___)\n# ___ = mean_squared_error(___, ___)**(1/2)\n\n\n# your code here\nraise NotImplementedError\nmarathon_summary\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_prediction is None)).encode(\"utf-8\")+b\"5e675\").hexdigest() == \"e171ec8a4f6e825d47e43ba2c8d7e4962ff2f8b4\", \"type of marathon_prediction is None is not bool. marathon_prediction is None should be a bool\"\nassert sha1(str(marathon_prediction is None).encode(\"utf-8\")+b\"5e675\").hexdigest() == \"c76b30dacb294c54dc2932240aa7fa4c23543c56\", \"boolean value of marathon_prediction is None is not correct\"\n\nassert sha1(str(type(marathon_prediction)).encode(\"utf-8\")+b\"5e676\").hexdigest() == \"be3ffc123e07105d4089655588fb6d71383eb08d\", \"type of type(marathon_prediction) is not correct\"\n\nassert sha1(str(type(marathon_prediction.sum())).encode(\"utf-8\")+b\"5e677\").hexdigest() == \"d3da4b9915f115587058740bd3c406c49cbf5b2c\", \"type of marathon_prediction.sum() is not correct\"\nassert sha1(str(marathon_prediction.sum()).encode(\"utf-8\")+b\"5e677\").hexdigest() == \"8c10d2be9c82e2bdbdcd44f3fbb35f6c102d0119\", \"value of marathon_prediction.sum() is not correct\"\n\nassert sha1(str(type(marathon_summary is None)).encode(\"utf-8\")+b\"5e678\").hexdigest() == \"1581c288216e96bfb8d0fcc29cf60c18dd92909b\", \"type of marathon_summary is None is not bool. marathon_summary is None should be a bool\"\nassert sha1(str(marathon_summary is None).encode(\"utf-8\")+b\"5e678\").hexdigest() == \"6f8e105f98933eb6b68dbbb0f78058a8ac9dd80a\", \"boolean value of marathon_summary is None is not correct\"\n\nassert sha1(str(type(marathon_summary)).encode(\"utf-8\")+b\"5e679\").hexdigest() == \"ba12bad71525a497b68612abadf62fd2a05b9230\", \"type of marathon_summary is not correct\"\nassert sha1(str(marathon_summary).encode(\"utf-8\")+b\"5e679\").hexdigest() == \"ecbf8a32ee6fde067052e0cf254aaf3b674d001f\", \"value of marathon_summary is not correct\"\n\nprint('Success!')\n\nWhat does this RMSPE mean? RMSPE is measured in the units of the target/response variable, so it can sometimes be a bit hard to interpret. In this case, we have a helpful reference to compare against: we know that a typical marathon race time is somewhere between 3 - 5 hours. So this model allows us to predict a runner’s race time up to about +/-0.6 of an hour, or +/- 36 minutes. Relative the total race time, this margin of error is not fantastic, but not terrible either. We can certainly use the model to determine roughly whether an athlete will have a bad, good, or excellent race time, but probably cannot reliably distinguish between athletes of a similar caliber.\nFor now, let’s consider this approach to thinking about RMSPE from our testing data set: as long as it’s not significantly worse than the cross-validation RMSPE of our best model (Question 8.1), then we can say that we’re not doing too much worse on the test data than we did on the training data. In future courses on statistical/machine learning, you will learn more about how to interpret RMSPE from testing data and other ways to assess models.\nQuestion 8.3 {points: 1}\nThe RMSPE from our testing data set is much worse than the cross-validation RMSPE of our best model.\nAssign your answer to an object named answer8_3. Make sure your answer is either True or False.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer8_3)).encode(\"utf-8\")+b\"a3e08\").hexdigest() == \"b07290d0d5a5f7304386a6085d09604c57fe55d4\", \"type of answer8_3 is not bool. answer8_3 should be a bool\"\nassert sha1(str(answer8_3).encode(\"utf-8\")+b\"a3e08\").hexdigest() == \"0fd0dee1f68a3e7794645e623f2b2be2db15b5cf\", \"boolean value of answer8_3 is not correct\"\n\nprint('Success!')\n\nQuestion 9.0  {points: 1}\nLet’s visualize what the relationship between max and time_hrs looks like with our best \\(k\\) value to ultimately explore how the \\(k\\) value affects \\(k\\)-nn regression.\nTo do so, use the predict function on marathon_tuned to use the model with the best \\(K\\) value to create predictions for the marathon_training data. Then, add the column of predictions to the marathon_training data frame using the assign function. Name the resulting data frame marathon_preds and the new column predictions.\nNext, create a scatterplot with the marathon time (y-axis) against the maximum distance run per week (x-axis) from marathon_preds. Use mark_circle with an opacity of 0.4 to avoid overplotting. Assign your plot to a variable called marathon_plot. Plot the predictions as a black line over the data points. Remember the fundamentals of effective visualizations such as having a human-readable axes titles.\nAssign the data frame from the first part to a variable called marathon_preds, and the plot to a variable called marathon_plot.\n\nnp.random.seed(2019) # DO NOT CHANGE\n\n# marathon_preds = ____.assign(\n#     predictions= _____.predict(____)\n# )\n# marathon_plot = ___\n\n# your code here\nraise NotImplementedError\nmarathon_plot\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_preds is None)).encode(\"utf-8\")+b\"80ef7\").hexdigest() == \"feb8d8685b9e26c54c06aaf7f0f281aee880c614\", \"type of marathon_preds is None is not bool. marathon_preds is None should be a bool\"\nassert sha1(str(marathon_preds is None).encode(\"utf-8\")+b\"80ef7\").hexdigest() == \"bc57fae1eae5ffdeeeada5122d0bb66260bd0ac6\", \"boolean value of marathon_preds is None is not correct\"\n\nassert sha1(str(type(marathon_preds)).encode(\"utf-8\")+b\"80ef8\").hexdigest() == \"5246d7fd78d900c1aaed511af4b93a395c8f8f6b\", \"type of type(marathon_preds) is not correct\"\n\nassert sha1(str(type(marathon_preds.shape)).encode(\"utf-8\")+b\"80ef9\").hexdigest() == \"47500ac86a3aa4df2352b61411ba2c643b26a21b\", \"type of marathon_preds.shape is not tuple. marathon_preds.shape should be a tuple\"\nassert sha1(str(len(marathon_preds.shape)).encode(\"utf-8\")+b\"80ef9\").hexdigest() == \"c1dee5594a1047ae527ce7ff6fd93c09f5d7087f\", \"length of marathon_preds.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_preds.shape))).encode(\"utf-8\")+b\"80ef9\").hexdigest() == \"45e6ce598dd4b9a99a59cd85e2bc95e4e6dcfa1d\", \"values of marathon_preds.shape are not correct\"\nassert sha1(str(marathon_preds.shape).encode(\"utf-8\")+b\"80ef9\").hexdigest() == \"8b0e8a50ed15369143176e9e9a444fb85a94925f\", \"order of elements of marathon_preds.shape is not correct\"\n\nassert sha1(str(type(\"predictions\" in marathon_preds.columns)).encode(\"utf-8\")+b\"80efa\").hexdigest() == \"bee6c28399c272e11c7b36d6af56c340b73f5de9\", \"type of \\\"predictions\\\" in marathon_preds.columns is not bool. \\\"predictions\\\" in marathon_preds.columns should be a bool\"\nassert sha1(str(\"predictions\" in marathon_preds.columns).encode(\"utf-8\")+b\"80efa\").hexdigest() == \"bb01defbd41c2fca442d129ecde3d06ba69c793e\", \"boolean value of \\\"predictions\\\" in marathon_preds.columns is not correct\"\n\nassert sha1(str(type(sum(marathon_preds.predictions))).encode(\"utf-8\")+b\"80efb\").hexdigest() == \"6694cadf894375ec79d08d843730bfa2ebeaa247\", \"type of sum(marathon_preds.predictions) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_preds.predictions), 2)).encode(\"utf-8\")+b\"80efb\").hexdigest() == \"14d7241b6832fe96cee98179e0bfbab800123337\", \"value of sum(marathon_preds.predictions) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(marathon_preds.time_hrs))).encode(\"utf-8\")+b\"80efc\").hexdigest() == \"32fcb9d092fe59373b115cd2156579052d4bb724\", \"type of sum(marathon_preds.time_hrs) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_preds.time_hrs), 2)).encode(\"utf-8\")+b\"80efc\").hexdigest() == \"de864616f527a3484c98bc8e5ce5448442b3c180\", \"value of sum(marathon_preds.time_hrs) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(marathon_plot is None)).encode(\"utf-8\")+b\"80efd\").hexdigest() == \"7802ca930a377d22b91bda34749b3473ae40ce6b\", \"type of marathon_plot is None is not bool. marathon_plot is None should be a bool\"\nassert sha1(str(marathon_plot is None).encode(\"utf-8\")+b\"80efd\").hexdigest() == \"6080493d69f4752d4f838318c797543ab4092182\", \"boolean value of marathon_plot is None is not correct\"\n\nassert sha1(str(type(len(marathon_plot.layer))).encode(\"utf-8\")+b\"80efe\").hexdigest() == \"f5cd3e76aff8db41831d68252d099f01b5af0eeb\", \"type of len(marathon_plot.layer) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(marathon_plot.layer)).encode(\"utf-8\")+b\"80efe\").hexdigest() == \"791ae34b7ef2c822983fb1c67241cccbe65ce087\", \"value of len(marathon_plot.layer) is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[0].mark)).encode(\"utf-8\")+b\"80eff\").hexdigest() == \"31988f4df3bdd784d531d340494800d13f512ec8\", \"type of marathon_plot.layer[0].mark is not correct\"\nassert sha1(str(marathon_plot.layer[0].mark).encode(\"utf-8\")+b\"80eff\").hexdigest() == \"833d54d5fea79d6426472768c3753a1d3e2aab10\", \"value of marathon_plot.layer[0].mark is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[1].mark)).encode(\"utf-8\")+b\"80f00\").hexdigest() == \"afb1d5a9e3ef12f0c2f446d06cc34948eac7babc\", \"type of marathon_plot.layer[1].mark is not correct\"\nassert sha1(str(marathon_plot.layer[1].mark).encode(\"utf-8\")+b\"80f00\").hexdigest() == \"50a72f4524c38645971c4b6a9df41dc1257a95c3\", \"value of marathon_plot.layer[1].mark is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"80f01\").hexdigest() == \"6a6854dca69a83d48caec2cba6e1338a14a346fb\", \"type of marathon_plot.layer[0].encoding.x['shorthand'] is not str. marathon_plot.layer[0].encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"80f01\").hexdigest() == \"e63a8532d269c7f5a36c4d9e504de2fe4993dd7f\", \"length of marathon_plot.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"80f01\").hexdigest() == \"1bd0afc1bf2c4a34122b26e13c975a1f3463b0b9\", \"value of marathon_plot.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.x['shorthand']).encode(\"utf-8\")+b\"80f01\").hexdigest() == \"1bd0afc1bf2c4a34122b26e13c975a1f3463b0b9\", \"correct string value of marathon_plot.layer[0].encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"80f02\").hexdigest() == \"f9dc36ce988a73cb1cb0b471255c1e10e4d91094\", \"type of marathon_plot.layer[0].encoding.y['shorthand'] is not str. marathon_plot.layer[0].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"80f02\").hexdigest() == \"6ce890fc97b51c036be327421ff4134099ff5209\", \"length of marathon_plot.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"80f02\").hexdigest() == \"5afe1d7a1dd51c71a80027335b5a7b623548ea9e\", \"value of marathon_plot.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.y['shorthand']).encode(\"utf-8\")+b\"80f02\").hexdigest() == \"5afe1d7a1dd51c71a80027335b5a7b623548ea9e\", \"correct string value of marathon_plot.layer[0].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"80f03\").hexdigest() == \"32deb9ef407143db608af31b29a3adedbc08ca59\", \"type of marathon_plot.layer[1].encoding.y['shorthand'] is not str. marathon_plot.layer[1].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"80f03\").hexdigest() == \"457b359edc893e20adf144c86cf19d1a75746764\", \"length of marathon_plot.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[1].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"80f03\").hexdigest() == \"91b908c0622ce400d6464a25c58cb64b9b250011\", \"value of marathon_plot.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[1].encoding.y['shorthand']).encode(\"utf-8\")+b\"80f03\").hexdigest() == \"91b908c0622ce400d6464a25c58cb64b9b250011\", \"correct string value of marathon_plot.layer[1].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(isinstance(marathon_plot.layer[0].encoding.x['title'], str))).encode(\"utf-8\")+b\"80f04\").hexdigest() == \"6a9334f3b0f6f1f0d22d19bec2755550c05f1b9b\", \"type of isinstance(marathon_plot.layer[0].encoding.x['title'], str) is not bool. isinstance(marathon_plot.layer[0].encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot.layer[0].encoding.x['title'], str)).encode(\"utf-8\")+b\"80f04\").hexdigest() == \"3902db8b41319e78ddf41fae51a6625a43b54f01\", \"boolean value of isinstance(marathon_plot.layer[0].encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_plot.layer[0].encoding.y['title'], str))).encode(\"utf-8\")+b\"80f05\").hexdigest() == \"523124c5efd969114f88146347e99ccc8593d726\", \"type of isinstance(marathon_plot.layer[0].encoding.y['title'], str) is not bool. isinstance(marathon_plot.layer[0].encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot.layer[0].encoding.y['title'], str)).encode(\"utf-8\")+b\"80f05\").hexdigest() == \"43ecd7181832f23dda45839dbbd29bdbaabcd0b5\", \"boolean value of isinstance(marathon_plot.layer[0].encoding.y['title'], str) is not correct\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/slides/ensembles-code.html",
    "href": "materials/slides/ensembles-code.html",
    "title": "Tree-based and ensemble models",
    "section": "",
    "text": "Code from the slides in an executable notebook."
  },
  {
    "objectID": "materials/slides/ensembles-code.html#example-the-heart-data-set",
    "href": "materials/slides/ensembles-code.html#example-the-heart-data-set",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set",
    "text": "Example: the heart data set\n\nimport pandas as pd\nheart = pd.read_csv(\"data/Heart.csv\", index_col=0)\nheart.info()\n\n\nheart.head()"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#do-we-have-a-class-imbalance",
    "href": "materials/slides/ensembles-code.html#do-we-have-a-class-imbalance",
    "title": "Tree-based and ensemble models",
    "section": "Do we have a class imbalance?",
    "text": "Do we have a class imbalance?\n\nheart['AHD'].value_counts(normalize=True)"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#data-splitting",
    "href": "materials/slides/ensembles-code.html#data-splitting",
    "title": "Tree-based and ensemble models",
    "section": "Data splitting",
    "text": "Data splitting\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(2024)\n\nheart_train, heart_test = train_test_split(\n    heart, train_size=0.8, stratify=heart[\"AHD\"]\n)\n\nX_train = heart_train.drop(columns=['AHD'])\ny_train = heart_train['AHD']\nX_test = heart_test.drop(columns=['AHD'])\ny_test = heart_test['AHD']"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#one-hot-encoding-pre-processing",
    "href": "materials/slides/ensembles-code.html#one-hot-encoding-pre-processing",
    "title": "Tree-based and ensemble models",
    "section": "One hot encoding & pre-processing",
    "text": "One hot encoding & pre-processing\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\nnumeric_feats = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak','Slope', 'Ca']\npassthrough_feats = ['Sex', 'Fbs', 'ExAng']\ncategorical_feats = ['ChestPain', 'Thal']\n\nheart_preprocessor = make_column_transformer(\n    (StandardScaler(), numeric_feats), \n    (\"passthrough\", passthrough_feats),     \n    (OneHotEncoder(handle_unknown = \"ignore\"), categorical_feats),     \n)"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#fitting-a-dummy-classifier",
    "href": "materials/slides/ensembles-code.html#fitting-a-dummy-classifier",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a dummy classifier",
    "text": "Fitting a dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\n\ndummy = DummyClassifier()\ndummy_pipeline = make_pipeline(heart_preprocessor, dummy)\ncv_10_dummy = pd.DataFrame(\n    cross_validate(\n        estimator=dummy_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dummy_metrics = cv_10_dummy.agg([\"mean\", \"sem\"])\nresults = pd.DataFrame({'mean' : [cv_10_dummy_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_dummy_metrics.test_score.iloc[1]]},\n  index = ['Dummy classifier']\n)\nresults"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#fitting-a-decision-tree",
    "href": "materials/slides/ensembles-code.html#fitting-a-decision-tree",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a decision tree",
    "text": "Fitting a decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=2026)\n\ndt_pipeline = make_pipeline(heart_preprocessor, decision_tree)\ncv_10_dt = pd.DataFrame(\n    cross_validate(\n        estimator=dt_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dt_metrics = cv_10_dt.agg([\"mean\", \"sem\"])\nresults_dt = pd.DataFrame({'mean' : [cv_10_dt_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_dt_metrics.test_score.iloc[1]]},\n  index = ['Decision tree']\n)\nresults = pd.concat([results, results_dt])\nresults"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#random-forest-in-scikit-learn-missing-values",
    "href": "materials/slides/ensembles-code.html#random-forest-in-scikit-learn-missing-values",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn & missing values",
    "text": "Random forest in scikit-learn & missing values\nHow many rows have missing observations:\n\nheart.isna().any(axis=1).sum()\n\nDrop rows with missing observations:\n\nheart_train_drop_na = heart_train.dropna()\n\nX_train_drop_na = heart_train_drop_na.drop(\n    columns=['AHD']\n)\ny_train_drop_na = heart_train_drop_na['AHD']"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#random-forest-in-scikit-learn",
    "href": "materials/slides/ensembles-code.html#random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn",
    "text": "Random forest in scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(random_state=2026)\nrf_pipeline = make_pipeline(heart_preprocessor, random_forest)\ncv_10_rf = pd.DataFrame(\n    cross_validate(\n        estimator=rf_pipeline,\n        cv=10,\n        X=X_train_drop_na,\n        y=y_train_drop_na\n    )\n)\n\ncv_10_rf_metrics = cv_10_rf.agg([\"mean\", \"sem\"])\nresults_rf = pd.DataFrame({'mean' : [cv_10_rf_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_rf_metrics.test_score.iloc[1]]},\n  index = ['Random forest']\n)\nresults = pd.concat([results, results_rf])\nresults"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#tuning-random-forest-in-scikit-learn",
    "href": "materials/slides/ensembles-code.html#tuning-random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning random forest in scikit-learn",
    "text": "Tuning random forest in scikit-learn\n\nfrom sklearn.model_selection import GridSearchCV\n\nrf_param_grid = {'randomforestclassifier__n_estimators': [200],\n              'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],\n              'randomforestclassifier__max_features': [1, 2, 3, 4, 5, 6, 7]}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\nrf_tune_grid.fit(\n    X_train_drop_na,\n    y_train_drop_na\n)\n\ncv_10_rf_tuned_metrics = pd.DataFrame(rf_tune_grid.cv_results_)\nresults_rf_tuned = pd.DataFrame({'mean' : rf_tune_grid.best_score_,\n  'sem' : pd.DataFrame(rf_tune_grid.cv_results_)['std_test_score'][6] / 10**(1/2)},\n  index = ['Random forest tuned']\n)\nresults = pd.concat([results, results_rf_tuned])\n\n\nresults"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#tuning-gradientboostingclassifier-with-scikit-learn",
    "href": "materials/slides/ensembles-code.html#tuning-gradientboostingclassifier-with-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning GradientBoostingClassifier with scikit-learn",
    "text": "Tuning GradientBoostingClassifier with scikit-learn\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosted_classifier = GradientBoostingClassifier(random_state=2026)\ngb_pipeline = make_pipeline(heart_preprocessor, gradient_boosted_classifier)\ngb_param_grid = {'gradientboostingclassifier__n_estimators': [200],\n              'gradientboostingclassifier__max_depth': [1, 3, 5, 7, 9],\n              'gradientboostingclassifier__learning_rate': [0.001, 0.005, 0.01]}\ngb_tune_grid = GridSearchCV(\n    estimator=gb_pipeline,\n    param_grid=gb_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\ngb_tune_grid.fit(\n    X_train_drop_na,\n    y_train_drop_na\n)\n\ncv_10_gb_tuned_metrics = pd.DataFrame(gb_tune_grid.cv_results_)\nresults_gb_tuned = pd.DataFrame({'mean' : gb_tune_grid.best_score_,\n  'sem' : pd.DataFrame(gb_tune_grid.cv_results_)['std_test_score'][6] / 10**(1/2)},\n  index = ['Gradient boosted classifier tuned']\n)\nresults = pd.concat([results, results_gb_tuned])\n\n\nresults"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#precision-and-recall-on-the-tuned-random-forest-model",
    "href": "materials/slides/ensembles-code.html#precision-and-recall-on-the-tuned-random-forest-model",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall on the tuned random forest model",
    "text": "Precision and recall on the tuned random forest model\n\nfrom sklearn.metrics import make_scorer, precision_score, recall_score\n\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score, pos_label='Yes'),\n    'recall': make_scorer(recall_score, pos_label='Yes')\n}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1,\n    scoring=scoring,\n    refit='accuracy'\n)\n\nrf_tune_grid.fit(X_train_drop_na, y_train_drop_na)\n\n\ncv_results = pd.DataFrame(rf_tune_grid.cv_results_)\n\nmean_precision = cv_results['mean_test_precision'].iloc[rf_tune_grid.best_index_]\nsem_precision = cv_results['std_test_precision'].iloc[rf_tune_grid.best_index_] / np.sqrt(10)\nmean_recall = cv_results['mean_test_recall'].iloc[rf_tune_grid.best_index_]\nsem_recall = cv_results['std_test_recall'].iloc[rf_tune_grid.best_index_] / np.sqrt(10)\n\nresults_rf_tuned = pd.DataFrame({\n    'mean': [rf_tune_grid.best_score_, mean_precision, mean_recall],\n    'sem': [cv_results['std_test_accuracy'].iloc[rf_tune_grid.best_index_] / np.sqrt(10), sem_precision, sem_recall],\n}, index=['accuracy', 'precision', 'recall'])\n\nresults_rf_tuned"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#feature-importances-in-scikit-learn",
    "href": "materials/slides/ensembles-code.html#feature-importances-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances in scikit-learn",
    "text": "Feature importances in scikit-learn\n\n# Access the best pipeline\nbest_pipeline = rf_tune_grid.best_estimator_\n\n# Extract the trained RandomForestClassifier from the pipeline\nbest_rf = best_pipeline.named_steps['randomforestclassifier']\n\n# Extract feature names after preprocessing\n# Get the names of features from each transformer in the pipeline\nnumeric_features = numeric_feats\ncategorical_feature_names = best_pipeline.named_steps['columntransformer'].transformers_[2][1].get_feature_names_out(categorical_feats)\npassthrough_features = passthrough_feats\n\n# Combine all feature names into a single list\nfeature_names = np.concatenate([numeric_features, passthrough_features, categorical_feature_names])\n\n# Calculate feature importances\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame to display feature importances\nimportances_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# Sort by importance (descending order)\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#visualizing-the-results",
    "href": "materials/slides/ensembles-code.html#visualizing-the-results",
    "title": "Tree-based and ensemble models",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\nimport altair as alt\n\nbar_chart = alt.Chart(importances_df).mark_bar().encode(\n    x=alt.X('Importance:Q', title='Feature Importance'),\n    y=alt.Y('Feature:N', sort='-x', title='Feature'),\n    tooltip=['Feature', 'Importance']\n).properties(\n    title='Feature Importances from Random Forest Model',\n    width=600,\n    height=400\n)\nbar_chart"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#evaluating-on-the-test-set",
    "href": "materials/slides/ensembles-code.html#evaluating-on-the-test-set",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\nheart_test_drop_na = heart_test.dropna()\nX_test_drop_na = heart_test_drop_na.drop(columns=['AHD'])\ny_test_drop_na = heart_test_drop_na['AHD']\n\nheart_test_drop_na[\"predicted\"] = rf_tune_grid.predict(\n    X_test_drop_na\n)\n\nAccuracy\n\nrf_tune_grid.score(\n    X_test_drop_na,\n    y_test_drop_na\n)\n\nPrecision\n\nprecision_score(\n    y_true=heart_test_drop_na[\"AHD\"],\n    y_pred=heart_test_drop_na[\"predicted\"],\n    pos_label='Yes'\n)\n\nRecall\n\nrecall_score(\n    y_true=heart_test_drop_na[\"AHD\"],\n    y_pred=heart_test_drop_na[\"predicted\"],\n    pos_label='Yes'\n)\n\nConfusion matrix\n\nconf_matrix = pd.crosstab(\n    heart_test_drop_na[\"AHD\"],\n    heart_test_drop_na[\"predicted\"]\n)\nprint(conf_matrix)"
  },
  {
    "objectID": "materials/slides/ensembles-code.html#referencesgareth-james-daniela-witten-trevor-hastie-robert-tibshirani-and-jonathan-taylor.-an-introduction-to-statistical-learning-with-applications-in-python.-springer-1st-edition-2023.-url-httpswww.statlearning.com.kolhatkar-v.-and-ostblom-j.-2024.-ubc-dsci-573-feature-and-model-selection-course-notes.-url-httpsubc-mds.github.iodsci_573_feat-model-selectpedregosa-f.-et-al.-2011.-scikit-learn-machine-learning-in-python.-journal-of-machine-learning-research-12oct-pp.28252830.",
    "href": "materials/slides/ensembles-code.html#referencesgareth-james-daniela-witten-trevor-hastie-robert-tibshirani-and-jonathan-taylor.-an-introduction-to-statistical-learning-with-applications-in-python.-springer-1st-edition-2023.-url-httpswww.statlearning.com.kolhatkar-v.-and-ostblom-j.-2024.-ubc-dsci-573-feature-and-model-selection-course-notes.-url-httpsubc-mds.github.iodsci_573_feat-model-selectpedregosa-f.-et-al.-2011.-scikit-learn-machine-learning-in-python.-journal-of-machine-learning-research-12oct-pp.28252830.",
    "title": "Tree-based and ensemble models",
    "section": "ReferencesGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-selectPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830.",
    "text": "ReferencesGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-selectPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830."
  },
  {
    "objectID": "materials/slides/classification2.html#session-learning-objectives",
    "href": "materials/slides/classification2.html#session-learning-objectives",
    "title": "Classification II: evaluation & tuning",
    "section": "Session learning objectives",
    "text": "Session learning objectives\nBy the end of the session, learners will be able to do the following:\n\nDescribe what training, validation, and test data sets are and how they are used in classification.\nSplit data into training, validation, and test data sets.\nDescribe what a random seed is and its importance in reproducible data analysis.\nSet the random seed in Python using the numpy.random.seed function.\nDescribe and interpret accuracy, precision, recall, and confusion matrices.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#session-learning-objectives-contd",
    "href": "materials/slides/classification2.html#session-learning-objectives-contd",
    "title": "Classification II: evaluation & tuning",
    "section": "Session learning objectives cont’d",
    "text": "Session learning objectives cont’d\nBy the end of the session, learners will be able to do the following:\n\nEvaluate classification accuracy, precision, and recall in Python using a test set, a single validation set, and cross-validation.\nProduce a confusion matrix in Python.\nChoose the number of neighbors in a K-nearest neighbors classifier by maximizing estimated cross-validation accuracy.\nDescribe underfitting and overfitting, and relate it to the number of neighbors in K-nearest neighbors classification.\nDescribe the advantages and disadvantages of the K-nearest neighbors classification algorithm.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluating-performance",
    "href": "materials/slides/classification2.html#evaluating-performance",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluating performance",
    "text": "Evaluating performance\n\nSometimes our classifier might make the wrong prediction.\nA classifier does not need to be right 100% of the time to be useful, though we don’t want the classifier to make too many wrong predictions.\nHow do we measure how “good” our classifier is?",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#data-splitting",
    "href": "materials/slides/classification2.html#data-splitting",
    "title": "Classification II: evaluation & tuning",
    "section": "Data splitting",
    "text": "Data splitting\n\nThe trick is to split the data into a training set and test set.\nOnly the training set when building the classifier.\nTo evaluate the performance of the classifier, we first set aside the labels from the test set, and then use the classifier to predict the labels in the test set.\nIf our predictions match the actual labels for the observations in the test set, then we have some confidence that our classifier might also accurately predict the class labels for new observations without known class labels.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#splitting-the-data-into-training-and-testing-sets",
    "href": "materials/slides/classification2.html#splitting-the-data-into-training-and-testing-sets",
    "title": "Classification II: evaluation & tuning",
    "section": "Splitting the data into training and testing sets",
    "text": "Splitting the data into training and testing sets",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#prediction-accuracy",
    "href": "materials/slides/classification2.html#prediction-accuracy",
    "title": "Classification II: evaluation & tuning",
    "section": "Prediction accuracy",
    "text": "Prediction accuracy\n\\[\\mathrm{accuracy} = \\frac{\\mathrm{number \\; of  \\; correct  \\; predictions}}{\\mathrm{total \\;  number \\;  of  \\; predictions}}\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#is-knowing-accuracy-enough",
    "href": "materials/slides/classification2.html#is-knowing-accuracy-enough",
    "title": "Classification II: evaluation & tuning",
    "section": "Is knowing accuracy enough?",
    "text": "Is knowing accuracy enough?\n\nExample accuracy calculation:\n\n\\[\\mathrm{accuracy} = \\frac{\\mathrm{number \\; of  \\; correct  \\; predictions}}{\\mathrm{total \\;  number \\;  of  \\; predictions}} = \\frac{58}{65} = 0.892\\]\n\nPrediction accuracy only tells us how often the classifier makes mistakes in general, but does not tell us anything about the kinds of mistakes the classifier makes.\nThe confusion matrix tells a more complete story.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#example-confusion-matrix-for-the-breast-cancer-data",
    "href": "materials/slides/classification2.html#example-confusion-matrix-for-the-breast-cancer-data",
    "title": "Classification II: evaluation & tuning",
    "section": "Example confusion matrix for the breast cancer data:",
    "text": "Example confusion matrix for the breast cancer data:\n\n\n\n\n\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActually Malignant\n1\n3\n\n\nActually Benign\n4\n57\n\n\n\n\nTrue Positive: A malignant observation that was classified as malignant (top left).\nFalse Positive: A benign observation that was classified as malignant (bottom left).\nTrue Negative: A benign observation that was classified as benign (bottom right).\nFalse Negative: A malignant observation that was classified as benign (top right).",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#precision-recall",
    "href": "materials/slides/classification2.html#precision-recall",
    "title": "Classification II: evaluation & tuning",
    "section": "Precision & recall",
    "text": "Precision & recall\n\nPrecision quantifies how many of the positive predictions the classifier made were actually positive.\n\n\\[\\mathrm{precision} = \\frac{\\mathrm{number \\; of  \\; correct \\; positive \\; predictions}}{\\mathrm{total \\;  number \\;  of \\; positive  \\; predictions}}\\]\n\nRecall quantifies how many of the positive observations in the test set were identified as positive.\n\n\\[\\mathrm{recall} = \\frac{\\mathrm{number \\; of  \\; correct  \\; positive \\; predictions}}{\\mathrm{total \\;  number \\;  of  \\; positive \\; test \\; set \\; observations}}\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#precision-and-recall-for-the-breast-cancer-data-set-example",
    "href": "materials/slides/classification2.html#precision-and-recall-for-the-breast-cancer-data-set-example",
    "title": "Classification II: evaluation & tuning",
    "section": "Precision and recall for the breast cancer data set example",
    "text": "Precision and recall for the breast cancer data set example\n\n\n\n\n\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActually Malignant\n1\n3\n\n\nActually Benign\n4\n57\n\n\n\n\\[\\mathrm{precision} = \\frac{1}{1+4} = 0.20, \\quad \\mathrm{recall} = \\frac{1}{1+3} = 0.25\\]\n\nSo even with an accuracy of 89%, the precision and recall of the classifier were both relatively low. For this data analysis context, recall is particularly important: if someone has a malignant tumor, we certainly want to identify it. A recall of just 25% would likely be unacceptable!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#randomness-and-seeds",
    "href": "materials/slides/classification2.html#randomness-and-seeds",
    "title": "Classification II: evaluation & tuning",
    "section": "Randomness and seeds",
    "text": "Randomness and seeds\n\nOur data analyses will often involve the use of randomness\nWe use randomness any time we need to make a decision in our analysis that needs to be fair, unbiased, and not influenced by human input (e.g., splitting into training and test sets).\nHowever, the use of randomness runs counter to one of the main tenets of good data analysis practice: reproducibility…\nThe trick is that in Python—and other programming languages—randomness is not actually random! Instead, Python uses a random number generator that produces a sequence of numbers that are completely determined by a seed value.\nOnce you set the seed value, everything after that point may look random, but is actually totally reproducible.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#setting-the-seed-in-python",
    "href": "materials/slides/classification2.html#setting-the-seed-in-python",
    "title": "Classification II: evaluation & tuning",
    "section": "Setting the seed in Python",
    "text": "Setting the seed in Python\nLet’s say we want to make a series object containing the integers from 0 to 9. And then we want to randomly pick 10 numbers from that list, but we want it to be reproducible.\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(1)\n\nnums_0_to_9 = pd.Series([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nrandom_numbers1 = nums_0_to_9.sample(n=10).to_list()\nrandom_numbers1\n\n[2, 9, 6, 4, 0, 3, 1, 7, 8, 5]\n\n\nYou can see that random_numbers1 is a list of 10 numbers from 0 to 9 that, from all appearances, looks random. If we run the sample method again, we will get a fresh batch of 10 numbers that also look random.\n\nrandom_numbers2 = nums_0_to_9.sample(n=10).to_list()\nrandom_numbers2\n\n[9, 5, 3, 0, 8, 4, 2, 1, 6, 7]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#setting-the-seed-in-python-contd",
    "href": "materials/slides/classification2.html#setting-the-seed-in-python-contd",
    "title": "Classification II: evaluation & tuning",
    "section": "Setting the seed in Python (cont’d)",
    "text": "Setting the seed in Python (cont’d)\nIf we choose a different value for the seed—say, 4235—we obtain a different sequence of random numbers:\n\n# seed was 1 on the last slide when we generated this list\nrandom_numbers1\n\n[2, 9, 6, 4, 0, 3, 1, 7, 8, 5]\n\n\n\nnp.random.seed(4235)\nrandom_numbers1_different = nums_0_to_9.sample(n=10).to_list()\n\nrandom_numbers1\nrandom_numbers1_different\n\n[6, 7, 2, 3, 5, 9, 1, 4, 0, 8]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#back-to-the-breast-cancer-data-set-example",
    "href": "materials/slides/classification2.html#back-to-the-breast-cancer-data-set-example",
    "title": "Classification II: evaluation & tuning",
    "section": "Back to the breast cancer data set example",
    "text": "Back to the breast cancer data set example\n\n# load packages\nimport altair as alt\nimport pandas as pd\nfrom sklearn import set_config\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")\n\n# set the seed\nnp.random.seed(3)\n\n# load data\ncancer = pd.read_csv(\"data/wdbc_unscaled.csv\")\n# re-label Class \"M\" as \"Malignant\", and Class \"B\" as \"Benign\"\ncancer[\"Class\"] = cancer[\"Class\"].replace({\n    \"M\" : \"Malignant\",\n    \"B\" : \"Benign\"\n})",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#breast-cancer-data",
    "href": "materials/slides/classification2.html#breast-cancer-data",
    "title": "Classification II: evaluation & tuning",
    "section": "Breast cancer data",
    "text": "Breast cancer data\n\n\n\nperim_concav = alt.Chart(cancer).mark_circle().encode(\n    x=alt.X(\"Smoothness\").scale(zero=False),\n    y=\"Concavity\",\n    color=alt.Color(\"Class\").title(\"Diagnosis\")\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#create-the-train-test-split",
    "href": "materials/slides/classification2.html#create-the-train-test-split",
    "title": "Classification II: evaluation & tuning",
    "section": "Create the train / test split",
    "text": "Create the train / test split\n\nBefore fitting any models, or doing exploratory data analysis, it is critical that you split the data into training and test sets.\nTypically, the training set is between 50% and 95% of the data, while the test set is the remaining 5% to 50%.\nThe train_test_split function from scikit-learn handles the procedure of splitting the data for us.\nUse shuffle=True to remove the influence of order in the data set.\nSet the stratify parameter to be the response variable to ensure the same proportion of each class ends up in both the training and testing sets.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#splitting-the-breast-cancer-data-set",
    "href": "materials/slides/classification2.html#splitting-the-breast-cancer-data-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Splitting the breast cancer data set",
    "text": "Splitting the breast cancer data set\n\nSplit the data so 75% are in the training set, and 25% in the test set\nData are shuffled\nSplit is stratified on the Class variable\n\n\nfrom sklearn.model_selection import train_test_split\n\ncancer_train, cancer_test = train_test_split(\n    cancer, train_size=0.75, stratify=cancer[\"Class\"]\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#checking-the-splits",
    "href": "materials/slides/classification2.html#checking-the-splits",
    "title": "Classification II: evaluation & tuning",
    "section": "Checking the splits",
    "text": "Checking the splits\n\nWe can use .info() to look at the splits\nLet’s look at the training split (in practice you look at both)\n\n\ncancer_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 426 entries, 196 to 296\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   ID                 426 non-null    int64  \n 1   Class              426 non-null    object \n 2   Radius             426 non-null    float64\n 3   Texture            426 non-null    float64\n 4   Perimeter          426 non-null    float64\n 5   Area               426 non-null    float64\n 6   Smoothness         426 non-null    float64\n 7   Compactness        426 non-null    float64\n 8   Concavity          426 non-null    float64\n 9   Concave_Points     426 non-null    float64\n 10  Symmetry           426 non-null    float64\n 11  Fractal_Dimension  426 non-null    float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 43.3+ KB",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#checking-the-splits-1",
    "href": "materials/slides/classification2.html#checking-the-splits-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Checking the splits",
    "text": "Checking the splits\n\nWe can use the value_counts method with the normalize argument set to True to find the percentage of malignant and benign classes in cancer_train.\nWe can see our class proportions were roughly preserved when we split the data.\n\n\ncancer_train[\"Class\"].value_counts(normalize=True)\n\nClass\nBenign       0.626761\nMalignant    0.373239\nName: proportion, dtype: float64",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#preprocessing-with-data-splitting",
    "href": "materials/slides/classification2.html#preprocessing-with-data-splitting",
    "title": "Classification II: evaluation & tuning",
    "section": "Preprocessing with data splitting",
    "text": "Preprocessing with data splitting\n\nMany machine learning models are sensitive to the scale of the predictors, and even if not, comparison of importance of features for prediction after fitting requires scaling.\nWhen preprocessing the data (scaling is part of this), it is critical that we use only the training set in creating the mathematical function to do this.\nIf this is not done, we will get overly optimistic test accuracy, as our test data will have influenced our model.\nAfter creating the preprocessing function, we can then apply it separately to both the training and test data sets.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#preprocessing-with-scikit-learn",
    "href": "materials/slides/classification2.html#preprocessing-with-scikit-learn",
    "title": "Classification II: evaluation & tuning",
    "section": "Preprocessing with scikit-learn",
    "text": "Preprocessing with scikit-learn\n\nscikit-learn helps us handle this properly as long as we wrap our analysis steps in a Pipeline.\nSpecifically, we construct and prepare the preprocessor using make_column_transformer:\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\ncancer_preprocessor = make_column_transformer(\n    (StandardScaler(), [\"Smoothness\", \"Concavity\"]),\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#train-the-classifier",
    "href": "materials/slides/classification2.html#train-the-classifier",
    "title": "Classification II: evaluation & tuning",
    "section": "Train the classifier",
    "text": "Train the classifier\n\n\n\nNow we can create our K-nearest neighbors classifier with only the training set.\nFor simplicity, we will just choose \\(K\\) = 3, and use only the concavity and smoothness predictors.\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nX = cancer_train[[\"Smoothness\", \"Concavity\"]]\ny = cancer_train[\"Class\"]\n\nknn_pipeline = make_pipeline(cancer_preprocessor, knn)\nknn_pipeline.fit(X, y)\n\nknn_pipeline\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['Smoothness',\n                                                   'Concavity'])])),\n                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=3))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['Smoothness',\n                                                   'Concavity'])])),\n                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=3))])  columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['Smoothness', 'Concavity'])]) standardscaler['Smoothness', 'Concavity']  StandardScaler?Documentation for StandardScalerStandardScaler()  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#predict-the-labels-in-the-test-set",
    "href": "materials/slides/classification2.html#predict-the-labels-in-the-test-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Predict the labels in the test set",
    "text": "Predict the labels in the test set\nNow that we have a K-nearest neighbors classifier object, we can use it to predict the class labels for our test set:\n\ncancer_test[\"predicted\"] = knn_pipeline.predict(cancer_test[[\"Smoothness\", \"Concavity\"]])\nprint(cancer_test[[\"ID\", \"Class\", \"predicted\"]])\n\n           ID      Class  predicted\n116    864726     Benign  Malignant\n146    869691  Malignant  Malignant\n..        ...        ...        ...\n281   8912055     Benign     Benign\n15   84799002  Malignant  Malignant\n\n[143 rows x 3 columns]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluate-performance",
    "href": "materials/slides/classification2.html#evaluate-performance",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluate performance",
    "text": "Evaluate performance\nTo evaluate the model, we will look at:\n\naccuracy\nprecision\nrecall\nconfusion matrix\ncompare to baseline model (majority classifier)\n\nAll of these together, will help us develop a fuller picture of how the model is performing, as opposed to only evaluating the model based on a single metric or table.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#accuracy-precision-and-recall",
    "href": "materials/slides/classification2.html#accuracy-precision-and-recall",
    "title": "Classification II: evaluation & tuning",
    "section": "Accuracy, precision and recall",
    "text": "Accuracy, precision and recall\n\nknn_pipeline.score(\n    cancer_test[[\"Smoothness\", \"Concavity\"]],\n    cancer_test[\"Class\"]\n)\n\n0.8951048951048951\n\n\n\nfrom sklearn.metrics import recall_score, precision_score\n\nprecision_score(\n    y_true=cancer_test[\"Class\"],\n    y_pred=cancer_test[\"predicted\"],\n    pos_label=\"Malignant\"\n)\n\nnp.float64(0.8275862068965517)\n\n\n\nrecall_score(\n    y_true=cancer_test[\"Class\"],\n    y_pred=cancer_test[\"predicted\"],\n    pos_label=\"Malignant\"\n)\n\nnp.float64(0.9056603773584906)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#confusion-matrix",
    "href": "materials/slides/classification2.html#confusion-matrix",
    "title": "Classification II: evaluation & tuning",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nWe can look at the confusion matrix for the classifier using the crosstab function from pandas.\nThe crosstab function takes two arguments: the actual labels first, then the predicted labels second.\nNote that crosstab orders its columns alphabetically, but the positive label is still Malignant, even if it is not in the top left corner as in the table shown earlier.\n\n\npd.crosstab(\n    cancer_test[\"Class\"],\n    cancer_test[\"predicted\"]\n)\n\n\n\n\n\n\n\npredicted\nBenign\nMalignant\n\n\nClass\n\n\n\n\n\n\nBenign\n80\n10\n\n\nMalignant\n5\n48",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#critically-analyze-performance",
    "href": "materials/slides/classification2.html#critically-analyze-performance",
    "title": "Classification II: evaluation & tuning",
    "section": "Critically analyze performance",
    "text": "Critically analyze performance\n\nIs 90% accuracy, a precision of 83% and a recall of 91% good enough?\nTo get a sense of scale, we often compare our model to a baseline model. In the case of classification, this would be the majority classifier (always guesses the majority class label from the training data).\nFor the breast cancer training data, the baseline classifier’s accuracy would be 63%\n\n\ncancer_train[\"Class\"].value_counts(normalize=True)\n\nClass\nBenign       0.626761\nMalignant    0.373239\nName: proportion, dtype: float64\n\n\n\nSo we do see that our model is doing a LOT better than the baseline, which is great, but considering our application domain is in cancer diagnosis, we still have a ways to go…\nAnalyzing model performance really depends on your application!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#tuning-the-classifier",
    "href": "materials/slides/classification2.html#tuning-the-classifier",
    "title": "Classification II: evaluation & tuning",
    "section": "Tuning the classifier",
    "text": "Tuning the classifier\n\nMost predictive models in statistics and machine learning have parameters (a number you have to pick in advance that determines some aspect of how the model behaves).\nFor our working example, \\(K\\)-nearest neighbors classification algorithm, \\(K\\) is a parameter that we have to pick that determines how many neighbors participate in the class vote.\nHow do we choose \\(K\\), or any parameter for other models?\nData splitting!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#validation-set",
    "href": "materials/slides/classification2.html#validation-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Validation set",
    "text": "Validation set\n\nCannot use the test set to choose the parameter!\nBut we can split the training set into two partitions, a traning set and a validation set.\nFor each parameter value we want to assess, we can fit on the training set, and evaluate on the validation set.\nThen after we find the best value for our parameter, we can refit the model with the best parameter on the entire training set and then evaluate our model on the test set.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#can-we-do-better",
    "href": "materials/slides/classification2.html#can-we-do-better",
    "title": "Classification II: evaluation & tuning",
    "section": "Can we do better?",
    "text": "Can we do better?\n\nDepending on how we split the data into the training and validation sets, we might get a lucky split (or an unlucky one) that doesn’t give us a good estimate of the model’s true accuracy.\nIn many cases, we can do better by making many splits, and averaging the accuracy scores to get a better estimate.\nWe call this cross-validation.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#cross-validation",
    "href": "materials/slides/classification2.html#cross-validation",
    "title": "Classification II: evaluation & tuning",
    "section": "Cross-validation",
    "text": "Cross-validation\nAn example of 5-fold cross-validation:",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#cross-validation-in-scikit-learn",
    "href": "materials/slides/classification2.html#cross-validation-in-scikit-learn",
    "title": "Classification II: evaluation & tuning",
    "section": "Cross-validation in scikit-learn",
    "text": "Cross-validation in scikit-learn\n\n\n\nUse the scikit-learn cross_validate function.\nNeed to specify:\n\na modelling Pipeline as the estimator argument,\nthe number of folds as the cv argument,\nthe training data predictors as the X argument\nthe labels as the y arguments.\n\nNote that the cross_validate function handles stratifying the classes in each train and validate fold automatically.\n\n\n\nfrom sklearn.model_selection import cross_validate\n\nknn = KNeighborsClassifier(n_neighbors=3)\ncancer_pipe = make_pipeline(cancer_preprocessor, knn)\nX = cancer_train[[\"Smoothness\", \"Concavity\"]]\ny = cancer_train[\"Class\"]\ncv_10_df = pd.DataFrame(\n    cross_validate(\n        estimator=cancer_pipe,\n        cv=10,\n        X=X,\n        y=y\n    )\n)\n\nprint(cv_10_df)\n\n    fit_time  score_time  test_score\n0   0.004684    0.006370    0.860465\n1   0.004858    0.005101    0.837209\n..       ...         ...         ...\n8   0.003927    0.004565    0.904762\n9   0.004060    0.004545    0.880952\n\n[10 rows x 3 columns]\n\n\n\ncv_10_metrics = cv_10_df.agg([\"mean\", \"sem\"])\ncv_10_metrics['test_score']\n\nmean    0.884939\nsem     0.013360\nName: test_score, dtype: float64",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection",
    "href": "materials/slides/classification2.html#parameter-value-selection",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection\n\nSince cross-validation helps us evaluate the accuracy of our classifier, we can use cross-validation to calculate an accuracy for each value of our parameter, here \\(K\\), in a reasonable range,\nThen we pick the value of \\(K\\) that gives us the best accuracy, and refit the model with our parameter on the training data, and then evaluate on the test data.\nThe scikit-learn package collection provides built-in functionality, named GridSearchCV, to automatically handle the details for us.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection-1",
    "href": "materials/slides/classification2.html#parameter-value-selection-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection\n\nknn = KNeighborsClassifier() #don't specify the number of neighbours\ncancer_tune_pipe = make_pipeline(cancer_preprocessor, knn)\n\nparameter_grid = {\n    \"kneighborsclassifier__n_neighbors\": range(1, 100, 5),\n}\n\nfrom sklearn.model_selection import GridSearchCV\n\ncancer_tune_grid = GridSearchCV(\n    estimator=cancer_tune_pipe,\n    param_grid=parameter_grid,\n    cv=10\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection-2",
    "href": "materials/slides/classification2.html#parameter-value-selection-2",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection\n\n\n\nNow we use the fit method on the GridSearchCV object to begin the tuning process.\n\n\n\ncancer_tune_grid.fit(\n    cancer_train[[\"Smoothness\", \"Concavity\"]],\n    cancer_train[\"Class\"]\n)\naccuracies_grid = pd.DataFrame(cancer_tune_grid.cv_results_)\naccuracies_grid.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20 entries, 0 to 19\nData columns (total 19 columns):\n #   Column                                   Non-Null Count  Dtype  \n---  ------                                   --------------  -----  \n 0   mean_fit_time                            20 non-null     float64\n 1   std_fit_time                             20 non-null     float64\n 2   mean_score_time                          20 non-null     float64\n 3   std_score_time                           20 non-null     float64\n 4   param_kneighborsclassifier__n_neighbors  20 non-null     int64  \n 5   params                                   20 non-null     object \n 6   split0_test_score                        20 non-null     float64\n 7   split1_test_score                        20 non-null     float64\n 8   split2_test_score                        20 non-null     float64\n 9   split3_test_score                        20 non-null     float64\n 10  split4_test_score                        20 non-null     float64\n 11  split5_test_score                        20 non-null     float64\n 12  split6_test_score                        20 non-null     float64\n 13  split7_test_score                        20 non-null     float64\n 14  split8_test_score                        20 non-null     float64\n 15  split9_test_score                        20 non-null     float64\n 16  mean_test_score                          20 non-null     float64\n 17  std_test_score                           20 non-null     float64\n 18  rank_test_score                          20 non-null     int32  \ndtypes: float64(16), int32(1), int64(1), object(1)\nmemory usage: 3.0+ KB",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection-3",
    "href": "materials/slides/classification2.html#parameter-value-selection-3",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection\n\naccuracies_grid[\"sem_test_score\"] = accuracies_grid[\"std_test_score\"] / 10**(1/2)\naccuracies_grid = (\n    accuracies_grid[[\n        \"param_kneighborsclassifier__n_neighbors\",\n        \"mean_test_score\",\n        \"sem_test_score\"\n    ]]\n    .rename(columns={\"param_kneighborsclassifier__n_neighbors\": \"n_neighbors\"})\n)\nprint(accuracies_grid)\n\n    n_neighbors  mean_test_score  sem_test_score\n0             1         0.845127        0.019966\n1             6         0.873200        0.015680\n..          ...              ...             ...\n18           91         0.875581        0.012967\n19           96         0.875581        0.008193\n\n[20 rows x 3 columns]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#visualize-paramter-value-selection",
    "href": "materials/slides/classification2.html#visualize-paramter-value-selection",
    "title": "Classification II: evaluation & tuning",
    "section": "Visualize paramter value selection",
    "text": "Visualize paramter value selection\n\n\n\naccuracy_vs_k = (\n    alt.Chart(accuracies_grid)\n    .mark_line(point=True)\n    .encode(\n        x=alt.X(\"n_neighbors\")\n        .title(\"Neighbors\"),\n        y=alt.Y(\"mean_test_score\")\n        .scale(zero=False)\n        .title(\"Accuracy estimate\")\n    )\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#best-parameter-value",
    "href": "materials/slides/classification2.html#best-parameter-value",
    "title": "Classification II: evaluation & tuning",
    "section": "Best parameter value",
    "text": "Best parameter value\n\nWe can also obtain the number of neighbours with the highest accuracy programmatically by accessing the best_params_ attribute of the fit GridSearchCV object.\n\n\ncancer_tune_grid.best_params_\n\n{'kneighborsclassifier__n_neighbors': 36}",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#best-parameter-value-1",
    "href": "materials/slides/classification2.html#best-parameter-value-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Best parameter value",
    "text": "Best parameter value\n\n\nDo we use \\(K\\) = 36?\nGenerally, when selecting a parameters, we are looking for a value where:\n\nwe get roughly optimal accuracy\nchanging the value to a nearby one doesn’t change the accuracy too much\nthe cost of training the model is not prohibitive",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#underoverfitting",
    "href": "materials/slides/classification2.html#underoverfitting",
    "title": "Classification II: evaluation & tuning",
    "section": "Under/Overfitting",
    "text": "Under/Overfitting\n\nWhat happens if we keep increasing the number of neighbors \\(K\\)?\nThe cross-validation accuracy estimate actually starts to decrease!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluating-on-the-test-set",
    "href": "materials/slides/classification2.html#evaluating-on-the-test-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\n\n\nBefore we evaluate on the test set, we need to refit the model using the best parameter(s) on the entire training set\nLuckily, scikit-learn does it for us automatically!\nTo make predictions and assess the estimated accuracy of the best model on the test data, we can use the score and predict methods of the fit GridSearchCV object.\n\n\n\ncancer_test[\"predicted\"] = cancer_tune_grid.predict(\n    cancer_test[[\"Smoothness\", \"Concavity\"]]\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluating-on-the-test-set-1",
    "href": "materials/slides/classification2.html#evaluating-on-the-test-set-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\nWe can then pass those predictions to the precision, recall, and crosstab functions to assess the estimated precision and recall, and print a confusion matrix.\n\n\n\n\ncancer_tune_grid.score(\n    cancer_test[[\"Smoothness\", \"Concavity\"]],\n    cancer_test[\"Class\"]\n)\n\n0.9090909090909091\n\n\n\nprecision_score(\n    y_true=cancer_test[\"Class\"],\n    y_pred=cancer_test[\"predicted\"],\n    pos_label='Malignant'\n)\n\nnp.float64(0.8846153846153846)\n\n\n\nrecall_score(\n    y_true=cancer_test[\"Class\"],\n    y_pred=cancer_test[\"predicted\"],\n    pos_label='Malignant'\n)\n\nnp.float64(0.8679245283018868)\n\n\n\n\nconf_matrix = pd.crosstab(\n    cancer_test[\"Class\"],\n    cancer_test[\"predicted\"]\n)\nprint(conf_matrix)\n\npredicted  Benign  Malignant\nClass                       \nBenign         84          6\nMalignant       7         46",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#summary",
    "href": "materials/slides/classification2.html#summary",
    "title": "Classification II: evaluation & tuning",
    "section": "Summary",
    "text": "Summary",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#k-nearest-neighbors-classification-algorithm",
    "href": "materials/slides/classification2.html#k-nearest-neighbors-classification-algorithm",
    "title": "Classification II: evaluation & tuning",
    "section": "K-nearest neighbors classification algorithm",
    "text": "K-nearest neighbors classification algorithm\nStrengths: K-nearest neighbors classification\n\nis a simple, intuitive algorithm,\nrequires few assumptions about what the data must look like, and\nworks for binary (two-class) and multi-class (more than 2 classes) classification problems.\n\nWeaknesses: K-nearest neighbors classification\n\nbecomes very slow as the training data gets larger,\nmay not perform well with a large number of predictors, and\nmay not perform well when classes are imbalanced.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#other-classification-algorithms",
    "href": "materials/slides/classification2.html#other-classification-algorithms",
    "title": "Classification II: evaluation & tuning",
    "section": "Other classification algorithms",
    "text": "Other classification algorithms\n\nscikit-learn classification documentation: https://scikit-learn.org/stable/supervised_learning.html",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#additional-resources",
    "href": "materials/slides/classification2.html#additional-resources",
    "title": "Classification II: evaluation & tuning",
    "section": "Additional resources",
    "text": "Additional resources\n\nThe Classification II: evaluation & tuning chapter of Data Science: A First Introduction (Python Edition) by Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy contains all the content presented here with a detailed narrative.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in the past two chapters. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning {cite:p}james2013introduction provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#references",
    "href": "materials/slides/classification2.html#references",
    "title": "Classification II: evaluation & tuning",
    "section": "References",
    "text": "References\nEvelyn Martin Lansdowne Beale, Maurice George Kendall, and David Mann. The discarding of variables in multivariate analysis. Biometrika, 54(3-4):357–366, 1967.\nNorman Draper and Harry Smith. Applied Regression Analysis. Wiley, 1966.\nM. Eforymson. Stepwise regression—a backward and forward look. In Eastern Regional Meetings of the Institute of Mathematical Statistics. 1966.\nRonald Hocking and R. N. Leslie. Selection of the best subset in regression analysis. Technometrics, 9(4):531–540, 1967.\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning. Springer, 1st edition, 2013. URL: https://www.statlearning.com/.\nWes McKinney. Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. ” O’Reilly Media, Inc.”, 2012.\nWilliam Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In International Symposium on Electronic Imaging: Science and Technology. 1993.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tree-based-methods",
    "href": "materials/slides/ensembles.html#tree-based-methods",
    "title": "Tree-based and ensemble models",
    "section": "Tree-based methods",
    "text": "Tree-based methods\n\nAlgorithms that stratifying or segmenting the predictor space into a number of simple regions.\nWe call these algorithms decision-tree methods because the decisions used to segment the predictor space can be summarized in a tree.\nDecision trees on their own, are very explainable and intuitive, but not very powerful at predicting.\nHowever, there are extensions of decision trees, such as random forest and boosted trees, which are very powerful at predicting. We will demonstrate two of these in this session.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#decision-trees",
    "href": "materials/slides/ensembles.html#decision-trees",
    "title": "Tree-based and ensemble models",
    "section": "Decision trees",
    "text": "Decision trees\n\nDecision Trees\nby Jared Wilber & Lucía Santamaría",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#classification-decision-trees",
    "href": "materials/slides/ensembles.html#classification-decision-trees",
    "title": "Tree-based and ensemble models",
    "section": "Classification Decision trees",
    "text": "Classification Decision trees\n\nUse recursive binary splitting to grow a classification tree (splitting of the predictor space into \\(J\\) distinct, non-overlapping regions).\nFor every observation that falls into the region \\(R_j\\) , we make the same prediction, which is the majority vote for the training observations in \\(R_j\\).\nWhere to split the predictor space is done in a top-down and greedy manner, and in practice for classification, the best split at any point in the algorithm is one that minimizes the Gini index (a measure of node purity).\nDecision trees are useful because they are very interpretable.\nA limitation of decision trees is that theyn tend to overfit, so in practice we use cross-validation to tune a hyperparameter, \\(\\alpha\\), to find the optimal, pruned tree.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#example-the-heart-data-set",
    "href": "materials/slides/ensembles.html#example-the-heart-data-set",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set",
    "text": "Example: the heart data set\n\n\n\nLet’s consider a situation where we’d like to be able to predict the presence of heart disease (AHD) in patients, based off 13 measured characteristics.\nThe heart data set contains a binary outcome for heart disease for patients who presented with chest pain.\n\n\n\nimport pandas as pd\nheart = pd.read_csv(\"data/Heart.csv\", index_col=0)\nheart.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 303 entries, 1 to 303\nData columns (total 14 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Age        303 non-null    int64  \n 1   Sex        303 non-null    int64  \n 2   ChestPain  303 non-null    object \n 3   RestBP     303 non-null    int64  \n 4   Chol       303 non-null    int64  \n 5   Fbs        303 non-null    int64  \n 6   RestECG    303 non-null    int64  \n 7   MaxHR      303 non-null    int64  \n 8   ExAng      303 non-null    int64  \n 9   Oldpeak    303 non-null    float64\n 10  Slope      303 non-null    int64  \n 11  Ca         299 non-null    float64\n 12  Thal       301 non-null    object \n 13  AHD        303 non-null    object \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 35.5+ KB",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#example-the-heart-data-set-1",
    "href": "materials/slides/ensembles.html#example-the-heart-data-set-1",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set",
    "text": "Example: the heart data set\n\nAn angiographic test was performed and a label for AHD of Yes was labelled to indicate the presence of heart disease, otherwise the label was No.\n\n\nheart.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n1\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n2\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n3\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n4\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n5\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#do-we-have-a-class-imbalance",
    "href": "materials/slides/ensembles.html#do-we-have-a-class-imbalance",
    "title": "Tree-based and ensemble models",
    "section": "Do we have a class imbalance?",
    "text": "Do we have a class imbalance?\nIt’s always important to check this, as it may impact your splitting and/or modeling decisions.\n\nheart['AHD'].value_counts(normalize=True)\n\nAHD\nNo     0.541254\nYes    0.458746\nName: proportion, dtype: float64\n\n\nThis looks pretty good! We can move forward this time without doing much more about this.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#data-splitting",
    "href": "materials/slides/ensembles.html#data-splitting",
    "title": "Tree-based and ensemble models",
    "section": "Data splitting",
    "text": "Data splitting\nLet’s split the data into training and test sets:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(2024)\n\nheart_train, heart_test = train_test_split(\n    heart, train_size=0.8, stratify=heart[\"AHD\"]\n)\n\nX_train = heart_train.drop(columns=['AHD'])\ny_train = heart_train['AHD']\nX_test = heart_test.drop(columns=['AHD'])\ny_test = heart_test['AHD']",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#categorical-variables",
    "href": "materials/slides/ensembles.html#categorical-variables",
    "title": "Tree-based and ensemble models",
    "section": "Categorical variables",
    "text": "Categorical variables\n\n\n\nThis is our first case of seeing categorical predictor variables, can we treat them the same as numerical ones? No!\nIn scikit-learn we must perform one-hot encoding\n\n\n\nSource: https://scales.arabpsychology.com/stats/how-can-i-perform-one-hot-encoding-in-r/",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#look-at-the-data-again",
    "href": "materials/slides/ensembles.html#look-at-the-data-again",
    "title": "Tree-based and ensemble models",
    "section": "Look at the data again",
    "text": "Look at the data again\nWhich columns do we need to standardize?\nWhich do we need to one-hot encode?\n\nheart.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n1\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n2\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n3\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n4\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n5\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#one-hot-encoding-pre-processing",
    "href": "materials/slides/ensembles.html#one-hot-encoding-pre-processing",
    "title": "Tree-based and ensemble models",
    "section": "One hot encoding & pre-processing",
    "text": "One hot encoding & pre-processing\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\nnumeric_feats = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak','Slope', 'Ca']\npassthrough_feats = ['Sex', 'Fbs', 'ExAng']\ncategorical_feats = ['ChestPain', 'Thal']\n\nheart_preprocessor = make_column_transformer(\n    (StandardScaler(), numeric_feats), \n    (\"passthrough\", passthrough_feats),     \n    (OneHotEncoder(handle_unknown = \"ignore\"), categorical_feats),     \n)\n\n\nhandle_unknown = \"ignore\" handles the case where categories exist in the test data, which were missing in the training set. Specifically, it sets the value for those to 0 for all cases of the category.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#fitting-a-dummy-classifier",
    "href": "materials/slides/ensembles.html#fitting-a-dummy-classifier",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a dummy classifier",
    "text": "Fitting a dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\n\ndummy = DummyClassifier()\ndummy_pipeline = make_pipeline(heart_preprocessor, dummy)\ncv_10_dummy = pd.DataFrame(\n    cross_validate(\n        estimator=dummy_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dummy_metrics = cv_10_dummy.agg([\"mean\", \"sem\"])\nresults = pd.DataFrame({'mean' : [cv_10_dummy_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_dummy_metrics.test_score.iloc[1]]},\n  index = ['Dummy classifier']\n)\nresults\n\n\n\n\n\n\n\n\nmean\nsem\n\n\n\n\nDummy classifier\n0.541333\n0.00299",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#fitting-a-decision-tree",
    "href": "materials/slides/ensembles.html#fitting-a-decision-tree",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a decision tree",
    "text": "Fitting a decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=2026)\n\ndt_pipeline = make_pipeline(heart_preprocessor, decision_tree)\ncv_10_dt = pd.DataFrame(\n    cross_validate(\n        estimator=dt_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dt_metrics = cv_10_dt.agg([\"mean\", \"sem\"])\nresults_dt = pd.DataFrame({'mean' : [cv_10_dt_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_dt_metrics.test_score.iloc[1]]},\n  index = ['Decision tree']\n)\nresults = pd.concat([results, results_dt])\nresults\n\n\n\n\n\n\n\n\nmean\nsem\n\n\n\n\nDummy classifier\n0.541333\n0.00299\n\n\nDecision tree\n0.769167\n0.02632",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#can-we-do-better",
    "href": "materials/slides/ensembles.html#can-we-do-better",
    "title": "Tree-based and ensemble models",
    "section": "Can we do better?",
    "text": "Can we do better?\n\nWe could tune some decision tree parameters (e.g., alpha, maximum tree depth, etc)…\nWe could also try a different tree-based method!\nThe Random Forest Algorithm by Jenny Yeon & Jared Wilber",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#the-random-forest-algorithm",
    "href": "materials/slides/ensembles.html#the-random-forest-algorithm",
    "title": "Tree-based and ensemble models",
    "section": "The Random Forest Algorithm",
    "text": "The Random Forest Algorithm\n\nBuild a number of decision trees on bootstrapped training samples.\nWhen building the trees from the bootstrapped samples, at each stage of splitting, the best splitting is computed using a randomly selected subset of the features.\nTake the majority votes across all the trees for the final prediction.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#random-forest-in-scikit-learn-missing-values",
    "href": "materials/slides/ensembles.html#random-forest-in-scikit-learn-missing-values",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn & missing values",
    "text": "Random forest in scikit-learn & missing values\n\n\n\nDoes not accept missing values, we need to deal with these somehow…\nWe can either drop the observations with missing values, or we can somehow impute them.\nFor the purposes of this demo we will drop them, but if you are interested in imputation, see the imputation tutorial in scikit-learn\n\n\nHow many rows have missing observations:\n\nheart.isna().any(axis=1).sum()\n\nnp.int64(6)\n\n\nDrop rows with missing observations:\n\nheart_train_drop_na = heart_train.dropna()\n\nX_train_drop_na = heart_train_drop_na.drop(\n    columns=['AHD']\n)\ny_train_drop_na = heart_train_drop_na['AHD']",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#random-forest-in-scikit-learn",
    "href": "materials/slides/ensembles.html#random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn",
    "text": "Random forest in scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(random_state=2026)\nrf_pipeline = make_pipeline(heart_preprocessor, random_forest)\ncv_10_rf = pd.DataFrame(\n    cross_validate(\n        estimator=rf_pipeline,\n        cv=10,\n        X=X_train_drop_na,\n        y=y_train_drop_na\n    )\n)\n\ncv_10_rf_metrics = cv_10_rf.agg([\"mean\", \"sem\"])\nresults_rf = pd.DataFrame({'mean' : [cv_10_rf_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_rf_metrics.test_score.iloc[1]]},\n  index = ['Random forest']\n)\nresults = pd.concat([results, results_rf])\nresults\n\n\n\n\n\n\n\n\nmean\nsem\n\n\n\n\nDummy classifier\n0.541333\n0.002990\n\n\nDecision tree\n0.769167\n0.026320\n\n\nRandom forest\n0.818297\n0.017362",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#can-we-do-better-1",
    "href": "materials/slides/ensembles.html#can-we-do-better-1",
    "title": "Tree-based and ensemble models",
    "section": "Can we do better?",
    "text": "Can we do better?\n\nRandom forest can be tuned a several important parameters, including:\n\nn_estimators: number of decision trees (higher = more complexity)\nmax_depth: max depth of each decision tree (higher = more complexity)\nmax_features: the number of features you get to look at each split (higher = more complexity)\n\nWe can use GridSearchCV to search for the optimal parameters for these, as we did for \\(K\\) in \\(K\\)-nearest neighbors.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tuning-random-forest-in-scikit-learn",
    "href": "materials/slides/ensembles.html#tuning-random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning random forest in scikit-learn",
    "text": "Tuning random forest in scikit-learn\n\nfrom sklearn.model_selection import GridSearchCV\n\nrf_param_grid = {'randomforestclassifier__n_estimators': [200],\n              'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],\n              'randomforestclassifier__max_features': [1, 2, 3, 4, 5, 6, 7]}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\nrf_tune_grid.fit(\n    X_train_drop_na,\n    y_train_drop_na\n)\n\ncv_10_rf_tuned_metrics = pd.DataFrame(rf_tune_grid.cv_results_)\nresults_rf_tuned = pd.DataFrame({'mean' : rf_tune_grid.best_score_,\n  'sem' : pd.DataFrame(rf_tune_grid.cv_results_)['std_test_score'][6] / 10**(1/2)},\n  index = ['Random forest tuned']\n)\nresults = pd.concat([results, results_rf_tuned])",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#random-forest-results",
    "href": "materials/slides/ensembles.html#random-forest-results",
    "title": "Tree-based and ensemble models",
    "section": "Random Forest results",
    "text": "Random Forest results\nHow did the Random Forest compare against the other models we tried?\n\nresults\n\n\n\n\n\n\n\n\nmean\nsem\n\n\n\n\nDummy classifier\n0.541333\n0.002990\n\n\nDecision tree\n0.769167\n0.026320\n\n\nRandom forest\n0.818297\n0.017362\n\n\nRandom forest tuned\n0.860688\n0.022223",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#boosting",
    "href": "materials/slides/ensembles.html#boosting",
    "title": "Tree-based and ensemble models",
    "section": "Boosting",
    "text": "Boosting\n\nNo randomization.\nThe key idea is combining many simple models called weak learners, to create a strong learner.\nThey combine multiple shallow (depth 1 to 5) decision trees.\nThey build trees in a serial manner, where each tree tries to correct the mistakes of the previous one.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tuning-gradientboostingclassifier-with-scikit-learn",
    "href": "materials/slides/ensembles.html#tuning-gradientboostingclassifier-with-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning GradientBoostingClassifier with scikit-learn",
    "text": "Tuning GradientBoostingClassifier with scikit-learn\n\nGradientBoostingClassifier can be tuned a several important parameters, including:\n\nn_estimators: number of decision trees (higher = more complexity)\nmax_depth: max depth of each decision tree (higher = more complexity)\nlearning_rate: the shrinkage parameter which controls the rate at which boosting learns. Values between 0.01 or 0.001 are typical.\n\nWe can use GridSearchCV to search for the optimal parameters for these, as we did for the parameters in Random Forest.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tuning-gradientboostingclassifier-with-scikit-learn-1",
    "href": "materials/slides/ensembles.html#tuning-gradientboostingclassifier-with-scikit-learn-1",
    "title": "Tree-based and ensemble models",
    "section": "Tuning GradientBoostingClassifier with scikit-learn",
    "text": "Tuning GradientBoostingClassifier with scikit-learn\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosted_classifier = GradientBoostingClassifier(random_state=2026)\ngb_pipeline = make_pipeline(heart_preprocessor, gradient_boosted_classifier)\ngb_param_grid = {'gradientboostingclassifier__n_estimators': [200],\n              'gradientboostingclassifier__max_depth': [1, 3, 5, 7, 9],\n              'gradientboostingclassifier__learning_rate': [0.001, 0.005, 0.01]}\ngb_tune_grid = GridSearchCV(\n    estimator=gb_pipeline,\n    param_grid=gb_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\ngb_tune_grid.fit(\n    X_train_drop_na,\n    y_train_drop_na\n)\n\ncv_10_gb_tuned_metrics = pd.DataFrame(gb_tune_grid.cv_results_)\nresults_gb_tuned = pd.DataFrame({'mean' : gb_tune_grid.best_score_,\n  'sem' : pd.DataFrame(gb_tune_grid.cv_results_)['std_test_score'][6] / 10**(1/2)},\n  index = ['Gradient boosted classifier tuned']\n)\nresults = pd.concat([results, results_gb_tuned])",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#gradientboostingclassifier-results",
    "href": "materials/slides/ensembles.html#gradientboostingclassifier-results",
    "title": "Tree-based and ensemble models",
    "section": "GradientBoostingClassifier results",
    "text": "GradientBoostingClassifier results\nHow did the GradientBoostingClassifier compare against the other models we tried?\n\nresults\n\n\n\n\n\n\n\n\nmean\nsem\n\n\n\n\nDummy classifier\n0.541333\n0.002990\n\n\nDecision tree\n0.769167\n0.026320\n\n\nRandom forest\n0.818297\n0.017362\n\n\nRandom forest tuned\n0.860688\n0.022223\n\n\nGradient boosted classifier tuned\n0.851993\n0.025671",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#how-do-we-choose-the-final-model",
    "href": "materials/slides/ensembles.html#how-do-we-choose-the-final-model",
    "title": "Tree-based and ensemble models",
    "section": "How do we choose the final model?",
    "text": "How do we choose the final model?\n\nRemember, what is your question or application?\nA good rule when models are not very different (considering SEM), what is the simplest model that does well?\nLook at other metrics that are important to you (not just the metric you used for tuning your model), remember precision & recall, for example.\nRemember - no peaking at the test set until you choose! And then, you should only look at the test set for one model!",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#precision-and-recall-on-the-tuned-random-forest-model",
    "href": "materials/slides/ensembles.html#precision-and-recall-on-the-tuned-random-forest-model",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall on the tuned random forest model",
    "text": "Precision and recall on the tuned random forest model\n\nfrom sklearn.metrics import make_scorer, precision_score, recall_score\n\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score, pos_label='Yes'),\n    'recall': make_scorer(recall_score, pos_label='Yes')\n}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1,\n    scoring=scoring,\n    refit='accuracy'\n)\n\nrf_tune_grid.fit(X_train_drop_na, y_train_drop_na)\n\nGridSearchCV(cv=10,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         ['Age',\n                                                                          'RestBP',\n                                                                          'Chol',\n                                                                          'RestECG',\n                                                                          'MaxHR',\n                                                                          'Oldpeak',\n                                                                          'Slope',\n                                                                          'Ca']),\n                                                                        ('passthrough',\n                                                                         'passthrough',\n                                                                         ['Sex',\n                                                                          'Fbs',\n                                                                          'ExAng']),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['ChestPain',\n                                                                          'Thal'])])),\n                                       ('randomforestclas...\n             param_grid={'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],\n                         'randomforestclassifier__max_features': [1, 2, 3, 4, 5,\n                                                                  6, 7],\n                         'randomforestclassifier__n_estimators': [200]},\n             refit='accuracy',\n             scoring={'accuracy': 'accuracy',\n                      'precision': make_scorer(precision_score, response_method='predict', pos_label=Yes),\n                      'recall': make_scorer(recall_score, response_method='predict', pos_label=Yes)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         ['Age',\n                                                                          'RestBP',\n                                                                          'Chol',\n                                                                          'RestECG',\n                                                                          'MaxHR',\n                                                                          'Oldpeak',\n                                                                          'Slope',\n                                                                          'Ca']),\n                                                                        ('passthrough',\n                                                                         'passthrough',\n                                                                         ['Sex',\n                                                                          'Fbs',\n                                                                          'ExAng']),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['ChestPain',\n                                                                          'Thal'])])),\n                                       ('randomforestclas...\n             param_grid={'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],\n                         'randomforestclassifier__max_features': [1, 2, 3, 4, 5,\n                                                                  6, 7],\n                         'randomforestclassifier__n_estimators': [200]},\n             refit='accuracy',\n             scoring={'accuracy': 'accuracy',\n                      'precision': make_scorer(precision_score, response_method='predict', pos_label=Yes),\n                      'recall': make_scorer(recall_score, response_method='predict', pos_label=Yes)}) best_estimator_: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'RestECG', 'MaxHR',\n                                                   'Oldpeak', 'Slope', 'Ca']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Sex', 'Fbs', 'ExAng']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['ChestPain', 'Thal'])])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(max_depth=1, max_features=2,\n                                        n_estimators=200, random_state=2026))])  columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR',\n                                  'Oldpeak', 'Slope', 'Ca']),\n                                ('passthrough', 'passthrough',\n                                 ['Sex', 'Fbs', 'ExAng']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['ChestPain', 'Thal'])]) standardscaler['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak', 'Slope', 'Ca']  StandardScaler?Documentation for StandardScalerStandardScaler() passthrough['Sex', 'Fbs', 'ExAng'] passthroughpassthrough onehotencoder['ChestPain', 'Thal']  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(max_depth=1, max_features=2, n_estimators=200,\n                       random_state=2026)",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#precision-and-recall-contd",
    "href": "materials/slides/ensembles.html#precision-and-recall-contd",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall cont’d",
    "text": "Precision and recall cont’d\n\nWhat do we think? Is this model ready for production in a diagnostic setting?\nHow could we improve it further?\n\n\ncv_results = pd.DataFrame(rf_tune_grid.cv_results_)\n\nmean_precision = cv_results['mean_test_precision'].iloc[rf_tune_grid.best_index_]\nsem_precision = cv_results['std_test_precision'].iloc[rf_tune_grid.best_index_] / np.sqrt(10)\nmean_recall = cv_results['mean_test_recall'].iloc[rf_tune_grid.best_index_]\nsem_recall = cv_results['std_test_recall'].iloc[rf_tune_grid.best_index_] / np.sqrt(10)\n\nresults_rf_tuned = pd.DataFrame({\n    'mean': [rf_tune_grid.best_score_, mean_precision, mean_recall],\n    'sem': [cv_results['std_test_accuracy'].iloc[rf_tune_grid.best_index_] / np.sqrt(10), sem_precision, sem_recall],\n}, index=['accuracy', 'precision', 'recall'])\n\nresults_rf_tuned\n\n\n\n\n\n\n\n\nmean\nsem\n\n\n\n\naccuracy\n0.860688\n0.018576\n\n\nprecision\n0.920505\n0.022982\n\n\nrecall\n0.770909\n0.038928",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#feature-importances",
    "href": "materials/slides/ensembles.html#feature-importances",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances",
    "text": "Feature importances\n\n\nKey points:\n\nDecision trees are very interpretable (decision rules!), however in ensemble models (e.g., Random Forest and Boosting) there are many trees - individual decision rules are not as meaningful…\nInstead, we can calculate feature importances as the total decrease in impurity for all splits involving that feature, weighted by the number of samples involved in those splits, normalized and averaged over all the trees.\nThese are calculated on the training set, as that is the set the model is trained on.\n\n\nNotes of caution!\n\nFeature importances can be unreliable with both highly cardinal, and multicollinear features.\nUnlike the linear model coefficients, feature importances do not have a sign! They tell us about importance, but not an “up or down”.\nIncreasing a feature may cause the prediction to first go up, and then go down.\nAlternatives to feature importance to understanding models exist (e.g., SHAP (SHapley Additive exPlanations))",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#feature-importances-in-scikit-learn",
    "href": "materials/slides/ensembles.html#feature-importances-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances in scikit-learn",
    "text": "Feature importances in scikit-learn\n\n# Access the best pipeline\nbest_pipeline = rf_tune_grid.best_estimator_\n\n# Extract the trained RandomForestClassifier from the pipeline\nbest_rf = best_pipeline.named_steps['randomforestclassifier']\n\n# Extract feature names after preprocessing\n# Get the names of features from each transformer in the pipeline\nnumeric_features = numeric_feats\ncategorical_feature_names = best_pipeline.named_steps['columntransformer'].transformers_[2][1].get_feature_names_out(categorical_feats)\npassthrough_features = passthrough_feats\n\n# Combine all feature names into a single list\nfeature_names = np.concatenate([numeric_features, passthrough_features, categorical_feature_names])\n\n# Calculate feature importances\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame to display feature importances\nimportances_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# Sort by importance (descending order)\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#visualizing-the-results",
    "href": "materials/slides/ensembles.html#visualizing-the-results",
    "title": "Tree-based and ensemble models",
    "section": "Visualizing the results",
    "text": "Visualizing the results",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#evaluating-on-the-test-set",
    "href": "materials/slides/ensembles.html#evaluating-on-the-test-set",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\nPredict on the test set:\n\nheart_test_drop_na = heart_test.dropna()\nX_test_drop_na = heart_test_drop_na.drop(columns=['AHD'])\ny_test_drop_na = heart_test_drop_na['AHD']\n\nheart_test_drop_na[\"predicted\"] = rf_tune_grid.predict(\n    X_test_drop_na\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#evaluating-on-the-test-set-1",
    "href": "materials/slides/ensembles.html#evaluating-on-the-test-set-1",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\nExamine accuracy, precision and recall:\n\n\n\nrf_tune_grid.score(\n    X_test_drop_na,\n    y_test_drop_na\n)\n\n0.7868852459016393\n\n\n\nprecision_score(\n    y_true=heart_test_drop_na[\"AHD\"],\n    y_pred=heart_test_drop_na[\"predicted\"],\n    pos_label='Yes'\n)\n\nnp.float64(0.8)\n\n\n\nrecall_score(\n    y_true=heart_test_drop_na[\"AHD\"],\n    y_pred=heart_test_drop_na[\"predicted\"],\n    pos_label='Yes'\n)\n\nnp.float64(0.7142857142857143)\n\n\n\n\nconf_matrix = pd.crosstab(\n    heart_test_drop_na[\"AHD\"],\n    heart_test_drop_na[\"predicted\"]\n)\nprint(conf_matrix)\n\npredicted  No  Yes\nAHD               \nNo         28    5\nYes         8   20",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#other-boosting-models",
    "href": "materials/slides/ensembles.html#other-boosting-models",
    "title": "Tree-based and ensemble models",
    "section": "Other boosting models:",
    "text": "Other boosting models:\n\n\nXGBoost\n\nNot part of sklearn but has similar interface.\nSupports missing values\nGPU training, networked parallel training\nSupports sparse data\nTypically better scores than random forests\n\n\nLightGBM\n\nNot part of sklearn but has similar interface.\nSmall model size\nFaster\nTypically better scores than random forests\n\nCatBoost\n\nNot part of sklearn but has similar interface.\nUsually better scores but slower compared to XGBoost and LightGBM",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#keep-learning",
    "href": "materials/slides/ensembles.html#keep-learning",
    "title": "Tree-based and ensemble models",
    "section": "Keep learning!",
    "text": "Keep learning!\n\n\n https://python.datasciencebook.ca/\n\n https://www.statlearning.com/",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#local-installation",
    "href": "materials/slides/ensembles.html#local-installation",
    "title": "Tree-based and ensemble models",
    "section": "Local installation",
    "text": "Local installation\n\nUsing Docker: Data Science: A First Introduction (Python Edition) Installation Instructions\nUsing conda: UBC MDS Installation Instructions",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#additional-resources",
    "href": "materials/slides/ensembles.html#additional-resources",
    "title": "Tree-based and ensemble models",
    "section": "Additional resources",
    "text": "Additional resources\n\nThe UBC DSCI 573 (Feature and Model Selection notes) chapter of Data Science: A First Introduction (Python Edition) by Varada Kolhatkar and Joel Ostblom. These notes cover classification and regression metrics, advanced variable selection and more on ensembles.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in the past two chapters. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning {cite:p}james2013introduction provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#references",
    "href": "materials/slides/ensembles.html#references",
    "title": "Tree-based and ensemble models",
    "section": "References",
    "text": "References\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.\nKolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-select\nPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#session-learning-objectives",
    "href": "materials/slides/classification1.html#session-learning-objectives",
    "title": "Classification I: training & predicting",
    "section": "Session learning objectives",
    "text": "Session learning objectives\nBy the end of the session, learners will be able to do the following:\n\nRecognize situations where a simple classifier would be appropriate for making predictions.\nExplain the \\(K\\)-nearest neighbor classification algorithm.\nInterpret the output of a classifier.\nDescribe what a training data set is and how it is used in classification.\nGiven a dataset with two explanatory variables/predictors, use \\(K\\)-nearest neighbor classification in Python using the scikit-learn framework to predict the class of a single new observation.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#the-classification-problem",
    "href": "materials/slides/classification1.html#the-classification-problem",
    "title": "Classification I: training & predicting",
    "section": "The classification problem",
    "text": "The classification problem\n\npredicting a categorical class (sometimes called a label) for an observation given its other variables (sometimes called features)\n\n\nDiagnose a patient as healthy or sick\nTag an email as “spam” or “not spam”\nPredict whether a purchase is fraudulent",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#exploring-a-data-set",
    "href": "materials/slides/classification1.html#exploring-a-data-set",
    "title": "Classification I: training & predicting",
    "section": "Exploring a data set",
    "text": "Exploring a data set\nData:\n\ndigitized breast cancer image features, created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian\nEach row:\n\ndiagnosis (benign or malignant)\nseveral other measurements (nucleus texture, perimeter, area, and more)\n\nDiagnosis for each image was conducted by physicians.\n\nFormulate a predictive question:\n\nCan we use the tumor image measurements available to us to predict whether a future tumor image (with unknown diagnosis) shows a benign or malignant tumor?",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#classification-with-k-nearest-neighbors",
    "href": "materials/slides/classification1.html#classification-with-k-nearest-neighbors",
    "title": "Classification I: training & predicting",
    "section": "Classification with K-nearest neighbors",
    "text": "Classification with K-nearest neighbors\n\nnew_point = [2, 4]\nattrs = [\"Perimeter\", \"Concavity\"]\n\npoints_df = pd.DataFrame(\n    {\"Perimeter\": new_point[0], \"Concavity\": new_point[1], \"Class\": [\"Unknown\"]}\n)\n\nperim_concav_with_new_point_df = pd.concat((cancer, points_df), ignore_index=True)\nprint(perim_concav_with_new_point_df.iloc[[-1]])\n\n     ID    Class  Radius  Texture  Perimeter  Area  Smoothness  Compactness  \\\n569 NaN  Unknown     NaN      NaN        2.0   NaN         NaN          NaN   \n\n     Concavity  Concave_Points  Symmetry  Fractal_Dimension  \n569        4.0             NaN       NaN                NaN  \n\n\n\nCompute the distance matrix between each pair from a vector array X and Y\n\n\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# distance of new point to all other points\nmy_distances = euclidean_distances(perim_concav_with_new_point_df[attrs])[len(cancer)][:-1]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#k-nearest-neighbors-with-scikit-learn",
    "href": "materials/slides/classification1.html#k-nearest-neighbors-with-scikit-learn",
    "title": "Classification I: training & predicting",
    "section": "K-nearest neighbors with scikit-learn",
    "text": "K-nearest neighbors with scikit-learn\n\nK-nearest neighbors algorithm is implemented in scikit-learn\n\n\nfrom sklearn import set_config\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")\n\nNow we can get started with sklearn and KNeighborsClassifier()\n\nfrom sklearn.neighbors import KNeighborsClassifier",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#data-preprocessing-scaling",
    "href": "materials/slides/classification1.html#data-preprocessing-scaling",
    "title": "Classification I: training & predicting",
    "section": "Data preprocessing: Scaling",
    "text": "Data preprocessing: Scaling\nFor KNN:\n\nthe scale of each variable (i.e., its size and range of values) matters\ndistance based algorithm\n\nCompare these 2 scenarios:\n\nPerson A (200 lbs, 6ft tall) vs Person B (202 lbs, 6ft tall)\nPerson A (200 lbs, 6ft tall) vs Person B (200 lbs, 8ft tall)\n\nAll have a distance of 2",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#missing-data",
    "href": "materials/slides/classification1.html#missing-data",
    "title": "Classification I: training & predicting",
    "section": "Missing data",
    "text": "Missing data\nAssume we are only looking at “randomly missing” data\n\nmissing_cancer = pd.read_csv(\"data/wdbc_missing.csv\")[\n    [\"Class\", \"Radius\", \"Texture\", \"Perimeter\"]\n]\nmissing_cancer[\"Class\"] = missing_cancer[\"Class\"].replace(\n    {\"M\": \"Malignant\", \"B\": \"Benign\"}\n)\nprint(missing_cancer)\n\n       Class    Radius   Texture  Perimeter\n0  Malignant       NaN       NaN   1.268817\n1  Malignant  1.828212 -0.353322   1.684473\n2  Malignant  1.578499       NaN   1.565126\n3  Malignant -0.768233  0.253509  -0.592166\n4  Malignant  1.748758 -1.150804   1.775011\n5  Malignant -0.475956 -0.834601  -0.386808\n6  Malignant  1.169878  0.160508   1.137124",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#put-it-all-together-preprocessor",
    "href": "materials/slides/classification1.html#put-it-all-together-preprocessor",
    "title": "Classification I: training & predicting",
    "section": "Put it all together: Preprocessor",
    "text": "Put it all together: Preprocessor\n\n# load the unscaled cancer data, make Class readable\nunscaled_cancer = pd.read_csv(\"data/wdbc_unscaled.csv\")\nunscaled_cancer[\"Class\"] = unscaled_cancer[\"Class\"].replace(\n    {\"M\": \"Malignant\", \"B\": \"Benign\"}\n)\n\n# create the K-NN model\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# create the centering / scaling preprocessor\npreprocessor = make_column_transformer(\n    (StandardScaler(), [\"Area\", \"Smoothness\"]),\n    # more column transformers here\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#reference-code",
    "href": "materials/slides/classification1.html#reference-code",
    "title": "Classification I: training & predicting",
    "section": "Reference Code",
    "text": "Reference Code\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import (\n    make_column_transformer,\n)\n\n\n# load the unscaled cancer data\nunscaled_cancer = pd.read_csv(\n    \"data/wdbc_unscaled.csv\"\n)\n\n# make Class readable\nunscaled_cancer[\"Class\"] = unscaled_cancer[\n    \"Class\"\n].replace({\"M\": \"Malignant\", \"B\": \"Benign\"})\n\n\n\n# create the K-NN model\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# create the centering / scaling preprocessor\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['Area', 'Smoothness']),\n    # more column transformers here\n)\n\nknn_pipeline = make_pipeline(preprocessor, knn)\nknn_pipeline.fit(X=unscaled_cancer, y=unscaled_cancer['Class'])\nknn_pipeline\n\nnew_observation = pd.DataFrame(\n    {\n        'Area': [500, 1500],\n        'Smoothness': [0.075, 0.1],\n    }\n)\nprediction = knn_pipeline.predict(new_observation)\nprediction\n\narray(['Benign', 'Malignant'], dtype=object)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#additional-resources",
    "href": "materials/slides/classification1.html#additional-resources",
    "title": "Classification I: training & predicting",
    "section": "Additional resources",
    "text": "Additional resources\n\nThe Classification I: training & predicting chapter of Data Science: A First Introduction (Python Edition) by Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy contains all the content presented here with a detailed narrative.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in this lesson. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning by Gareth James Daniela Witten Trevor Hastie, and Robert Tibshirani provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#references",
    "href": "materials/slides/classification1.html#references",
    "title": "Classification I: training & predicting",
    "section": "References",
    "text": "References\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 108–122. 2013.\nThomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27, 1967.\nEvelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.\nWilliam Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In International Symposium on Electronic Imaging: Science and Technology. 1993.\nStanford Health Care. What is cancer? 2021. URL: https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#session-learning-objectives-knn",
    "href": "materials/slides/regression.html#session-learning-objectives-knn",
    "title": "Regression",
    "section": "Session learning objectives: KNN",
    "text": "Session learning objectives: KNN\n\nRecognize situations where a regression analysis would be appropriate for making predictions.\nExplain the K-nearest neighbors (K-NN) regression algorithm and describe how it differs from K-NN classification.\nInterpret the output of a K-NN regression.\nIn a data set with two or more variables, perform K-nearest neighbors regression in Python.\nEvaluate K-NN regression prediction quality in Python using the root mean squared prediction error (RMSPE).\nEstimate the RMSPE in Python using cross-validation or a test set.\nChoose the number of neighbors in K-nearest neighbors regression by minimizing estimated cross-validation RMSPE.\nDescribe underfitting and overfitting, and relate it to the number of neighbors in K-nearest neighbors regression.\nDescribe the advantages and disadvantages of K-nearest neighbors regression.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#session-learning-objective-linear-regression",
    "href": "materials/slides/regression.html#session-learning-objective-linear-regression",
    "title": "Regression",
    "section": "Session learning objective: Linear Regression",
    "text": "Session learning objective: Linear Regression\n\nUse Python to fit simple and multivariable linear regression models on training data.\nEvaluate the linear regression model on test data.\nCompare and contrast predictions obtained from K-nearest neighbors regression to those obtained using linear regression from the same data set.\nDescribe how linear regression is affected by outliers and multicollinearity.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#the-regression-problem",
    "href": "materials/slides/regression.html#the-regression-problem",
    "title": "Regression",
    "section": "The regression problem",
    "text": "The regression problem\n\nPredictive problem\nUse past information to predict future observations\nPredict numerical values instead of categorical values\n\nExamples:\n\nRace time in the Boston marathon\nsize of a house to predict its sale price",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#explore-a-data-set",
    "href": "materials/slides/regression.html#explore-a-data-set",
    "title": "Regression",
    "section": "Explore a data set",
    "text": "Explore a data set\n932 real estate transactions in Sacramento, California\n\nCan we use the size of a house in the Sacramento, CA area to predict its sale price?",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#k-nearest-neighbors-regression",
    "href": "materials/slides/regression.html#k-nearest-neighbors-regression",
    "title": "Regression",
    "section": "K-nearest neighbors regression",
    "text": "K-nearest neighbors regression\n\n# look at a small sample of data\nnp.random.seed(10)\n\nsmall_sacramento = sacramento.sample(n=30)\nprint(small_sacramento)\n\n           city     zip  beds  baths  sqft         type   price   latitude  \\\n538   ELK_GROVE  z95758     3    3.0  2503  Residential  484500  38.409689   \n304     ROCKLIN  z95765     4    2.0  2607  Residential  402000  38.805749   \n..          ...     ...   ...    ...   ...          ...     ...        ...   \n559  SACRAMENTO  z95817     2    1.0  1080  Residential   65000  38.544162   \n917  SACRAMENTO  z95834     3    2.0  1665  Residential  224000  38.631026   \n\n      longitude  \n538 -121.446059  \n304 -121.280931  \n..          ...  \n559 -121.460652  \n917 -121.501879  \n\n[30 rows x 9 columns]",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#training-evaluating-and-tuning-the-model",
    "href": "materials/slides/regression.html#training-evaluating-and-tuning-the-model",
    "title": "Regression",
    "section": "Training, evaluating, and tuning the model",
    "text": "Training, evaluating, and tuning the model\n\nnp.random.seed(1)\n\nsacramento_train, sacramento_test = train_test_split(\n    sacramento, train_size=0.75\n)\n\n\n\n\n\n\n\nNote\n\n\nWe are not specifying the stratify argument. The train_test_split() function cannot stratify on a quantitative variable",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#underfitting-and-overfitting",
    "href": "materials/slides/regression.html#underfitting-and-overfitting",
    "title": "Regression",
    "section": "Underfitting and overfitting",
    "text": "Underfitting and overfitting\n\n\n\nsacr_tunek_plot\n\n\n\n\n\n\n\n\nThe RMSPE values start to get higher after a certain k value",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#evaluating-on-the-test-set",
    "href": "materials/slides/regression.html#evaluating-on-the-test-set",
    "title": "Regression",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\nRMSPE on the test data\n\nRetrain the K-NN regression model on the entire training data set using best k\n\n\nfrom sklearn.metrics import mean_squared_error\n\nsacramento_test['predicted'] = sacr_gridsearch.predict(sacramento_test)\nRMSPE = mean_squared_error(\n    y_true=sacramento_test['price'], y_pred=sacramento_test['predicted']\n) ** (1 / 2)\n\nRMSPE\n\nnp.float64(87498.86808211416)",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression",
    "title": "Regression",
    "section": "Multivariable K-NN regression",
    "text": "Multivariable K-NN regression\nWe can use multiple predictors in K-NN regression",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#strengths-and-limitations-of-k-nn-regression",
    "href": "materials/slides/regression.html#strengths-and-limitations-of-k-nn-regression",
    "title": "Regression",
    "section": "Strengths and limitations of K-NN regression",
    "text": "Strengths and limitations of K-NN regression\nStrengths:\n\nsimple, intuitive algorithm\nrequires few assumptions about what the data must look like\nworks well with non-linear relationships (i.e., if the relationship is not a straight line)\n\nWeaknesses:\n\nvery slow as the training data gets larger\nmay not perform well with a large number of predictors\nmay not predict well beyond the range of values input in your training data",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression",
    "href": "materials/slides/regression.html#linear-regression",
    "title": "Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nAddresses the limitations from KNN regression\nprovides an interpretable mathematical equation that describes the relationship between the predictor and response variables\nCreate a straight line of best fit through the training data\n\n\n\n\n\n\n\nNote\n\n\nLogistic regression is the linear model we can use for binary classification",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression-in-python",
    "href": "materials/slides/regression.html#linear-regression-in-python",
    "title": "Regression",
    "section": "Linear regression in Python",
    "text": "Linear regression in Python\nThe scikit-learn pattern still applies:\n\nCreate a training and test set\nInstantiate a model\nFit the model on training\nUse model on testing set",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#comparing-simple-linear-and-k-nn-regression",
    "href": "materials/slides/regression.html#comparing-simple-linear-and-k-nn-regression",
    "title": "Regression",
    "section": "Comparing simple linear and K-NN regression",
    "text": "Comparing simple linear and K-NN regression",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-linear-regression",
    "href": "materials/slides/regression.html#multivariable-linear-regression",
    "title": "Regression",
    "section": "Multivariable linear regression",
    "text": "Multivariable linear regression\nMore predictor variables!\n\nMore does not always mean better\nWe will not cover variable selection in this workshop\nWill talk about categorical predictors later in the workshop",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#outliers-and-multicollinearity",
    "href": "materials/slides/regression.html#outliers-and-multicollinearity",
    "title": "Regression",
    "section": "Outliers and Multicollinearity",
    "text": "Outliers and Multicollinearity\n\nOutliers: extreme values that can move the best fit line\nMulticollinearity: variables that are highly correlated to one another",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#references",
    "href": "materials/slides/regression.html#references",
    "title": "Regression",
    "section": "References",
    "text": "References\nThomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27, 1967.\nEvelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html",
    "title": "Tree-based and ensemble models",
    "section": "",
    "text": "Code from the slides in an executable notebook."
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#example-the-heart-data-set",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#example-the-heart-data-set",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set",
    "text": "Example: the heart data set\n\nimport pandas as pd\nheart = pd.read_csv(\"data/Heart.csv\", index_col=0)\nheart.info()\n\n\nheart.head()"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#do-we-have-a-class-imbalance",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#do-we-have-a-class-imbalance",
    "title": "Tree-based and ensemble models",
    "section": "Do we have a class imbalance?",
    "text": "Do we have a class imbalance?\n\nheart['AHD'].value_counts(normalize=True)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#data-splitting",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#data-splitting",
    "title": "Tree-based and ensemble models",
    "section": "Data splitting",
    "text": "Data splitting\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(2024)\n\nheart_train, heart_test = train_test_split(\n    heart, train_size=0.8, stratify=heart[\"AHD\"]\n)\n\nX_train = heart_train.drop(columns=['AHD'])\ny_train = heart_train['AHD']\nX_test = heart_test.drop(columns=['AHD'])\ny_test = heart_test['AHD']"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#one-hot-encoding-pre-processing",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#one-hot-encoding-pre-processing",
    "title": "Tree-based and ensemble models",
    "section": "One hot encoding & pre-processing",
    "text": "One hot encoding & pre-processing\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\nnumeric_feats = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak','Slope', 'Ca']\npassthrough_feats = ['Sex', 'Fbs', 'ExAng']\ncategorical_feats = ['ChestPain', 'Thal']\n\nheart_preprocessor = make_column_transformer(\n    (StandardScaler(), numeric_feats), \n    (\"passthrough\", passthrough_feats),     \n    (OneHotEncoder(handle_unknown = \"ignore\"), categorical_feats),     \n)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-dummy-classifier",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-dummy-classifier",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a dummy classifier",
    "text": "Fitting a dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\n\ndummy = DummyClassifier()\ndummy_pipeline = make_pipeline(heart_preprocessor, dummy)\ncv_10_dummy = pd.DataFrame(\n    cross_validate(\n        estimator=dummy_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dummy_metrics = cv_10_dummy.agg([\"mean\", \"sem\"])\nresults = pd.DataFrame({'mean' : [cv_10_dummy_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_dummy_metrics.test_score.iloc[1]]},\n  index = ['Dummy classifier']\n)\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-decision-tree",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-decision-tree",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a decision tree",
    "text": "Fitting a decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\n\ndt_pipeline = make_pipeline(heart_preprocessor, decision_tree)\ncv_10_dt = pd.DataFrame(\n    cross_validate(\n        estimator=dt_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dt_metrics = cv_10_dt.agg([\"mean\", \"sem\"])\nresults_dt = pd.DataFrame({'mean' : [cv_10_dt_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_dt_metrics.test_score.iloc[1]]},\n  index = ['Decision tree']\n)\nresults = pd.concat([results, results_dt])\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#random-forest-in-scikit-learn-missing-values",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#random-forest-in-scikit-learn-missing-values",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn & missing values",
    "text": "Random forest in scikit-learn & missing values\nHow many rows have missing observations:\n\nheart.isna().any(axis=1).sum()\n\nDrop rows with missing observations:\n\nheart_train_drop_na = heart_train.dropna()\n\nX_train_drop_na = heart_train_drop_na.drop(\n    columns=['AHD']\n)\ny_train_drop_na = heart_train_drop_na['AHD']"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#random-forest-in-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn",
    "text": "Random forest in scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\nrf_pipeline = make_pipeline(heart_preprocessor, random_forest)\ncv_10_rf = pd.DataFrame(\n    cross_validate(\n        estimator=rf_pipeline,\n        cv=10,\n        X=X_train_drop_na,\n        y=y_train_drop_na\n    )\n)\n\ncv_10_rf_metrics = cv_10_rf.agg([\"mean\", \"sem\"])\nresults_rf = pd.DataFrame({'mean' : [cv_10_rf_metrics.test_score.iloc[0]],\n  'sem' : [cv_10_rf_metrics.test_score.iloc[1]]},\n  index = ['Random forest']\n)\nresults = pd.concat([results, results_rf])\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-random-forest-in-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning random forest in scikit-learn",
    "text": "Tuning random forest in scikit-learn\n\nfrom sklearn.model_selection import GridSearchCV\n\nrf_param_grid = {'randomforestclassifier__n_estimators': [200],\n              'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],\n              'randomforestclassifier__max_features': [1, 2, 3, 4, 5, 6, 7]}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\nrf_tune_grid.fit(\n    X_train_drop_na,\n    y_train_drop_na\n)\n\ncv_10_rf_tuned_metrics = pd.DataFrame(rf_tune_grid.cv_results_)\nresults_rf_tuned = pd.DataFrame({'mean' : rf_tune_grid.best_score_,\n  'sem' : pd.DataFrame(rf_tune_grid.cv_results_)['std_test_score'][6] / 10**(1/2)},\n  index = ['Random forest tuned']\n)\nresults = pd.concat([results, results_rf_tuned])\n\n\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-gradientboostingclassifier-with-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-gradientboostingclassifier-with-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning GradientBoostingClassifier with scikit-learn",
    "text": "Tuning GradientBoostingClassifier with scikit-learn\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosted_classifier = GradientBoostingClassifier()\ngb_pipeline = make_pipeline(heart_preprocessor, gradient_boosted_classifier)\ngb_param_grid = {'gradientboostingclassifier__n_estimators': [200],\n              'gradientboostingclassifier__max_depth': [1, 3, 5, 7, 9],\n              'gradientboostingclassifier__learning_rate': [0.001, 0.005, 0.01]}\ngb_tune_grid = GridSearchCV(\n    estimator=gb_pipeline,\n    param_grid=gb_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\ngb_tune_grid.fit(\n    X_train_drop_na,\n    y_train_drop_na\n)\n\ncv_10_gb_tuned_metrics = pd.DataFrame(gb_tune_grid.cv_results_)\nresults_gb_tuned = pd.DataFrame({'mean' : gb_tune_grid.best_score_,\n  'sem' : pd.DataFrame(gb_tune_grid.cv_results_)['std_test_score'][6] / 10**(1/2)},\n  index = ['Gradient boosted classifier tuned']\n)\nresults = pd.concat([results, results_gb_tuned])\n\n\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#precision-and-recall-on-the-tuned-random-forest-model",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#precision-and-recall-on-the-tuned-random-forest-model",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall on the tuned random forest model",
    "text": "Precision and recall on the tuned random forest model\n\nfrom sklearn.metrics import make_scorer, precision_score, recall_score\n\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score, pos_label='Yes'),\n    'recall': make_scorer(recall_score, pos_label='Yes')\n}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1,\n    scoring=scoring,\n    refit='accuracy'\n)\n\nrf_tune_grid.fit(X_train_drop_na, y_train_drop_na)\n\n\ncv_results = pd.DataFrame(rf_tune_grid.cv_results_)\n\nmean_precision = cv_results['mean_test_precision'].iloc[rf_tune_grid.best_index_]\nsem_precision = cv_results['std_test_precision'].iloc[rf_tune_grid.best_index_] / np.sqrt(10)\nmean_recall = cv_results['mean_test_recall'].iloc[rf_tune_grid.best_index_]\nsem_recall = cv_results['std_test_recall'].iloc[rf_tune_grid.best_index_] / np.sqrt(10)\n\nresults_rf_tuned = pd.DataFrame({\n    'mean': [rf_tune_grid.best_score_, mean_precision, mean_recall],\n    'sem': [cv_results['std_test_accuracy'].iloc[rf_tune_grid.best_index_] / np.sqrt(10), sem_precision, sem_recall],\n}, index=['accuracy', 'precision', 'recall'])\n\nresults_rf_tuned"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#feature-importances-in-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#feature-importances-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances in scikit-learn",
    "text": "Feature importances in scikit-learn\n\n# Access the best pipeline\nbest_pipeline = rf_tune_grid.best_estimator_\n\n# Extract the trained RandomForestClassifier from the pipeline\nbest_rf = best_pipeline.named_steps['randomforestclassifier']\n\n# Extract feature names after preprocessing\n# Get the names of features from each transformer in the pipeline\nnumeric_features = numeric_feats\ncategorical_feature_names = best_pipeline.named_steps['columntransformer'].transformers_[2][1].get_feature_names_out(categorical_feats)\npassthrough_features = passthrough_feats\n\n# Combine all feature names into a single list\nfeature_names = np.concatenate([numeric_features, passthrough_features, categorical_feature_names])\n\n# Calculate feature importances\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame to display feature importances\nimportances_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# Sort by importance (descending order)\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#visualizing-the-results",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#visualizing-the-results",
    "title": "Tree-based and ensemble models",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\nimport altair as alt\n\nbar_chart = alt.Chart(importances_df).mark_bar().encode(\n    x=alt.X('Importance:Q', title='Feature Importance'),\n    y=alt.Y('Feature:N', sort='-x', title='Feature'),\n    tooltip=['Feature', 'Importance']\n).properties(\n    title='Feature Importances from Random Forest Model',\n    width=600,\n    height=400\n)\nbar_chart"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#evaluating-on-the-test-set",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#evaluating-on-the-test-set",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\nheart_test_drop_na = heart_test.dropna()\nX_test_drop_na = heart_test_drop_na.drop(columns=['AHD'])\ny_test_drop_na = heart_test_drop_na['AHD']\n\nheart_test_drop_na[\"predicted\"] = rf_tune_grid.predict(\n    X_test_drop_na\n)\n\nAccuracy\n\nrf_tune_grid.score(\n    X_test_drop_na,\n    y_test_drop_na\n)\n\nPrecision\n\nprecision_score(\n    y_true=heart_test_drop_na[\"AHD\"],\n    y_pred=heart_test_drop_na[\"predicted\"],\n    pos_label='Yes'\n)\n\nRecall\n\nrecall_score(\n    y_true=heart_test_drop_na[\"AHD\"],\n    y_pred=heart_test_drop_na[\"predicted\"],\n    pos_label='Yes'\n)\n\nConfusion matrix\n\nconf_matrix = pd.crosstab(\n    heart_test_drop_na[\"AHD\"],\n    heart_test_drop_na[\"predicted\"]\n)\nprint(conf_matrix)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#referencesgareth-james-daniela-witten-trevor-hastie-robert-tibshirani-and-jonathan-taylor.-an-introduction-to-statistical-learning-with-applications-in-python.-springer-1st-edition-2023.-url-httpswww.statlearning.com.kolhatkar-v.-and-ostblom-j.-2024.-ubc-dsci-573-feature-and-model-selection-course-notes.-url-httpsubc-mds.github.iodsci_573_feat-model-selectpedregosa-f.-et-al.-2011.-scikit-learn-machine-learning-in-python.-journal-of-machine-learning-research-12oct-pp.28252830.",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#referencesgareth-james-daniela-witten-trevor-hastie-robert-tibshirani-and-jonathan-taylor.-an-introduction-to-statistical-learning-with-applications-in-python.-springer-1st-edition-2023.-url-httpswww.statlearning.com.kolhatkar-v.-and-ostblom-j.-2024.-ubc-dsci-573-feature-and-model-selection-course-notes.-url-httpsubc-mds.github.iodsci_573_feat-model-selectpedregosa-f.-et-al.-2011.-scikit-learn-machine-learning-in-python.-journal-of-machine-learning-research-12oct-pp.28252830.",
    "title": "Tree-based and ensemble models",
    "section": "ReferencesGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-selectPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830.",
    "text": "ReferencesGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-selectPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830."
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html",
    "title": "Worksheet Classification (Part I)",
    "section": "",
    "text": "After completing workshop session, you will be able to:\n\nRecognize situations where a simple classifier would be appropriate for making predictions.\nExplain the \\(K\\)-nearest neighbour classification algorithm.\nInterpret the output of a classifier.\nCompute, by hand, the distance between points when there are two explanatory variables/predictors.\nDescribe what a training data set is and how it is used in classification.\nGiven a dataset with two explanatory variables/predictors, use \\(K\\)-nearest neighbour classification in Python using the scikit-learn framework to predict the class of a single new observation.\n\nThis worksheet covers parts of Chapter 5 of the online textbook. You should read this chapter before attempting this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing\nimport random\n\nimport altair as alt\nimport pandas as pd\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import set_config\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")\n\nQuestion 0.1 Multiple Choice:  {points: 1}\nWhich of the following statements is NOT true of a training data set (in the context of classification)?\nA. A training data set is a collection of observations for which we know the true classes.\nB. We can use a training set to explore and build our classifier.\nC. The training data set is the underlying collection of observations for which we don’t know the true classes.\nAssign your answer to an object called answer0_1. Make sure the correct answer is an uppercase letter. Remember to surround your answer with quotation marks (e.g. “D”).\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_1)).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"f70a6af77babf11afa3a50f7a15e532aa7baf3f3\", \"type of answer0_1 is not str. answer0_1 should be an str\"\nassert sha1(str(len(answer0_1)).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"3b3b8f9d183db109adfa13225d741063b08dff85\", \"length of answer0_1 is not correct\"\nassert sha1(str(answer0_1.lower()).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"6edfdac10655fefa98b28f34c9d1f860dcce8ff7\", \"value of answer0_1 is not correct\"\nassert sha1(str(answer0_1).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"9dd39830e122b8184b1799ac7d2e687f95a317b7\", \"correct string value of answer0_1 but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 0.2 Multiple Choice  {points: 1}\n(Adapted from James et al, “An introduction to statistical learning” (page 53))\nConsider the scenario below:\nWe collect data on 20 similar products. For each product we have recorded whether it was a success or failure (labelled as such by the Sales team), price charged for the product, marketing budget, competition price, customer data, and ten other variables.\nWhich of the following is a classification problem?\nA. We are interested in comparing the profit margins for products that are a success and products that are a failure.\nB. We are considering launching a new product and wish to know whether it will be a success or a failure.\nC. We wish to group customers based on their preferences and use that knowledge to develop targeted marketing programs.\nAssign your answer to an object called answer0_2. Make sure the correct answer is an uppercase letter. Remember to surround your answer with quotation marks (e.g. “F”).\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_2)).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"351b6021c8f87571484eee4e4c7bf0a0b7ed21fc\", \"type of answer0_2 is not str. answer0_2 should be an str\"\nassert sha1(str(len(answer0_2)).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"439859842b2ff5343f142aebc166bbd2f5c1ba63\", \"length of answer0_2 is not correct\"\nassert sha1(str(answer0_2.lower()).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"eac7d86ae102d0cf10bcd9c8da29efa7bd262db9\", \"value of answer0_2 is not correct\"\nassert sha1(str(answer0_2).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"219cf8d4b385560346b8cfeb877dfdfc50ca9e69\", \"correct string value of answer0_2 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#learning-goals",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#learning-goals",
    "title": "Worksheet Classification (Part I)",
    "section": "",
    "text": "After completing workshop session, you will be able to:\n\nRecognize situations where a simple classifier would be appropriate for making predictions.\nExplain the \\(K\\)-nearest neighbour classification algorithm.\nInterpret the output of a classifier.\nCompute, by hand, the distance between points when there are two explanatory variables/predictors.\nDescribe what a training data set is and how it is used in classification.\nGiven a dataset with two explanatory variables/predictors, use \\(K\\)-nearest neighbour classification in Python using the scikit-learn framework to predict the class of a single new observation.\n\nThis worksheet covers parts of Chapter 5 of the online textbook. You should read this chapter before attempting this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing\nimport random\n\nimport altair as alt\nimport pandas as pd\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import set_config\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")\n\nQuestion 0.1 Multiple Choice:  {points: 1}\nWhich of the following statements is NOT true of a training data set (in the context of classification)?\nA. A training data set is a collection of observations for which we know the true classes.\nB. We can use a training set to explore and build our classifier.\nC. The training data set is the underlying collection of observations for which we don’t know the true classes.\nAssign your answer to an object called answer0_1. Make sure the correct answer is an uppercase letter. Remember to surround your answer with quotation marks (e.g. “D”).\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_1)).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"f70a6af77babf11afa3a50f7a15e532aa7baf3f3\", \"type of answer0_1 is not str. answer0_1 should be an str\"\nassert sha1(str(len(answer0_1)).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"3b3b8f9d183db109adfa13225d741063b08dff85\", \"length of answer0_1 is not correct\"\nassert sha1(str(answer0_1.lower()).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"6edfdac10655fefa98b28f34c9d1f860dcce8ff7\", \"value of answer0_1 is not correct\"\nassert sha1(str(answer0_1).encode(\"utf-8\")+b\"59e8\").hexdigest() == \"9dd39830e122b8184b1799ac7d2e687f95a317b7\", \"correct string value of answer0_1 but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 0.2 Multiple Choice  {points: 1}\n(Adapted from James et al, “An introduction to statistical learning” (page 53))\nConsider the scenario below:\nWe collect data on 20 similar products. For each product we have recorded whether it was a success or failure (labelled as such by the Sales team), price charged for the product, marketing budget, competition price, customer data, and ten other variables.\nWhich of the following is a classification problem?\nA. We are interested in comparing the profit margins for products that are a success and products that are a failure.\nB. We are considering launching a new product and wish to know whether it will be a success or a failure.\nC. We wish to group customers based on their preferences and use that knowledge to develop targeted marketing programs.\nAssign your answer to an object called answer0_2. Make sure the correct answer is an uppercase letter. Remember to surround your answer with quotation marks (e.g. “F”).\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer0_2)).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"351b6021c8f87571484eee4e4c7bf0a0b7ed21fc\", \"type of answer0_2 is not str. answer0_2 should be an str\"\nassert sha1(str(len(answer0_2)).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"439859842b2ff5343f142aebc166bbd2f5c1ba63\", \"length of answer0_2 is not correct\"\nassert sha1(str(answer0_2.lower()).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"eac7d86ae102d0cf10bcd9c8da29efa7bd262db9\", \"value of answer0_2 is not correct\"\nassert sha1(str(answer0_2).encode(\"utf-8\")+b\"b9cdb\").hexdigest() == \"219cf8d4b385560346b8cfeb877dfdfc50ca9e69\", \"correct string value of answer0_2 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#breast-cancer-data-set",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#breast-cancer-data-set",
    "title": "Worksheet Classification (Part I)",
    "section": "1. Breast Cancer Data Set",
    "text": "1. Breast Cancer Data Set\nWe will work with the breast cancer data from this from the accompanying textbook chapter.\n\nNote that the breast cancer data in this worksheet have been standardized (centred and scaled) for you already. We will implement these steps in future worksheet later, but for now, know the data has been standardized. Therefore the variables are unitless and hence why we have zero and negative values for variables like Radius.\n\nQuestion 1.0  {points: 1}\nRead the clean-wdbc-data.csv file (found in the data directory) using the pd.read_csv function into the notebook and store it as a data frame. Name it cancer.\n\n# your code here\nraise NotImplementedError\ncancer\n\n\nfrom hashlib import sha1\nassert sha1(str(type(cancer is None)).encode(\"utf-8\")+b\"1edfa\").hexdigest() == \"f89107c5738f5567ac4ce7e619af326d8dc7e7e4\", \"type of cancer is None is not bool. cancer is None should be a bool\"\nassert sha1(str(cancer is None).encode(\"utf-8\")+b\"1edfa\").hexdigest() == \"71bbe216c3f112174b74da6122dd837cb4abaafa\", \"boolean value of cancer is None is not correct\"\n\nassert sha1(str(type(cancer)).encode(\"utf-8\")+b\"1edfb\").hexdigest() == \"5f4717efa0f9568127506afdab187398929a3f76\", \"type of type(cancer) is not correct\"\n\nassert sha1(str(type(cancer.shape)).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"6790c026fc62f7f025cc5dfb65b5589e70c94f24\", \"type of cancer.shape is not tuple. cancer.shape should be a tuple\"\nassert sha1(str(len(cancer.shape)).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"e400bacf4406589a930f6cb04146f55ae09adbe1\", \"length of cancer.shape is not correct\"\nassert sha1(str(sorted(map(str, cancer.shape))).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"bc58412175f96aad1b5f00e35cbf014630e9667a\", \"values of cancer.shape are not correct\"\nassert sha1(str(cancer.shape).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"03f45e6b2934a9d89a82761d8cd384e41a96882e\", \"order of elements of cancer.shape is not correct\"\n\nassert sha1(str(type(sum(cancer.Area))).encode(\"utf-8\")+b\"1edfd\").hexdigest() == \"8459034dfcce3dc398256d3235e7ea1c0a6eab66\", \"type of sum(cancer.Area) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(cancer.Area), 2)).encode(\"utf-8\")+b\"1edfd\").hexdigest() == \"578eeb3cce10656ea3740570a63c462e74b5bee3\", \"value of sum(cancer.Area) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(cancer.columns.values)).encode(\"utf-8\")+b\"1edfe\").hexdigest() == \"ec1b028616e5bf668b318569d326edbe4fa5792d\", \"type of cancer.columns.values is not correct\"\nassert sha1(str(cancer.columns.values).encode(\"utf-8\")+b\"1edfe\").hexdigest() == \"46e462ccc418670863709af1c7c6c89e4aa501be\", \"value of cancer.columns.values is not correct\"\n\nassert sha1(str(type(cancer['Class'].dtype)).encode(\"utf-8\")+b\"1edff\").hexdigest() == \"afffce8467d6f078e21aa318ecedfd6b8782be5f\", \"type of cancer['Class'].dtype is not correct\"\nassert sha1(str(cancer['Class'].dtype).encode(\"utf-8\")+b\"1edff\").hexdigest() == \"852a6171a68036a67ec486822760a69c8cd61e0d\", \"value of cancer['Class'].dtype is not correct\"\n\nprint('Success!')\n\nQuestion 1.1 True or False:  {points: 1}\nAfter looking at the first six rows of the cancer data fame, suppose we asked you to predict the variable “area” for a new observation. Is this a classification problem?\nAssign your answer to an object called answer1_1. Make sure the correct answer is a boolean. i.e. True or False.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_1)).encode(\"utf-8\")+b\"3524b\").hexdigest() == \"96e967f4261014f1a2a76ba5f230d7b7e85abe83\", \"type of answer1_1 is not bool. answer1_1 should be a bool\"\nassert sha1(str(answer1_1).encode(\"utf-8\")+b\"3524b\").hexdigest() == \"88e4fbb75d430084961a28b324179d14f4999b12\", \"boolean value of answer1_1 is not correct\"\n\nprint('Success!')\n\nQuestion 1.2  {points: 1}\nCreate a scatterplot of the data with Symmetry on the x-axis and Radius on the y-axis. Modify your aesthetics by colouring for Class. As you create this plot, ensure you follow the guidelines for creating effective visualizations. In particular, note in the chart axis titles whether the data is standardized or not and add a suitable opacity level to the graphical mark. You should also replace the values in the dataframe’s Class column from 'M' to 'Malignant' and from 'B' to 'Benign'.\nAssign your plot to an object called cancer_plot.\n\ncancer[\"Class\"] = cancer[\"Class\"].replace({\n    'M' : 'Malignant',\n    'B' : 'Benign'\n})\ncancer_plot = alt.Chart(cancer).mark_point(opacity=0.5).encode(\n    x=alt.X(\"Symmetry\").title(\"Standardized symmetry\"),\n    y=alt.Y(\"Radius\").title(\"Standardized radius\"),\n    color=alt.Color(\"Class\").title(\"Diagnosis\")\n)\ncancer_plot\n\n\nfrom hashlib import sha1\nassert sha1(str(type(cancer['Class'].unique())).encode(\"utf-8\")+b\"7038e\").hexdigest() == \"2c870d7a3657b742557d66961de4a4891ee76aa2\", \"type of cancer['Class'].unique() is not correct\"\nassert sha1(str(cancer['Class'].unique()).encode(\"utf-8\")+b\"7038e\").hexdigest() == \"7be82206c225ab0f0a4ffad7c1488a64143481f7\", \"value of cancer['Class'].unique() is not correct\"\n\nassert sha1(str(type(cancer_plot is None)).encode(\"utf-8\")+b\"7038f\").hexdigest() == \"5f05f0c0e171e12d0d2b2a22d2789f0d9a8e342e\", \"type of cancer_plot is None is not bool. cancer_plot is None should be a bool\"\nassert sha1(str(cancer_plot is None).encode(\"utf-8\")+b\"7038f\").hexdigest() == \"515f1ad42fa781dff7c9975c8ded7f317ce95332\", \"boolean value of cancer_plot is None is not correct\"\n\nassert sha1(str(type(cancer_plot.encoding.x['shorthand'])).encode(\"utf-8\")+b\"70390\").hexdigest() == \"1b054e65bf0aac12e848da1d2afa2b12a5226b61\", \"type of cancer_plot.encoding.x['shorthand'] is not str. cancer_plot.encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(cancer_plot.encoding.x['shorthand'])).encode(\"utf-8\")+b\"70390\").hexdigest() == \"dd930b7fa9aa4d16b4d1a89c7a39089300827489\", \"length of cancer_plot.encoding.x['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"70390\").hexdigest() == \"dd17739f27d755296153ed0ec742286fd3b39cb2\", \"value of cancer_plot.encoding.x['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.x['shorthand']).encode(\"utf-8\")+b\"70390\").hexdigest() == \"d4212b1d5679f4601309609b6ea38ffb2d4ed07c\", \"correct string value of cancer_plot.encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(cancer_plot.encoding.y['shorthand'])).encode(\"utf-8\")+b\"70391\").hexdigest() == \"6fac44d279065f2a3f185c5a884fd9df6dd81cfb\", \"type of cancer_plot.encoding.y['shorthand'] is not str. cancer_plot.encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(cancer_plot.encoding.y['shorthand'])).encode(\"utf-8\")+b\"70391\").hexdigest() == \"df067cfef2a7c5291dd0f9acc53fe92d50ccf1fe\", \"length of cancer_plot.encoding.y['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"70391\").hexdigest() == \"edf8bc4ede29cf991a36bb6d8b38713806753b47\", \"value of cancer_plot.encoding.y['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.y['shorthand']).encode(\"utf-8\")+b\"70391\").hexdigest() == \"e3f5fd9a0d9f3c695791e536282f29ee7d2c6b4a\", \"correct string value of cancer_plot.encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(cancer_plot.encoding.color['shorthand'])).encode(\"utf-8\")+b\"70392\").hexdigest() == \"a1514e0d638e7fa47039d17e310bba44e6a8ff2c\", \"type of cancer_plot.encoding.color['shorthand'] is not str. cancer_plot.encoding.color['shorthand'] should be an str\"\nassert sha1(str(len(cancer_plot.encoding.color['shorthand'])).encode(\"utf-8\")+b\"70392\").hexdigest() == \"a43d370b386a7d21c0ac8e73bdc942b85ee93cae\", \"length of cancer_plot.encoding.color['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.color['shorthand'].lower()).encode(\"utf-8\")+b\"70392\").hexdigest() == \"a2e54ddb2a451b8647df9f5b9aae970b19620f66\", \"value of cancer_plot.encoding.color['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.color['shorthand']).encode(\"utf-8\")+b\"70392\").hexdigest() == \"dfff632fd3104b75a651bf07567aa5d572835f70\", \"correct string value of cancer_plot.encoding.color['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(cancer_plot.mark)).encode(\"utf-8\")+b\"70393\").hexdigest() == \"e736aec18d8c2224b69bfe505db7d3968a4a2c7e\", \"type of cancer_plot.mark is not correct\"\nassert sha1(str(cancer_plot.mark).encode(\"utf-8\")+b\"70393\").hexdigest() == \"1746955c8bc53ef47b4cf0267c98ce50ce3b184a\", \"value of cancer_plot.mark is not correct\"\n\nassert sha1(str(type(isinstance(cancer_plot.encoding.color['title'], str))).encode(\"utf-8\")+b\"70394\").hexdigest() == \"40775fb7d185dcb2ad7acf54925bb5075bd50aca\", \"type of isinstance(cancer_plot.encoding.color['title'], str) is not bool. isinstance(cancer_plot.encoding.color['title'], str) should be a bool\"\nassert sha1(str(isinstance(cancer_plot.encoding.color['title'], str)).encode(\"utf-8\")+b\"70394\").hexdigest() == \"e42ecc8f1d08c9a9cbb062b31d6180e6271e740b\", \"boolean value of isinstance(cancer_plot.encoding.color['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(cancer_plot.encoding.x['title'], str))).encode(\"utf-8\")+b\"70395\").hexdigest() == \"711e00550c10a33b1b90544076b3b8d4a89d23bf\", \"type of isinstance(cancer_plot.encoding.x['title'], str) is not bool. isinstance(cancer_plot.encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(cancer_plot.encoding.x['title'], str)).encode(\"utf-8\")+b\"70395\").hexdigest() == \"b697a6b22f874c7f85b3e081a24d1b5cdd084659\", \"boolean value of isinstance(cancer_plot.encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(cancer_plot.encoding.y['title'], str))).encode(\"utf-8\")+b\"70396\").hexdigest() == \"37f686ca3136e2e102714b66fd51cf3ef4703fc6\", \"type of isinstance(cancer_plot.encoding.y['title'], str) is not bool. isinstance(cancer_plot.encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(cancer_plot.encoding.y['title'], str)).encode(\"utf-8\")+b\"70396\").hexdigest() == \"d0eb5c4439424db54df14662a17296d7dc74ff3d\", \"boolean value of isinstance(cancer_plot.encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 1.3  {points: 1}\nJust by looking at the scatterplot above, how would you classify an observation with Symmetry = 1 and Radius = 1 (Benign or Malignant)?\nAssign your answer to an object called answer1_3. Make sure the correct answer is written fully. Remember to surround your answer with quotation marks (e.g. “Benign” / “Malignant”).\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_3)).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"c22b7e131d7091f92de118d328cf99ebfc373e56\", \"type of answer1_3 is not str. answer1_3 should be an str\"\nassert sha1(str(len(answer1_3)).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"426e383fda7dfde77b6cbef2a70be0047a593331\", \"length of answer1_3 is not correct\"\nassert sha1(str(answer1_3.lower()).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"04f0ffa0d0870e9e4d9a8bf7bb1ed255f9165a52\", \"value of answer1_3 is not correct\"\nassert sha1(str(answer1_3).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"11394ec2da0fa66e3dd679f582c3d6024557dc1a\", \"correct string value of answer1_3 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#using-scikit-learn-to-perform-k-nearest-neighbours",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#using-scikit-learn-to-perform-k-nearest-neighbours",
    "title": "Worksheet Classification (Part I)",
    "section": "2. Using scikit-learn to perform k-nearest neighbours",
    "text": "2. Using scikit-learn to perform k-nearest neighbours\nNow that we understand how K-nearest neighbours (k-nn) classification works, let’s get familar with the scikit-learn Python package. The benefit of using scikit-learn is that it will keep our code simple, readable and accurate. Coding less and in a tidier format means that there is less chance for errors to occur.\nWe’ll again focus on Radius and Symmetry as the two predictors. This time, we would like to predict the class of a new observation with Symmetry = 1 and Radius = 0. This one is a bit tricky to do visually from the plot below, and so is a motivating example for us to compute the prediction using k-nn with the scikit-learn package. Let’s use K = 7.\n\n# Run this to remind yourself what the data looks like\ncancer_plot\n\nQuestion 2.1  {points: 1}\nCreate a model for K-nearest neighbours classification by using the KNeighborsClassifier function. Specify that we want to set n_neighbors = 7.\nName your model specification knn_spec.\n\n# ___ = KNeighborsClassifier(n_neighbors=___)\n\n# your code here\nraise NotImplementedError\nknn_spec\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_spec is None)).encode(\"utf-8\")+b\"2245\").hexdigest() == \"78a85e8a7790e4e7a7d20d518a6e9e23249aeb08\", \"type of knn_spec is None is not bool. knn_spec is None should be a bool\"\nassert sha1(str(knn_spec is None).encode(\"utf-8\")+b\"2245\").hexdigest() == \"b44afb32e99c75fdbfb62a424921d1a2fa6f12ee\", \"boolean value of knn_spec is None is not correct\"\n\nassert sha1(str(type(knn_spec.n_neighbors)).encode(\"utf-8\")+b\"2246\").hexdigest() == \"1458700d2adfd82356dfd982b7941979cbd45589\", \"type of knn_spec.n_neighbors is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_spec.n_neighbors).encode(\"utf-8\")+b\"2246\").hexdigest() == \"22e82178737bce10c71283d1cf37544dd1dcbbb8\", \"value of knn_spec.n_neighbors is not correct\"\n\nassert sha1(str(type(knn_spec.algorithm)).encode(\"utf-8\")+b\"2247\").hexdigest() == \"c71db7d462b7abd56ce64492cec1d48ea9040224\", \"type of knn_spec.algorithm is not str. knn_spec.algorithm should be an str\"\nassert sha1(str(len(knn_spec.algorithm)).encode(\"utf-8\")+b\"2247\").hexdigest() == \"fc15a33f2e81ff2a1ebe2cdd52ca95582dc1df46\", \"length of knn_spec.algorithm is not correct\"\nassert sha1(str(knn_spec.algorithm.lower()).encode(\"utf-8\")+b\"2247\").hexdigest() == \"6fb8474d83ec0dc215e52cdc08b990751b7d94a9\", \"value of knn_spec.algorithm is not correct\"\nassert sha1(str(knn_spec.algorithm).encode(\"utf-8\")+b\"2247\").hexdigest() == \"6fb8474d83ec0dc215e52cdc08b990751b7d94a9\", \"correct string value of knn_spec.algorithm but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 2.2  {points: 1}\nTo train the model on the breast cancer dataset, pass knn_spec and the cancer dataset to the .fit function. Specify Class as your target variable and the Symmetry and Radius variables as your predictors. Name your fitted model as knn_fit.\n\n# X = ___[[\"Symmetry\", ___]]\n# y = ___[___]\n# ___ = ___.fit(___, ___)\n\n# your code here\nraise NotImplementedError\nknn_fit\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_fit is None)).encode(\"utf-8\")+b\"2bff6\").hexdigest() == \"56a79c088e5ec5a42b96d6ea0d14cd8712fac03e\", \"type of knn_fit is None is not bool. knn_fit is None should be a bool\"\nassert sha1(str(knn_fit is None).encode(\"utf-8\")+b\"2bff6\").hexdigest() == \"268e8a79ae1936524d056ca5169d2c71a72e70a5\", \"boolean value of knn_fit is None is not correct\"\n\nassert sha1(str(type(type(knn_fit))).encode(\"utf-8\")+b\"2bff7\").hexdigest() == \"097a69bbecb57e7e121a5edde688e93d2fbf9d17\", \"type of type(knn_fit) is not correct\"\nassert sha1(str(type(knn_fit)).encode(\"utf-8\")+b\"2bff7\").hexdigest() == \"40872fc0b9fdb9d1467bfb606dfb31df50cfe4ea\", \"value of type(knn_fit) is not correct\"\n\nassert sha1(str(type(knn_fit.classes_)).encode(\"utf-8\")+b\"2bff8\").hexdigest() == \"cb4bb5ca40cc7954a382bfa33bd1532a3df2e583\", \"type of knn_fit.classes_ is not correct\"\nassert sha1(str(knn_fit.classes_).encode(\"utf-8\")+b\"2bff8\").hexdigest() == \"c422ff7ed5336560783f0ada3740882c912a4d97\", \"value of knn_fit.classes_ is not correct\"\n\nassert sha1(str(type(knn_fit.effective_metric_)).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"aa9d11a76326cf64bec451997ff4ae28ec4e2a9a\", \"type of knn_fit.effective_metric_ is not str. knn_fit.effective_metric_ should be an str\"\nassert sha1(str(len(knn_fit.effective_metric_)).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"6e6efca67ebf0603a534109614346757c0ef146d\", \"length of knn_fit.effective_metric_ is not correct\"\nassert sha1(str(knn_fit.effective_metric_.lower()).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"49892ddeb7ca1f1c4bf772ff4a585dd10249b813\", \"value of knn_fit.effective_metric_ is not correct\"\nassert sha1(str(knn_fit.effective_metric_).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"49892ddeb7ca1f1c4bf772ff4a585dd10249b813\", \"correct string value of knn_fit.effective_metric_ but incorrect case of letters\"\n\nassert sha1(str(type(knn_fit.n_features_in_)).encode(\"utf-8\")+b\"2bffa\").hexdigest() == \"08548a765b78a776a8b132bc9a51a75e1d960554\", \"type of knn_fit.n_features_in_ is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_fit.n_features_in_).encode(\"utf-8\")+b\"2bffa\").hexdigest() == \"114d53a02f71bf2cea7abe3280555955b16647b8\", \"value of knn_fit.n_features_in_ is not correct\"\n\nassert sha1(str(type(X.columns.values)).encode(\"utf-8\")+b\"2bffb\").hexdigest() == \"bd7b6259f8053a315fca30986b168530209f266d\", \"type of X.columns.values is not correct\"\nassert sha1(str(X.columns.values).encode(\"utf-8\")+b\"2bffb\").hexdigest() == \"b7dfb165b13d0e67f16b09e1dc18733df42a65a0\", \"value of X.columns.values is not correct\"\n\nassert sha1(str(type(y.name)).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"e8b9e2753401141ac2885e2394f32cc9cffa348a\", \"type of y.name is not str. y.name should be an str\"\nassert sha1(str(len(y.name)).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"793a9c94797d9b5899e9865188980d72d080ac2d\", \"length of y.name is not correct\"\nassert sha1(str(y.name.lower()).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"bcffa9a693072729c80524aafde25fd6b259f4c3\", \"value of y.name is not correct\"\nassert sha1(str(y.name).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"788a47e28ae9d6cd3cbf4823a5d93017aa6addd9\", \"correct string value of y.name but incorrect case of letters\"\n\nassert sha1(str(type(sum(X.Symmetry))).encode(\"utf-8\")+b\"2bffd\").hexdigest() == \"903f7c4e29e57fe29f567ef056e34f51335aec78\", \"type of sum(X.Symmetry) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X.Symmetry), 2)).encode(\"utf-8\")+b\"2bffd\").hexdigest() == \"9397d4f7e4087a12b928268b533b6f46554e7efa\", \"value of sum(X.Symmetry) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(X.Radius))).encode(\"utf-8\")+b\"2bffe\").hexdigest() == \"d9df2ec2ea3462b25d7d591366163062bdb4021e\", \"type of sum(X.Radius) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X.Radius), 2)).encode(\"utf-8\")+b\"2bffe\").hexdigest() == \"5bd565738755b0a3859dbce84dffc4a36fe6656e\", \"value of sum(X.Radius) is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 2.3 {points: 1}\nNow we will make our prediction on the Class of a new observation with a Symmetry of 1 and a Radius of 0. First, create a dataframe with these variables and values and call it new_obs. Next, use the .predict function to obtain our prediction by passing knn_fit and new_obs to it. Name your predicted class as class_prediction.\n\n# ___ = pd.DataFrame([[1, 0]], columns=[___, ___])\n# ___ = ___.predict(___)\n\n# your code here\nraise NotImplementedError\nclass_prediction\n\n\nfrom hashlib import sha1\nassert sha1(str(type(new_obs is None)).encode(\"utf-8\")+b\"cc461\").hexdigest() == \"ab0357cbb8bfe0c9949b6d69e4c245d2af0635c6\", \"type of new_obs is None is not bool. new_obs is None should be a bool\"\nassert sha1(str(new_obs is None).encode(\"utf-8\")+b\"cc461\").hexdigest() == \"a170f5d0292d586ae590d1baca2f13355fb1c178\", \"boolean value of new_obs is None is not correct\"\n\nassert sha1(str(type(new_obs)).encode(\"utf-8\")+b\"cc462\").hexdigest() == \"46dce56999c53d1950618cc8e55690c524892a78\", \"type of type(new_obs) is not correct\"\n\nassert sha1(str(type(new_obs.Symmetry.values)).encode(\"utf-8\")+b\"cc463\").hexdigest() == \"113c0758f0ab20fd6712d816469f64cbc4243a12\", \"type of new_obs.Symmetry.values is not correct\"\nassert sha1(str(new_obs.Symmetry.values).encode(\"utf-8\")+b\"cc463\").hexdigest() == \"a41abcf000cf506078cdf4e306ca8121772da0f6\", \"value of new_obs.Symmetry.values is not correct\"\n\nassert sha1(str(type(new_obs.Radius.values)).encode(\"utf-8\")+b\"cc464\").hexdigest() == \"ca2beddfd89352b63fc45c70ca25260e7271c928\", \"type of new_obs.Radius.values is not correct\"\nassert sha1(str(new_obs.Radius.values).encode(\"utf-8\")+b\"cc464\").hexdigest() == \"195b0a370675aea18bf01c541c099428f4e248bd\", \"value of new_obs.Radius.values is not correct\"\n\nassert sha1(str(type(class_prediction is None)).encode(\"utf-8\")+b\"cc465\").hexdigest() == \"1ebd9c72926a2430221b10ae35a374b95040d55f\", \"type of class_prediction is None is not bool. class_prediction is None should be a bool\"\nassert sha1(str(class_prediction is None).encode(\"utf-8\")+b\"cc465\").hexdigest() == \"883aa082b8853226ec339ff84fd4744558d7fd3b\", \"boolean value of class_prediction is None is not correct\"\n\nassert sha1(str(type(class_prediction)).encode(\"utf-8\")+b\"cc466\").hexdigest() == \"a0d1780e9e6b14b3a86e32b907408ae8253ceec5\", \"type of class_prediction is not correct\"\nassert sha1(str(class_prediction).encode(\"utf-8\")+b\"cc466\").hexdigest() == \"2704eb31133cad075ccb97f6c93ca707487c9642\", \"value of class_prediction is not correct\"\n\nprint('Success!')\n\nQuestion 2.4  {points: 1}\nLet’s perform K-nearest neighbour classification again, but with three predictors. Use the scikit-learn package and K = 7 to classify a new observation where we measure Symmetry = 1, Radius = 0 and Concavity = 1. Use the scaffolding from Questions 3.2 and 3.3 to help you.\n\nPass the same knn_spec from before to fit, but this time specify Symmetry, Radius, and Concavity as the predictors. Save the predictor as X_2 and the target as y_2. Store the output in knn_fit_2.\nStore the new observation values in an object called new_obs_2.\nStore the output of predict in an object called class_prediction_2.\n\n\n# your code here\nraise NotImplementedError\nclass_prediction_2\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_fit_2 is None)).encode(\"utf-8\")+b\"1c52c\").hexdigest() == \"56fcaa2be4db81bcc9472911429b629cba6101f3\", \"type of knn_fit_2 is None is not bool. knn_fit_2 is None should be a bool\"\nassert sha1(str(knn_fit_2 is None).encode(\"utf-8\")+b\"1c52c\").hexdigest() == \"99533bc9488848c2608ec11993eb5cfe3033cdf5\", \"boolean value of knn_fit_2 is None is not correct\"\n\nassert sha1(str(type(knn_fit_2.kneighbors)).encode(\"utf-8\")+b\"1c52d\").hexdigest() == \"3f30af90ec8c1a811004f1225ae54fbd83681019\", \"type of knn_fit_2.kneighbors is not correct\"\nassert sha1(str(knn_fit_2.kneighbors).encode(\"utf-8\")+b\"1c52d\").hexdigest() == \"55cb84f4cc156f43e6f8f2b5aa8379f34d7d579d\", \"value of knn_fit_2.kneighbors is not correct\"\n\nassert sha1(str(type(knn_fit_2.effective_metric_)).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"4a071b6a39a1366f1c50cfff6bf5bafe83bd944b\", \"type of knn_fit_2.effective_metric_ is not str. knn_fit_2.effective_metric_ should be an str\"\nassert sha1(str(len(knn_fit_2.effective_metric_)).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"6949133fe0bc9d23f3f2391308a85eeac883a74a\", \"length of knn_fit_2.effective_metric_ is not correct\"\nassert sha1(str(knn_fit_2.effective_metric_.lower()).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"fed6a9fc14d942c80421836e802e4f459699e299\", \"value of knn_fit_2.effective_metric_ is not correct\"\nassert sha1(str(knn_fit_2.effective_metric_).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"fed6a9fc14d942c80421836e802e4f459699e299\", \"correct string value of knn_fit_2.effective_metric_ but incorrect case of letters\"\n\nassert sha1(str(type(type(knn_fit_2))).encode(\"utf-8\")+b\"1c52f\").hexdigest() == \"c0688283eb347b3a716716a2762a46dfa6000cbd\", \"type of type(knn_fit_2) is not correct\"\nassert sha1(str(type(knn_fit_2)).encode(\"utf-8\")+b\"1c52f\").hexdigest() == \"0704b8be28fd8da7859dd6fc8e5148535b75399d\", \"value of type(knn_fit_2) is not correct\"\n\nassert sha1(str(type(knn_fit_2.n_features_in_)).encode(\"utf-8\")+b\"1c530\").hexdigest() == \"3ac6a926c4de23bcae7e9a759aca7cd489f4cad2\", \"type of knn_fit_2.n_features_in_ is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_fit_2.n_features_in_).encode(\"utf-8\")+b\"1c530\").hexdigest() == \"a032550f3adad26547a9e2adaae58acd93a5b36e\", \"value of knn_fit_2.n_features_in_ is not correct\"\n\nassert sha1(str(type(X_2.columns.values)).encode(\"utf-8\")+b\"1c531\").hexdigest() == \"a7b153af47464cec438aaac3a5398422f15c739c\", \"type of X_2.columns.values is not correct\"\nassert sha1(str(X_2.columns.values).encode(\"utf-8\")+b\"1c531\").hexdigest() == \"c3fd80e7a523addffb5f040e8560caf25c601447\", \"value of X_2.columns.values is not correct\"\n\nassert sha1(str(type(y_2.name)).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"6ecd85411910b1c1e49f8fb6a6c446122031f3a3\", \"type of y_2.name is not str. y_2.name should be an str\"\nassert sha1(str(len(y_2.name)).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"ebad5f09e153e6397258881e828d3d859c85154e\", \"length of y_2.name is not correct\"\nassert sha1(str(y_2.name.lower()).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"7c29dd51d86bde999e3497b24a52324ffa88d80b\", \"value of y_2.name is not correct\"\nassert sha1(str(y_2.name).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"ff10a06265a833d1274bee055d40fe11d6c1de8f\", \"correct string value of y_2.name but incorrect case of letters\"\n\nassert sha1(str(type(y_2.values)).encode(\"utf-8\")+b\"1c533\").hexdigest() == \"6701ed81a4e9d4e3d605bff672f33abc4cab49ab\", \"type of y_2.values is not correct\"\nassert sha1(str(y_2.values).encode(\"utf-8\")+b\"1c533\").hexdigest() == \"1eeb2c2596b3d6987df53c982b8921d368bfa856\", \"value of y_2.values is not correct\"\n\nassert sha1(str(type(sum(X_2.Symmetry))).encode(\"utf-8\")+b\"1c534\").hexdigest() == \"401b25657e3ccc6f373b04c3f73b718954f2f87f\", \"type of sum(X_2.Symmetry) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X_2.Symmetry), 2)).encode(\"utf-8\")+b\"1c534\").hexdigest() == \"10b6bfc8d4273cf2cda252fff3ead0fcdb4a13cd\", \"value of sum(X_2.Symmetry) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(X_2.Radius))).encode(\"utf-8\")+b\"1c535\").hexdigest() == \"bc045aaaa3e654827d701f1a154d82ce3bf159de\", \"type of sum(X_2.Radius) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X_2.Radius), 2)).encode(\"utf-8\")+b\"1c535\").hexdigest() == \"ba710607201f7fb546dfbff2bbf8125c43eb507a\", \"value of sum(X_2.Radius) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(X_2.Concavity))).encode(\"utf-8\")+b\"1c536\").hexdigest() == \"bdd5f789273332ff93a37a929e933342daf588ce\", \"type of sum(X_2.Concavity) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X_2.Concavity), 2)).encode(\"utf-8\")+b\"1c536\").hexdigest() == \"c83ba58970a37bc501a808908479fa9f85081954\", \"value of sum(X_2.Concavity) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(new_obs_2 is None)).encode(\"utf-8\")+b\"1c537\").hexdigest() == \"0e6db0897dec3a5a0be1727ee2174838fed8392d\", \"type of new_obs_2 is None is not bool. new_obs_2 is None should be a bool\"\nassert sha1(str(new_obs_2 is None).encode(\"utf-8\")+b\"1c537\").hexdigest() == \"3bd0c39ffd477eef3d20295014e9603c292a36db\", \"boolean value of new_obs_2 is None is not correct\"\n\nassert sha1(str(type(new_obs_2)).encode(\"utf-8\")+b\"1c538\").hexdigest() == \"dbd7bddf2ba93297c398200c3aba22eee791cc2b\", \"type of type(new_obs_2) is not correct\"\n\nassert sha1(str(type(new_obs_2.Symmetry.values)).encode(\"utf-8\")+b\"1c539\").hexdigest() == \"f025231cf463eb1b53a6eb94ed504ae8eacbd2b3\", \"type of new_obs_2.Symmetry.values is not correct\"\nassert sha1(str(new_obs_2.Symmetry.values).encode(\"utf-8\")+b\"1c539\").hexdigest() == \"ee3ccc35dc76e76f3d3f26ce2af31aba1c00475d\", \"value of new_obs_2.Symmetry.values is not correct\"\n\nassert sha1(str(type(new_obs_2.Radius.values)).encode(\"utf-8\")+b\"1c53a\").hexdigest() == \"88a9bfa1595cca275e5f844a75c8fd0e7e660b85\", \"type of new_obs_2.Radius.values is not correct\"\nassert sha1(str(new_obs_2.Radius.values).encode(\"utf-8\")+b\"1c53a\").hexdigest() == \"71bc38e24dca943a89fdba0e143c0c93a97d9dbd\", \"value of new_obs_2.Radius.values is not correct\"\n\nassert sha1(str(type(new_obs_2.Concavity.values)).encode(\"utf-8\")+b\"1c53b\").hexdigest() == \"581e0fa5a3a53d9c88d6df38ab3e022f128d41bb\", \"type of new_obs_2.Concavity.values is not correct\"\nassert sha1(str(new_obs_2.Concavity.values).encode(\"utf-8\")+b\"1c53b\").hexdigest() == \"6366ed5e9216d5cf0dab850bfd420217af90487c\", \"value of new_obs_2.Concavity.values is not correct\"\n\nassert sha1(str(type(class_prediction_2 is None)).encode(\"utf-8\")+b\"1c53c\").hexdigest() == \"102656310fdb351c06ebfcd34be208a380387416\", \"type of class_prediction_2 is None is not bool. class_prediction_2 is None should be a bool\"\nassert sha1(str(class_prediction_2 is None).encode(\"utf-8\")+b\"1c53c\").hexdigest() == \"2a4bb29e14b26cb4fba6321e00f5ea7264414756\", \"boolean value of class_prediction_2 is None is not correct\"\n\nassert sha1(str(type(class_prediction_2)).encode(\"utf-8\")+b\"1c53d\").hexdigest() == \"0d077a418cd3754263e26adf0dd15e657d98c01a\", \"type of class_prediction_2 is not correct\"\nassert sha1(str(class_prediction_2).encode(\"utf-8\")+b\"1c53d\").hexdigest() == \"fa0463880a9a71ddc10bfae3b027f9f4a6da0038\", \"value of class_prediction_2 is not correct\"\n\nprint('Success!')\n\nQuestion 2.5 {points: 1}\nFinally, we will perform K-nearest neighbour classification again, using the scikit-learn package and K = 7 to classify a new observation where we use all the predictors in our data set (we give you the values in the code below).\nBut we first have to do one important thing: we need to remove the ID variable from the analysis (it’s not a numerical measurement that we should use for classification). Thankfully, scikit-learn provides a nice way of combining data preprocessing and training into a single consistent pipeline.\nWe will first create a preprocessor to remove the ID variable using the drop preprocessing step. Since we aren’t doing any preprocessing to other columns, we will set the remainder parameter to passthrough. Do so below using the provided scaffolding. Name the preprocessor object knn_preprocessor.\n\n# ___ = make_column_transformer(\n#     (\"drop\", [___]),\n#     remainder=___\n# )\n\n# your code here\nraise NotImplementedError\nknn_preprocessor\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_preprocessor is None)).encode(\"utf-8\")+b\"c9190\").hexdigest() == \"1aa697ad8121d7a033d091dd1dde11fd9a9048c0\", \"type of knn_preprocessor is None is not bool. knn_preprocessor is None should be a bool\"\nassert sha1(str(knn_preprocessor is None).encode(\"utf-8\")+b\"c9190\").hexdigest() == \"fa97b74142ab349c61853efd4aed109620ea4419\", \"boolean value of knn_preprocessor is None is not correct\"\n\nassert sha1(str(type(type(knn_preprocessor))).encode(\"utf-8\")+b\"c9191\").hexdigest() == \"1109d30d9e1570f804f20d5d028ef6afce897841\", \"type of type(knn_preprocessor) is not correct\"\nassert sha1(str(type(knn_preprocessor)).encode(\"utf-8\")+b\"c9191\").hexdigest() == \"b7f41df01f0ad5de7c336fbe446f33229e39ffba\", \"value of type(knn_preprocessor) is not correct\"\n\nassert sha1(str(type(knn_preprocessor.get_feature_names_out)).encode(\"utf-8\")+b\"c9192\").hexdigest() == \"bb33b66fca24d58fe1cd06f2a2985674ee8f0698\", \"type of knn_preprocessor.get_feature_names_out is not correct\"\nassert sha1(str(knn_preprocessor.get_feature_names_out).encode(\"utf-8\")+b\"c9192\").hexdigest() == \"ee35e154ea1785364b8455f97248689b3f960199\", \"value of knn_preprocessor.get_feature_names_out is not correct\"\n\nprint('Success!')\n\nQuestion 2.6  {points: 1}\nCreate a pipeline that includes the new preprocessor (knn_preprocessor) and the model specification (knn_spec) using the scaffolding below. Name the pipeline object knn_pipeline.\n\n# ___ = make_pipeline(___, ___)\n\n# your code here\nraise NotImplementedError\nknn_pipeline\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_pipeline is None)).encode(\"utf-8\")+b\"570b0\").hexdigest() == \"5229d62594055e066352668824c5c3908bb29f74\", \"type of knn_pipeline is None is not bool. knn_pipeline is None should be a bool\"\nassert sha1(str(knn_pipeline is None).encode(\"utf-8\")+b\"570b0\").hexdigest() == \"1d2994bb8a065a370b5d478a8087d37e3841c353\", \"boolean value of knn_pipeline is None is not correct\"\n\nassert sha1(str(type(type(knn_pipeline))).encode(\"utf-8\")+b\"570b1\").hexdigest() == \"4b6b2b6e8dcd91c384d5e7b3ffaa5df5ed5fac76\", \"type of type(knn_pipeline) is not correct\"\nassert sha1(str(type(knn_pipeline)).encode(\"utf-8\")+b\"570b1\").hexdigest() == \"7cff373362fd36faa9d06c119ebecf6820b812f0\", \"value of type(knn_pipeline) is not correct\"\n\nassert sha1(str(type(knn_pipeline.named_steps.kneighborsclassifier.n_neighbors)).encode(\"utf-8\")+b\"570b2\").hexdigest() == \"c0cf487b545aea403a76ddbbedba6f508f1e6b8f\", \"type of knn_pipeline.named_steps.kneighborsclassifier.n_neighbors is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_pipeline.named_steps.kneighborsclassifier.n_neighbors).encode(\"utf-8\")+b\"570b2\").hexdigest() == \"0c675a0ab79e6e0c763779b13443212fca47db27\", \"value of knn_pipeline.named_steps.kneighborsclassifier.n_neighbors is not correct\"\n\nprint('Success!')\n\nQuestion 2.7 {points: 1}\nFinally, fit the pipeline and predict the class label for the new observation named new_obs_all. Name the fit object knn_fit_all, and the class prediction class_prediction_all. Name the new predictor as X_3 and the new target as y_3.\n\nnew_obs_all = pd.DataFrame(\n    [[None, 0, 0, 0, 0, 0.5, 0, 1, 0, 1, 0]],\n    columns=[\n        \"ID\",\n        \"Radius\",\n        \"Texture\",\n        \"Perimeter\",\n        \"Area\",\n        \"Smoothness\",\n        \"Compactness\",\n        \"Concavity\",\n        \"Concave_points\",\n        \"Symmetry\",\n        \"Fractal_dimension\",\n    ],\n)\n# X_3 = cancer.drop(columns=[___])\n# y_3 = cancer[___]\n# ___ = knn_pipeline.fit(___, ___)\n# ___ = knn_fit_all.____(____)\n\n# your code here\nraise NotImplementedError\nclass_prediction_all\n\n\nfrom hashlib import sha1\nassert sha1(str(type(class_prediction_all)).encode(\"utf-8\")+b\"bdab9\").hexdigest() == \"6a39e4695fdfb5df240814e4336bf420375f8ab0\", \"type of class_prediction_all is not correct\"\nassert sha1(str(class_prediction_all).encode(\"utf-8\")+b\"bdab9\").hexdigest() == \"f3a6c0bfa60af2a0f2398b36ffe3f27359ac2478\", \"value of class_prediction_all is not correct\"\n\nprint('Success!')"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. __Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  }
]